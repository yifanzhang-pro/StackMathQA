{"Q": "How to compute the standard error of the mean of an AR(1) process? I try to compute the standard error of the mean for a demeaned AR(1) process $x_{t+1} = \\rho x_t + \\varepsilon_{t+1} =\\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t+1-i}$\nHere is what I did:\n$$\n\\begin{align*}\nVar(\\overline{x}) &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} x_t\\right) \\\\\n                  &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} \\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t-i}\\right) \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_0 +  & \\rho^1 \\varepsilon_{-1} +  & \\rho^2 \\varepsilon_{-2} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{-\\infty} + \\\\ \n                                                      \\rho^0  \\varepsilon_1 +  & \\rho^1 \\varepsilon_{0}  +   & \\rho^2 \\varepsilon_{-1} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{1-\\infty} + \\\\\n                                                      \\vdots               & \\vdots                 & \\vdots                & \\ddots  & \\vdots \\\\\n                                                      \\rho^0\\varepsilon_{N-1} + & \\rho^1 \\varepsilon_{N-2} +  & \\rho^2 \\varepsilon_{N-3} + & \\cdots  & \\rho^{\\infty} \\varepsilon_{N-1-\\infty} + \\\\\n                                      \\end{pmatrix} \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_{N-1} + \\\\ \n                                                      (\\rho^0 + \\rho^1)  \\varepsilon_{N-2} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  \\varepsilon_{N-3} + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  \\varepsilon_{1} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  \\varepsilon_{0} + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})  \\varepsilon_{-1} + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  \\varepsilon_{-2} + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                 &= \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^0   + \\\\ \n                                                      (\\rho^0 + \\rho^1)  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                &= \\frac{N \\sigma_{\\varepsilon}^2}{N^2} (\\rho^0 + \\rho^1 + \\dots + \\rho^{\\infty}) \\\\\n                &= \\frac{\\sigma_{\\varepsilon}^2}{N} \\frac{1}{1 - \\rho} \\\\\n\\end{align*}\n$$\nProbably, not every step is done in the most obvious way, so let me add some thoughts. In the third row, I just write out to two sum-signs. Here, the matrix has N rows. In the fourth row, I realign the matrix so that there is one row for every epsilon, so the number of rows is infinite here. Note that the last three parts in the matrix have the same number of elements, just differencing by a factor $\\rho$ in each row. In the fifth row, I apply the rule that the variance of the sum of independent shocks is the sum of the variances of those shocks and notice that each $\\rho^j$ element is summed up $N$ times.\nThe end result looks neat, but is probably wrong. Why do I think so? Because I run a MCS in R and things don't add up:\nnrMCS <- 10000\nN <- 100\npers <- 0.9\nmeans <- numeric(nrMCS)\nfor (i in 1:nrMCS) {\n   means[i] <- mean(arima.sim(list(order=c(1,0,0), ar=pers), n = N))\n}\n#quantile(means, probs=c(0.025, 0.05, 0.5, 0.95, 0.975))\n#That is the empirical standard error\nsd(means)\n0.9459876\n#This should be the standard error according to my formula\n1/(N*(1-pers))\n0.1\n\nAny hints on what I am doing wrong would be great! Or maybe a hint where I can find the correct derivation (I couldn't find anything). Is the problem maybe that I assume independence between the same errors?\n$$Var(X + X) = Var(2X) = 4Var(X) \\neq 2Var(X)$$\nI thought about that, but don't see where I make that erroneous assumption in my derivation.\nUPDATE\nI forgot to square the rhos, as Nuzhi correctly pointed out. Hence it should look like:\n$$                Var(\\overline{x}) = \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^{2\\times0}   + \\\\ \n                                                      (\\rho^0 + \\rho^1)^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)^2  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})^2  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})^2    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})^2  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} $$\n", "A": "Well actually  when you take the following\n\\begin{align*}\nVar(\\overline{x}) &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} x_t\\right) \\\\\n\\end{align*}\nIt is easier to derive an implicit value rather than an explicit value in this case..your answer and mine are the same ..it's just that yours is a bit more difficult to handle because of the expansion of rho's..but some algebraic manipulation should be able to do the trick i guess ....I derived the answer as follows\nsince $ Var(\\overline{x})$ is a linear combination..... \n\\begin{align*}\nVar(\\overline{x}) &= \\frac{1}{N^2}Cov\\left(\\sum\\limits_{t=0}^{N-1}\\sum\\limits_{j=0}^{N-1} x_t x_j\\right) \\\\\n\\end{align*}\n\\begin{align*}\nVar(\\overline{x}) &= \\frac{1}{N^2}\\sum\\limits_{t=0}^{N-1}Var\\left( x_t \\right) + \\frac{1}{N^2}\\sum\\limits_{t=0}^{N-1}\\sum\\limits_{j \\ne t}^{N-1}Cov\\left( x_t x_j\\right) \\\\\n\\end{align*}\nNow for an AR(1) process $Var(x_t) =  \\frac{{\\sigma_{\\varepsilon}}^2}{1 - \\rho^2} $ and $Cov(x_tx_j) =   \\frac{{\\sigma_{\\varepsilon}}^2}{1 - \\rho^2}\\rho^{|j-t|} $....\nSubstituting in the above gives the required equation... hope this answers your question :)\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/40585", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 4, "answer_id": 0}}
{"Q": "how can calculate $E(X^Y|X+Y=1)$ let $X,Y$ are two independent random variables Bernoulli with probability $P$. how can calculate $E(X^Y|X+Y=1)$\n", "A": "Let's calculate all outcomes for the $X^Y$:\n\n\n*\n\n*$X=0$, $Y=0$ $\\Rightarrow X^Y= 0^0 = 1$, $P=(1-p)^2$\n\n*$X=0$, $Y=1$ $\\Rightarrow X^Y= 0^1 = 0$, $P=p(1-p)$\n\n*$X=1$, $Y=0$ $\\Rightarrow X^Y= 1^0 = 1$, $P=p(1-p)$\n\n*$X=1$, $Y=1$ $\\Rightarrow X^Y= 1^1 = 1$, $P=p^2$\nCondition $X+Y=1$ means we consider only the two\nequally likely outcomes 2 and 3, that is, conditioned\non $X+Y = 1$, $X^Y$ is a Bernoulli random variable with\nparameter $\\frac{1}{2}$ and so $E[X^Y\\mid X+Y = 1] = \\frac{1}{2}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/86790", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Tossing 2 coins, distribution We are tossing 2 fair coins once. Then for the next toss we are only tossing the coin(s) that came up heads before.\nLet $X$ be the total number of heads. \nThe question is $EX$ and the distribution of $X$\nI tried to calculate the expected values for small $X=x$-es but it gets really complicated soon.\n", "A": "Start by considering each coin in isolation, and the question becomes easier. Let $Y$ denote the number of heads for the first coin, and $Z$ denote the number of heads for the second coin, so $X=Y+Z$. $Y$ and $Z$ are identically distributed so let's just consider $Y$. \nFirst, we know that if $Y=y$, then the first coin must have come up heads exactly $y$ times, and then tails once. The probability of $y$ heads in a row is $\\left ( \\frac{1}{2} \\right )^{y}$, and the probability of getting tails after that is $\\frac{1}{2} $. Thus:\n$P(Y=y) = \\left ( \\frac{1}{2} \\right )^{(y+1)}$\nTo calculate the expected value of $Y$, we sum $y \\cdot P(Y=y)$ over all values of $Y$, from zero to infinity:\n\\begin{align} \nE[Y] &= \\sum_{y=0}^{\\infty} y \\cdot P(Y = y)\\\\\n&= \\sum_{y=0}^{\\infty} y \\cdot \\left ( \\frac{1}{2} \\right )^{(y+1)} \\\\\n&= 1\n\\end{align}\nThe expectation of the sum of two random variables is the sum of their expectations, so $E[X] = E[Y+Z] = E[Y] + E[Z] = 2$.\nHow do we use $P(Y)$ to get $P(X)$? Here's an example: Say $X=2$. Then we know there are three possibilities: (1) $Y=2$ and $Z=0$, (2) $Y=1$ and $Z=1$, or (3) $Y=0$ and $Z=1$. Since $Y$ and $Z$ are independent, we have: \n\\begin{align}\nP(X=2) &= \\left( \\left( \\frac{1}{2} \\right)^{3}\\cdot \\frac{1}{2} \\right ) + \\left( \\left( \\frac{1}{2} \\right)^{2}\\cdot \\left(\\frac{1}{2} \\right)^2 \\right) + \\left( \\frac{1}{2} \\cdot \\left( \\frac{1}{2} \\right)^{3} \\right )\\\\\n&= 3 \\cdot \\left(  \\frac{1}{2} \\right)^{4}\n\\end{align}\nThis example gives the intuition that maybe $P(X=x) = (x+1) \\cdot \\left(  \\frac{1}{2} \\right)^{(x+2)}$. It is true for $X=0$: both heads have to come up tails on the first flip, and the probability of that occurring is $\\frac{1}{4} = (0+1) \\cdot \\left(  \\frac{1}{2} \\right)^{(0+2)}$. \nIt should be simple to show by induction that this is true for all values of $X$. Here is a sketch. First note that if $X=x$ there are $x+1$ possible combinations of $Y$ and $Z$ values that can produce $y + z = x$. Each value of $Y$ corresponds to a unique series of heads and a tail (and likewise for $Z$). If we iterate, and ask what values of $Y$ and $Z$ could give $y' + z' = x+1$, we can start with our original set of possible combinations of $Y$ and $Z$ values, and just add an extra head to the start of each run for the first coin, which would multiply the probability of each combination by $\\frac{1}{2}$. That is, we set $y'= y+1$ and $z'=z$. Then we need to add one new term to the sum, to account for the case where $y=0$, and $z'=x+1$. \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/119775", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Normal Distribution Puzzle/Riddle Some time in the future, a lottery takes place, with winning number N. 3 of your friends from the future, John, Joe, and James, provide you with guesses on the number N.\nJohn's guess a is randomly selected from a gaussian distribution centered at N with stdev x;\nJoe's guess b is randomly selected from a gaussian distribution centered at N with stdev y;\nJames' guess c is randomly selected from a gaussian distribution centered at N with stdev z;\nGiven the values of a, x, b, y, c, z, what would be the best guess of N? Also define what \"best\" is.\n", "A": "You can calculate and maximize the likelihood of N given a,b,c, with x,y,z being fixed. \nThe Likelihood of a value of N (the probability of sampling a,b,c given that the mean is N) is: \n$LL_{a,b,c}(N) = Pr(a | x,N) \\cdot Pr(b | y,N) \\cdot Pr(c | z,N)$ \nWith the distributions being independent and Gaussian, this is \n$LL_{a,b,x}(N) = \\frac{1}{x\\sqrt{2\\pi}} e^{-\\frac{(a-N)^2}{2x^2}} \\cdot \\frac{1}{y\\sqrt{2\\pi}} e^{-\\frac{(b-N)^2}{2y^2}} \\cdot \n\\frac{1}{z\\sqrt{2\\pi}} e^{-\\frac{(c-N)^2}{2z^2}} = $\n$\\frac{1}{xyz(\\sqrt{2\\pi})^3} e^{-\\frac{1}{2}(\\frac{(a-N)^2}{x^2}  +\\frac{(b-N)^2}{y^2}+\\frac{(c-N)^2}{z^2})}$ \nAnd we want to find the N that maximizes this likelihood. To find the maximum, we will search for a point where the derivative of the likelihood equals zero. \n$\\frac{d}{dN}LL_{a,b,c}(N) = \\frac{1}{xyz(\\sqrt{2\\pi})^3}\\cdot -\\frac{1}{2}(\\frac{2(a-N)}{x^2} + \\frac{2(b-N) }{y^2}+ \\frac{2(c-N)}{z^2}) e^{-\\frac{1}{2}(\\frac{(a-N)^2}{x^2}  +\\frac{(b-N)^2}{y^2}+\\frac{(c-N)^2}{z^2})}$\nThis equals zero if and only if \n$\\frac{2(a-N)}{x^2} + \\frac{2(b-N) }{y^2}+ \\frac{2(c-N)}{z^2} = 0$ \nSo we get that \n$y^2z^2a - y^2z^2N + x^2 z^2 b- x^2 z^2 N +  x^2 y^2 c-x^2 y^2 N = 0$ \n$N = \\frac{y^2z^2a+ x^2z^2b + x^2y^2c}{y^2z^2 + x^2z^2 + x^2y^2}$ \nIs the maximum likelihood estimate. \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/442888", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Expected rolls to roll every number on a dice an odd number of times Our family has recently learned how to play a simple game called 'Oh Dear'. Each player has six playing cards (Ace,2,3,4,5,6) turned face-up, and take turns to roll the dice. Whatever number the dice rolls, the corresponding card is turned over. The winner is the player to turn all their cards face down first, but if you roll the number of a card that has been turned face-down, then that card is turned face-up again (and you say 'Oh Dear!').\nI want to work out the expected length of a game (in rolls of the dice). I'm interested first in working this out in the case of a single-player playing alone, and then also in the question of how the answer changes with multiple players. This is equivalent to working out the expected number of times a player must roll the dice to have rolled every number on the dice an odd number of times. (I assume a fair six-sided dice, but again would be interested in a more general solution too).\nIt is simple to work out the odds of winning as quickly as possible from any position, but I'm not sure how to go about calculating the expected number of rolls before a player would win...\n", "A": "I think I've found the answer for the single player case:\nIf we write $e_{i}$ for the expected remaining length of the game if $i$ cards are facedown, then we can work out that:\n(i). $e_{5} = \\frac{1}{6}(1) + \\frac{5}{6}(e_{4} + 1)$\n(ii). $e_{4} = \\frac{2}{6}(e_{5} + 1) + \\frac{4}{6}(e_{3} + 1)$\n(iii). $e_{3} = \\frac{3}{6}(e_{4} + 1) + \\frac{3}{6}(e_{2} + 1)$\n(iv). $e_{2} = \\frac{4}{6}(e_{3} + 1) + \\frac{2}{6}(e_{1} + 1)$\n(v). $e_{1} = \\frac{5}{6}(e_{2} + 1) + \\frac{1}{6}(e_{0} + 1)$\n(vi). $e_{0} = \\frac{6}{6}(e_{1} + 1)$\n(vi) and (v) then give us (vii). $e_{1} = e_{2} + \\frac{7}{5}$;\n(vii) and (iv) then give us (viii). $e_{2} = e_{3} + \\frac{11}{5}$;\n(viii) and (iii) then give us (ix). $e_{3} = e_{4} + \\frac{21}{5}$;\n(ix) and (ii) then give us (x). $e_{4} = e_{5} + \\frac{57}{5}$;\n(x) and (i) then give us $e_{5} = 63 $\nWe can then add up to get $e_{0} = 63 + \\frac{57}{5} + \\frac{21}{5} + \\frac{11}{5} + \\frac{7}{5} + 1 = 83.2$.\nNow, how would one generalize this to find the expected length of game with $n$ players?\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/473444", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "16", "answer_count": 1, "answer_id": 0}}
{"Q": "Deriving posterior from a single observation z from a normal distribution (ESL book) I am reading the book The Elements of Statistical Learning by Hastie, Tibshirani and Friedman.\nOn page 271 the authors derive a posterior distribution from a single observation $z\\sim N(\\theta, 1)$, where the prior of $\\theta$ is specified as $\\theta \\sim N(0, \\tau)$.  It then follows (according to the authors) that the posterior distribution equals $\\theta | z \\sim N\\left( \\frac{z}{1+\\frac{1}{\\tau}}, \\frac{1}{1+\\frac{1}{\\tau}} \\right).$\nNow, my calculations yield\n\\begin{align}\n\\Pr\\left(\\theta |\\textbf{Z}\\right) \n&= \n\\frac{\\Pr\\left(\\textbf{Z} \\mid \\theta\\right) \\Pr(\\theta)}{\\int \\Pr\\left(\\textbf{Z} \\mid \\theta\\right) \\Pr(\\theta)d\\theta} \n\\propto\n\\Pr\\left(\\textbf{Z} \\mid \\theta\\right) \\Pr(\\theta) \\\\\n&=\n\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(z-\\theta)^2 \\right) \n\\frac{1}{\\sqrt{2\\pi\\tau}}\\exp\\left(-\\frac{1}{2\\tau}\\theta^2 \\right) \\\\\n&= \n\\frac{1}{2\\pi\\sqrt{\\tau}}\\exp\\left(-\\frac{1}{2} (z^2 + \\theta^2 -2z\\theta + \\frac{\\theta ^2}{\\tau}) \\right) \\\\\n&= \n\\frac{1}{2\\pi\\sqrt{\\tau}}\\exp\\left(-\\frac{1}{2} (\\theta^2(1+\\frac{1}{\\tau}) +  z^2  -2z\\theta) \\right) \\\\\n&= \n\\frac{1}{2\\pi\\sqrt{\\tau}}\\exp\\left(-\\frac{1}{2 \\frac{1}{1+\\frac{1}{\\tau}}}\n(\\theta^2 +  \\frac{z^2}{1+\\frac{1}{\\tau}}  -2 \\frac{z\\theta}{1+\\frac{1}{\\tau}} ) \\right). \n\\end{align}\nThe denominator of $\\frac{z^2}{1+\\frac{1}{\\tau}} $ should equal  $(1+\\frac{1}{\\tau})^2$ for me to be able to \"complete the square\" and get\n\\begin{align}\n\\Pr\\left(\\theta |\\textbf{Z}\\right) \n&\\propto \n\\frac{1}{2\\pi\\sqrt{\\tau}}\\exp\\left(-\\frac{1}{2 \\frac{1}{1+\\frac{1}{\\tau}}}\n(\\theta^2 +  \\frac{z^2}{(1+\\frac{1}{\\tau})^2}  -2 \\frac{z\\theta}{1+\\frac{1}{\\tau}} ) \\right) \\\\\n&=\\text{constant}\\times\\exp\\left(-\\frac{1}{2 \\frac{1}{1+\\frac{1}{\\tau}}}\n(\\theta - \\frac{z}{1+\\frac{1}{\\tau}})^2 \\right), \n\\end{align}\nsuch that  $\\theta | z \\sim N\\left( \\frac{z}{1+\\frac{1}{\\tau}}, \\frac{1}{1+\\frac{1}{\\tau}} \\right)$.\nMy question is:\nWhere do I go wrong in the process? Should I divide with $\\int \\Pr\\left(\\textbf{Z} \\mid \\theta\\right) \\Pr(\\theta)d\\theta = \\Pr(\\textbf{Z})$? If so, what is the difference between $\\Pr(\\textbf{Z})$ and $\\Pr\\left(\\textbf{Z} \\mid \\theta\\right)$ in this given example?\nBest regards,\nwanderingashenvalewisp\n", "A": "Since we're looking for the pdf of $\\theta$, we're only concerned with terms that include it.\n\\begin{align}\n\\Pr\\left(\\theta |\\textbf{Z}\\right) \n&\\propto \\Pr\\left(\\textbf{Z} \\mid \\theta\\right) \\Pr(\\theta) \\\\\n&\\propto \\exp\\left(-\\frac{1}{2}(z-\\theta)^2 -\\frac{1}{2\\tau}\\theta^2 \\right) \\\\\n&= \\exp\\left(-\\frac{1}{2}\\left((1+\\frac{1}{\\tau})\\theta^2 -2z\\theta+z^2 \\right)\\right)\\\\\n&= \\exp\\left(-\\frac{1}{2}(1+\\frac{1}{\\tau})\\left(\\theta^2 -2\\frac{z}{1+\\frac{1}{\\tau}}\\theta+\\frac{z^2}{1+\\frac{1}{\\tau}} \\right)\\right)\\\\\n&\\propto \\exp\\left(-\\frac{1}{2}(1+\\frac{1}{\\tau})\\left(\\theta - \\frac{z}{1+\\frac{1}{\\tau}} \\right)^2\\right)\n\\end{align}\nAnd that last line implies the desired result.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/501858", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Suppose $X \\mid Y$ and $Y$ are normally distributed. Does it follow that $Y \\mid X$ is normally distributed? Suppose $X \\mid Y$ and $Y$ are normally distributed. Does it follow that $Y \\mid X$ is normally distributed?\nAnd if so, how would one prove this?\n", "A": "Here I will augment the excellent answer by whuber by showing the mathematical form of your general model and the sufficient conditions that imply a normal distribution for $Y|X$.  Consider the general hierarchical model form:\n$$\\begin{align}\nX|Y=y \n&\\sim \\text{N}(\\mu(y),\\sigma^2(y)), \\\\[6pt]\nY \n&\\sim \\text{N}(\\mu_*,\\sigma^2_*). \\\\[6pt]\n\\end{align}$$\nThis model gives the joint density kernel:\n$$\\begin{align}\nf_{X,Y}(x,y)\n&= f_{X|Y}(x|y) f_{Y}(y) \\\\[12pt]\n&\\propto \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Big( \\frac{x-\\mu(y)}{\\sigma(y)} \\Big)^2 \\Bigg) \\exp \\Bigg( -\\frac{1}{2} \\Big( \\frac{y-\\mu_*}{\\sigma_*} \\Big)^2 \\Bigg) \\\\[6pt]\n&= \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\Big( \\frac{x-\\mu(y)}{\\sigma(y)} \\Big)^2 + \\Big( \\frac{y-\\mu_*}{\\sigma_*} \\Big)^2 \\Bigg] \\Bigg) \\\\[6pt]\n&= \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(x-\\mu(y))^2 \\sigma_*^2 + (y-\\mu_*)^2 \\sigma(y)^2}{\\sigma(y)^2 \\sigma_*^2} \\Bigg] \\Bigg) \\\\[6pt]\n&\\overset{y}{\\propto} \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(\\mu(y)^2 - 2x \\mu(y)) \\sigma_*^2 + (y^2-2y\\mu_* + \\mu_*^2) \\sigma(y)^2}{\\sigma(y)^2 \\sigma_*^2} \\Bigg] \\Bigg), \\\\[6pt]\n\\end{align}$$\nwhich gives the conditional density kernel:\n$$\\begin{align}\nf_{Y|X}(y|x)\n&\\overset{y}{\\propto} \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(\\mu(y)^2 - 2x \\mu(y)) \\sigma_*^2 + (y^2-2y\\mu_* + \\mu_*^2) \\sigma(y)^2}{\\sigma(y)^2 \\sigma_*^2} \\Bigg] \\Bigg). \\\\[6pt]\n\\end{align}$$\nIn general, this is not the form of a normal density.  However, suppose we impose the following conditions on the condtional mean and variance of $X|Y$:\n$$\\mu(y) = a + by\n\\quad \\quad \\quad \\quad \\quad \n\\sigma^2(y) = \\sigma^2.$$\nThese conditions mean that we require $\\mu(y) \\equiv \\mathbb{E}(X|Y=y)$ to be an affine function of $y$ and we require $\\sigma^2(y) \\equiv \\mathbb{V}(X|Y=y)$ to be a fixed value.  Incorporating these conditions gives:\n$$\\begin{align}\nf_{Y|X}(y|x)\n&\\overset{y}{\\propto} \\frac{1}{\\sigma(y)} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(\\mu(y)^2 - 2x \\mu(y)) \\sigma_*^2 + (y^2-2y\\mu_* + \\mu_*^2) \\sigma(y)^2}{\\sigma(y)^2 \\sigma_*^2} \\Bigg] \\Bigg) \\\\[6pt]\n&= \\frac{1}{\\sigma} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{((a + by)^2 - 2x (a + by)) \\sigma_*^2 + (y^2-2y\\mu_* + \\mu_*^2) \\sigma^2}{\\sigma^2 \\sigma_*^2} \\Bigg] \\Bigg) \\\\[6pt]\n&= \\frac{1}{\\sigma} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(b^2 y^2 + 2ab y + a^2 b^2 - 2xa - 2xb y) \\sigma_*^2 + (y^2-2y\\mu_* + \\mu_*^2) \\sigma^2}{\\sigma^2 \\sigma_*^2} \\Bigg] \\Bigg) \\\\[6pt]\n&\\overset{y}{\\propto} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{(\\sigma^2 + b^2 \\sigma_*^2 ) y^2 + 2(b(a - x) \\sigma_*^2 - \\mu_* \\sigma^2) y}{\\sigma^2 \\sigma_*^2} \\Bigg] \\Bigg) \\\\[6pt]\n&\\overset{y}{\\propto} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{y^2 + 2[(b(a - x) \\sigma_*^2 - \\mu_* \\sigma^2)/(\\sigma^2 + b^2 \\sigma_*^2) ] y}{\\sigma^2 \\sigma_*^2/(\\sigma^2 + b^2 \\sigma_*^2 ) } \\Bigg] \\Bigg) \\\\[6pt]\n&\\overset{y}{\\propto} \\cdot \\exp \\Bigg( -\\frac{1}{2} \\Bigg[ \\frac{1}{\\sigma^2 \\sigma_*^2/(\\sigma^2 + b^2 \\sigma_*^2 )} \\cdot \\Big( y - \\frac{b(a - x) \\sigma_*^2 - \\mu_* \\sigma^2}{\\sigma^2 + b^2 \\sigma_*^2} \\Big)^2 \\Bigg] \\Bigg) \\\\[6pt]\n&\\overset{y}{\\propto} \\text{N} \\Bigg( y \\Bigg| \\frac{b(a - x) \\sigma_*^2 - \\mu_* \\sigma^2}{\\sigma^2 + b^2 \\sigma_*^2}, \\frac{\\sigma^2 \\sigma_*^2}{\\sigma^2 + b^2 \\sigma_*^2} \\Bigg). \\\\[6pt]\n\\end{align}$$\nHere we see that we have a normal distribution for $Y|X$ which confirms that the above conditions on the conditional mean and variance of $X|Y$ are sufficient to give this property.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/602428", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 4, "answer_id": 1}}
