{"Q": "Poisson process - calls arriving Already posted on MSE.  Had no answer, so will post here.\nAssume the number of calls per hour arriving at an answering service follows a Poisson process with $\\lambda = 4$.   \nQuestion: If it is know that $8$ calls came in the first two hours.  What is the probability that exactly five arrived in the first hour?\nAttempt: Isn't this just a combinatorial question?  So the answer is ${8 \\choose 5}/2^8$\n", "A": "Thinking this through, I believe this should be calculated with a binomial distribution with $n = 8$ and $p = 0.5$ as follows:\n$P = \\binom{8}{5} \\cdot 0.5^{5} \\cdot (1-0.5)^{3} $\nLet me try to proof this:\nLet\n$X_1$ = number of calls that arrive in the first hour \n$X_2$ = number of calls that arrive in the second hour\n$X_3$ = number of calls that arrive in the two hours \nWhat you want to calculate is the conditional probability of 5 calls arriving in the first hour given that 8 calls arrived in two hours:\n$P(X_1 = 5 | X_3 = 8) = \\frac {P[(X_1 = 5) \\cap (X_3 = 8)]} {P(X_3 = 8)}$\nThis would be equivalent to : $\\frac {P[(X_1 = 5) \\cap (X_2 = 3)]} {P(X_3 = 8)}$, however now the events occur over non overlapping time frames which allow us to use the independent increment property of the poisson processes.\n$\\frac {P[(X_1 = 5) \\cap (X_2 = 3)]} {P(X_3 = 8)} = \\frac {P(X_1 = 5) \\cdot P(X_2 = 3)]} {P(X_3 = 8)}$\n$           =\\frac {\\left[\\frac {e^{-4} \\cdot 4^5} {5!} \\right] \\cdot \\left[\\frac {e^{-4} \\cdot 4^3} {3!} \\right]} {\\frac {e^{-(4 \\cdot 2)} \\cdot {(4 \\cdot 2)}^8} {8!}} $\n$=\\frac{8!} {5! \\cdot 3!} \\frac {(4^5) \\cdot (4^3)} {8^8} $\n$=\\frac{8!} {5! \\cdot 3!} \\frac {(4^5) \\cdot (4^3)} {(8^5) \\cdot (8^3)} $\n$=\\frac{8!} {5! \\cdot 3!} \\cdot \\left(\\frac {4} {8}\\right)^5 \\cdot \\left(\\frac {4} {8}\\right)^3$\n$= \\binom{8}{5} \\cdot 0.5^{5} \\cdot (0.5)^{3} $\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/74838", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 0}}
{"Q": "Regular Transition Matrix \nA transition matrix is regular if some power of the matrix contains\nall positive entries. [1]\n\n\nIts powers have all positive entries...\n\nWhy isn't this matrix a Regular Transition Matrix?\nReference:\n[1] http://fazekas-andras-istvan.hu/9_11_markov_lancok/DFAI_MARKOV_CHAINS_02.pdf\n", "A": "It can be shown that (either by theoretical derivation or using Matlab) $B$ has the Jordan canonical form decomposition as follows:\n$$B = PJP^{-1},$$\nwhere \n\\begin{align*}\nP = \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix},\n\\end{align*}\n\\begin{align*}\nJ = \\begin{bmatrix}\n0.5 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\end{align*}\nHence for each natural number $n$, we have:\n\\begin{align*}\nB^n = & (PJP^{-1})^n = PJ^nP^{-1} \\\\\n= & \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix}\n\\begin{bmatrix}\n0.5^n & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix} \\\\\n= &\n\\begin{bmatrix}\n0.5^n & 0 & 1 - 0.5^n \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix}.\n\\end{align*}\nIt thus can be seen that not every entry of $B^n$ is positive, hence $B$ is not a regular transition matrix.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/253931", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "How to put an ARMA(2,2) model in state-space form I am interested in an ARMA$(2,2)$ model with an additional input variable, which I want to put in state-space form.   If $w_t$ is white noise, and $x_t$ is a known input, the model is given by:\n$$y_t = \\beta_0 + \\beta_1 \\cdot x_{t-1} + \\alpha_1 \\cdot y_{t-1} + \\alpha_2 \\cdot y_{t-2} + w_t + \\theta_1 \\cdot w_{t-1} + \\theta_2 \\cdot w_{t-2}.$$\nCan someone please show how to write this in state-space form?\n", "A": "One way to do it is to define the state vector as\n$$\n\\xi_t = \\begin{pmatrix}\ny_t \\\\\ny_{t-1} \\\\\nw_{t} \\\\\nw_{t-1} \\\\\n1 \\\\\n\\end{pmatrix}\n$$\nThe measurement equation is just\n$$\ny_t = \\begin{pmatrix}\n1 & 0 & 0 & 0   & 0\n\\end{pmatrix} \\, \\xi_t\n$$\ni.e. there is no noise term. The state transition equation is then\n$$\n\\underbrace{\\begin{pmatrix}\ny_t \\\\\ny_{t-1} \\\\\nw_{t} \\\\\nw_{t-1} \\\\\n1 \\\\\n\\end{pmatrix}}_{\\xi_t}\n = \n\\begin{pmatrix}\n\\alpha_1 & \\alpha_2 & \\theta_1 & \\theta_2 & \\beta_0+\\beta_1 x_{t-1} \\\\\n1 & 0 & 0 & 0 & 0  \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\\underbrace{\\begin{pmatrix}\ny_{t-1} \\\\\ny_{t-2} \\\\\nw_{t-1} \\\\\nw_{t-2} \\\\\n1 \\\\\n\\end{pmatrix}}_{\\xi_{t-1}}\n+\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n\\end{pmatrix} w_t\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/338910", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "How to calculate the Transposed Convolution? Studying for my finals in Deep learning. I'm trying to solve the following question:\n\nCalculate the Transposed Convolution of input $A$ with kernel $K$:\n$$\nA=\\begin{pmatrix}1 & 0 & 1\\\\\n0 & 1 & 0\\\\\n1 & 0 & 1\n\\end{pmatrix},\\quad K=\\begin{pmatrix}1 & 0\\\\\n1 & 1\n\\end{pmatrix}\n$$\n\nI can't seem to find the formula which is used to calculate the Transposed Convolution (found only the formula to calculate the dimension). I know that the Convolution formula is:\n$$\nG(i,j)=\\sum_{u=-k}^{k}\\sum_{v=-k}^{k}K(u,v)A(i-u,j-v)\n$$\nBut how to calculate the Transposed Convolution?\nIn a video I saw the following example:\n\nWhich is easy for $2\\times2$ kernel and image to see that:\n$$\n\\begin{align*}\n&K_{0,0}\\star^{T}A=2\\star^{T}\\begin{pmatrix}3 & 1\\\\\n1 & 5\n\\end{pmatrix}=\\begin{pmatrix}6 & 2 & 0\\\\\n2 & 10 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}&&K_{0,1}\\star^{T}A=0\\star^{T}\\begin{pmatrix}3 & 1\\\\\n1 & 5\n\\end{pmatrix}=\\begin{pmatrix}0 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}\\\\&K_{0,1}\\star^{T}A=4\\star^{T}\\begin{pmatrix}3 & 1\\\\\n1 & 5\n\\end{pmatrix}=\\begin{pmatrix}0 & 12 & 4\\\\\n0 & 4 & 20\\\\\n0 & 0 & 0\n\\end{pmatrix}&&K_{1,1}\\star^{T}A=1\\star^{T}\\begin{pmatrix}3 & 1\\\\\n1 & 5\n\\end{pmatrix}=\\begin{pmatrix}0 & 0 & 0\\\\\n0 & 3 & 1\\\\\n0 & 1 & 5\n\\end{pmatrix}\n\\end{align*}\n$$\nThen you have:\n$$\nA'=\\begin{pmatrix}6 & 2 & 0\\\\\n2 & 10 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}+\\begin{pmatrix}0 & 12 & 4\\\\\n0 & 4 & 20\\\\\n0 & 0 & 0\n\\end{pmatrix}+\\begin{pmatrix}0 & 0 & 0\\\\\n0 & 0 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}+\\begin{pmatrix}0 & 0 & 0\\\\\n0 & 3 & 1\\\\\n0 & 1 & 5\n\\end{pmatrix}=\\begin{pmatrix}6 & 14 & 4\\\\\n2 & 17 & 21\\\\\n0 & 1 & 5\n\\end{pmatrix}\n$$\nBut I can't seem to figure how to make it for $3\\times 3$ image and $2\\times 2$ kernel.\nI do know that the dim of the output should be $4\\times 4$ because:\n$$\n\\begin{cases}\nH=(3-1)\\cdot1+2-2\\cdot0=4\\\\\nW=(3-1)\\cdot1+2-2\\cdot0=4\n\\end{cases}\n$$\n", "A": "In 3x3 images, just like in 2x2, each pixel of the input is multiplied with the kernel matrix and accumulated. A naive implementation is given by this page:\ndef trans_conv(X, K):\n    h, w = K.shape\n    Y = np.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Y[i: i + h, j: j + w] += X[i, j] * K\n    return Y\n\nSo, every $h\\times w$ block of output matrix starting at $y_{ij}$ is accumulated with $K\\times x_{ij}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/587634", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
