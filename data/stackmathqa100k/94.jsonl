{"Q": "How to compute the standard error of the mean of an AR(1) process? I try to compute the standard error of the mean for a demeaned AR(1) process $x_{t+1} = \\rho x_t + \\varepsilon_{t+1} =\\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t+1-i}$\nHere is what I did:\n$$\n\\begin{align*}\nVar(\\overline{x}) &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} x_t\\right) \\\\\n                  &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} \\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t-i}\\right) \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_0 +  & \\rho^1 \\varepsilon_{-1} +  & \\rho^2 \\varepsilon_{-2} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{-\\infty} + \\\\ \n                                                      \\rho^0  \\varepsilon_1 +  & \\rho^1 \\varepsilon_{0}  +   & \\rho^2 \\varepsilon_{-1} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{1-\\infty} + \\\\\n                                                      \\vdots               & \\vdots                 & \\vdots                & \\ddots  & \\vdots \\\\\n                                                      \\rho^0\\varepsilon_{N-1} + & \\rho^1 \\varepsilon_{N-2} +  & \\rho^2 \\varepsilon_{N-3} + & \\cdots  & \\rho^{\\infty} \\varepsilon_{N-1-\\infty} + \\\\\n                                      \\end{pmatrix} \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_{N-1} + \\\\ \n                                                      (\\rho^0 + \\rho^1)  \\varepsilon_{N-2} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  \\varepsilon_{N-3} + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  \\varepsilon_{1} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  \\varepsilon_{0} + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})  \\varepsilon_{-1} + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  \\varepsilon_{-2} + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                 &= \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^0   + \\\\ \n                                                      (\\rho^0 + \\rho^1)  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                &= \\frac{N \\sigma_{\\varepsilon}^2}{N^2} (\\rho^0 + \\rho^1 + \\dots + \\rho^{\\infty}) \\\\\n                &= \\frac{\\sigma_{\\varepsilon}^2}{N} \\frac{1}{1 - \\rho} \\\\\n\\end{align*}\n$$\nProbably, not every step is done in the most obvious way, so let me add some thoughts. In the third row, I just write out to two sum-signs. Here, the matrix has N rows. In the fourth row, I realign the matrix so that there is one row for every epsilon, so the number of rows is infinite here. Note that the last three parts in the matrix have the same number of elements, just differencing by a factor $\\rho$ in each row. In the fifth row, I apply the rule that the variance of the sum of independent shocks is the sum of the variances of those shocks and notice that each $\\rho^j$ element is summed up $N$ times.\nThe end result looks neat, but is probably wrong. Why do I think so? Because I run a MCS in R and things don't add up:\nnrMCS <- 10000\nN <- 100\npers <- 0.9\nmeans <- numeric(nrMCS)\nfor (i in 1:nrMCS) {\n   means[i] <- mean(arima.sim(list(order=c(1,0,0), ar=pers), n = N))\n}\n#quantile(means, probs=c(0.025, 0.05, 0.5, 0.95, 0.975))\n#That is the empirical standard error\nsd(means)\n0.9459876\n#This should be the standard error according to my formula\n1/(N*(1-pers))\n0.1\n\nAny hints on what I am doing wrong would be great! Or maybe a hint where I can find the correct derivation (I couldn't find anything). Is the problem maybe that I assume independence between the same errors?\n$$Var(X + X) = Var(2X) = 4Var(X) \\neq 2Var(X)$$\nI thought about that, but don't see where I make that erroneous assumption in my derivation.\nUPDATE\nI forgot to square the rhos, as Nuzhi correctly pointed out. Hence it should look like:\n$$                Var(\\overline{x}) = \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^{2\\times0}   + \\\\ \n                                                      (\\rho^0 + \\rho^1)^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)^2  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})^2  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})^2    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})^2  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} $$\n", "A": "Don't know if it qualifies as a formal answer, but on the simulation side, standard error of a means estimator is defined as est(sd(means))/sqrt(N), which would give:\n> .9459876/sqrt(100)\n[1] 0.09459876\n\nNot sure why you were using sd(means) and calling it standard error (if I understood the code comment right).  It would make more sense to call the value SE(u) rather than Var(u) in the derivation as well, since I think that's what you intended?  \nThe variance of an AR(1) process is the variance of the error term divided by (1-phi^2), where you had N*(1-phi) (the N wouldn't be there if it was just variance).\n.\nI'll have to dig deeper to try to find a derivation of that.\nvarianceAR1 simple AR(1) derivation on p. 36\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/40585", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 4, "answer_id": 3}}
{"Q": "Finding the PMF of conditional probability, poisson process. Don't understand where $10^6$ goes \"Customers arrive at a bank according to a Poisson process with rate 6 per hour.\nState (together with a proof) clearly the (conditional) probability mass function of the numbers of customers arrived during the first 20 minutes, given that 10 customers have arrived during the first hour.\"\nI did this:\n$$\nP(X_{\\frac{1}{3}} = x | X_1 = 10)\n$$\nFrom the definition of joint probability, we know\n$$\nP(X_{\\frac{1}{3}} = x , X_1 = 10) = P(X_{\\frac{1}{3}} = x | X_1 = 10) P(X_1 = 10)$$\nRe-aranging gives:\n$$\nP(X_{\\frac{1}{3}} = x | X_1 = 10) = \\frac{ P(X_{\\frac{1}{3}} = x , X_1 = 10)} {P(X_1 = 10)}$$\nLet's first calculate $P(X_{\\frac{1}{3}} = x , X_1 = 10)$:\nThis becomes:\n$$\nP(X_{\\frac{1}{3}} = x , X_{\\frac{2}{3}}= y) = P(X_{\\frac{1}{3}} = x) P(X_{\\frac{2}{3}} = y) \\hspace{1cm} y = 10 - x $$\nUsing the Poisson formula, you get:\n$$P(X_{\\frac{1}{3}} = x) = \\frac{e^{-2}2^x}{x!} \\hspace 2cm P(X_{\\frac{2}{3}}= y) = \\frac{e^{-4}4^y}{y!}$$\nAnd so \n$$\nP(X_{\\frac{1}{3}} = x , X_{\\frac{2}{3}}= y) = e^{-6} \\frac{2^x4^y}{x!y!}\n$$\nAlso, using the Poisson formula, we get:\n$$\nP(X_1 = 10) = \\frac{e^{-6}6^{10}}{10!}\n$$\nSo \n$$\\frac{ P(X_{\\frac{1}{3}} = x , X_1 = 10)} {P(X_1 = 10)} = \\frac{e^{-6} \\frac{2^x4^y}{x!y!}}{\\frac{e^{-6}6^{10}}{10!}}\n$$\nWhich I get to be\n$$\n\\frac{10! 2^x 4^y}{x!y! 6^{10}}$$\nBut in the answers, it says it should be\n$$\n\\binom{10}{x}  \\left( \\frac{1}{3} \\right)^x \\left( \\frac{2}{3} \\right)^{10 - x}\n$$\nWhy is this?\n", "A": "I assume in your question you mean $6^{10}$ as opposed to $10^6$. So, let's start with what you have:\n$$\\frac{10! 2^x 4^{10-x}}{x!(10-x)!6^{10}} = \\frac{10!}{x!(10-x)!}\\times\\frac{2^x 4^{10-x}}{6^{10}}.$$\nThe first term is $\\binom{10}{x}.$ Let's pull $2^{10}$ out of the top and bottom of the second term and cancel them, leaving us with\n$$\\binom{10}{x} \\frac{1^x 2^{10-x}}{3^{10}} = \\binom{10}{x} \\frac{1^x}{3^x} \\frac{2^{10-x}}{3^{10-x}} = \\binom{10}{x}\\left(\\frac13\\right)^x\\left(\\frac23\\right)^{10-x}$$\nwhich is the answer you want.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/46241", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "What's the probability that as I roll dice I'll see a sum of $7$ on them before I see a sum of $8$? This question is from DEGROOT's PROBABILITY and STATISTICS.\nProblem\nSuppose that two dice are to be rolled repeatedly and the\nsum $T$ of the two numbers is to be observed for each roll. We shall determine the\nprobability $p$ that the value $T =7$ will be observed before the value $T =8$ is observed.\nSolution\nThe desired probability $p$ could be calculated directly as follows: We could assume that the sample space $S$ contains all sequences of outcomes that terminate as\nsoon as either the sum $T = 7$ or the sum $T = 8$ is obtained. Then we could find the\nsum of the probabilities of all the sequences that terminate when the value $T = 7$ is\nobtained.\nHowever,there is a simpler approach in this example. We can consider the simple\nexperiment in which two dice are rolled. If we repeat the experiment until either the\nsum $T = 7$ or the sum $T = 8$ is obtained, the effect is to restrict the outcome of the\nexperiment to one of these two values. Hence, the problem can be restated as follows:\nGiven that the outcome of the experiment is either $T = 7$ or $T = 8$, determine the\nprobability $p$ that the outcome is actually $T = 7$.\nIf we let $A$ be the event that $T = 7$ and let $B$ be the event that the value of $T$ is\neither $7$ or $8$, then $A \u2229 B = A$ and\n$$\np = Pr(A|B) = \\frac{Pr(A \u2229 B)}{Pr(B)}\n=\\frac{Pr(A)}{Pr(B)}\n$$\nBut $Pr(A) = 6/36$ and\n$Pr(B) = (6/36) + (5/36) = 11/36$. Hence, $p = 6/11$.\nNow, my doubts are\n\n*\n\n*Why does the author say \n\nWe could assume that the sample space $S$ contains all sequences of outcomes that terminate as\n  soon as either the sum $T = 7$ or the sum $T = 8$ is obtained. Then we could find the\n  sum of the probabilities of all the sequences that terminate when the value $T = 7$ is\n  obtained.\n  \n*How can we go from lengthy sequences of outcomes that terminate as\n  soon as either the sum $T = 7$ or the sum $T = 8$ is obtained to just the outcome of the experiment for which either $T = 7$ or $T = 8$ ?\n  \n\n", "A": "Question 1\n\n\n*\n\n*Why does the author say \n\nWe could assume that the sample space $S$ contains all sequences of outcomes that terminate as\n  soon as either the sum $T = 7$ or the sum $T = 8$ is obtained. Then we could find the\n  sum of the probabilities of all the sequences that terminate when the value $T = 7$ is\n  obtained.\n\nAnswer\nSample space $S$ has $m \\rightarrow \\infty$ sequences of length $n \\rightarrow \\infty$ that end in either $7$ or $8$. Out of these sequences we're interested in summing up the probabilities of all the series that end in a $7$. The probability of a sequence of precisely $n$ throws ending in a $7$ is:\n$$\nP_n(7) = \\left(\\frac{25}{36}\\right)^{n-1} \\cdot \\frac{6}{36}\n$$\nHowever, since $n$ can take any value up to infinity, the overall probability of ending a sequence of any length in a $7$ is the sum of the prob. of ending a seq. after one throw plus the prob. of ending a sequence after two throws, and so on. This is the geometric series:\n$$\n\\Phi_7 = P_1(7) + P_2(7) + P_3(7) + ... + P_n(7)\n$$\nwhich, as $n \\rightarrow \\infty$, sums up to (basic geometric sum formula)\n$$\n\\Phi_7 = \\lim_{n \\rightarrow \\infty} \\frac{\\frac{6}{36}\\left(1-\\left(\\frac{25}{36}\\right)^n\\right)}{1-\\frac{25}{36}} = \\lim_{n \\rightarrow \\infty} \\frac{6}{11}\\left(1-\\left(\\frac{25}{36}\\right)^n\\right) = \\frac{6}{11}\n$$\nThis is the probability of ending a sequence of throws in a $7$ without ever hitting $8$. It's the answer you're looking for using the first, \"more complicated\" method.\nQuestion 2\n\n\n*\n\n*How can we go from lengthy sequences of outcomes that terminate as\nsoon as either the sum $T = 7$ or the sum $T = 8$ is obtained to just the outcome of the experiment for which either $T = 7$ or $T = 8$ ?\n\n\nAnswer\nThis will become clear if we rephrase the first method a little bit. Sample space $S$ has $m \\rightarrow \\infty$ sequences of length $n \\rightarrow \\infty$ which end in either a $7$ or an $8$. The probability of you running a sequence of length $n$ which ends in $7$ is the probability\n$$\nP_n(7)|(P_n(7) \\cup P_n(8)) = \\frac{P_n(7) \\cap (P_n(7)\\cup P_n(8))}{P_n(7)\\cup P_n(8)} = \\frac{P_n(7)}{P_n(7) \\cup P_n(8)}\n$$\n$$\n\\frac{P_n(7)}{P_n(7) \\cup P_n(8)} = \\frac{\\left(\\frac{25}{36}\\right)^{n-1} \\cdot \\frac{6}{36}}{\\left(\\frac{25}{36}\\right)^{n-1} \\cdot \\frac{6}{36} + \\left(\\frac{25}{36}\\right)^{n-1} \\cdot \\frac{5}{36}} = \\frac{6}{11}\n$$\nThis is a lot of LaTeX for not a very impressive statement but it is useful because we can now use it to prove by induction the jump from a sequence of length $n$ to a sequence of length $1$. If we run the same formula for $n-1$ we get\n$$\nP_{n-1}(7)|(P_{n-1}(7) \\cup P_{n-1}(8)) = \\frac{P_{n-1}(7)}{P_{n-1}(7) \\cup P_{n-1}(8)}\n$$\nwhere \n$$\n\\frac{P_{n-1}(7)}{P_{n-1}(7) \\cup P_{n-1}(8)} = \\frac{\\left(\\frac{25}{36}\\right)^{n-2} \\cdot \\frac{6}{36}}{\\left(\\frac{25}{36}\\right)^{n-2} \\cdot \\frac{6}{36} + \\left(\\frac{25}{36}\\right)^{n-2} \\cdot \\frac{5}{36}} = \\frac{6}{11}\n$$\nBut this means that\n$$\n\\frac{P_n(7)}{P_n(7) \\cup P_n(8)} = \\frac{P_{n-1}(7)}{P_{n-1}(7) \\cup P_{n-1}(8)}\n$$\nand it follows, by induction, that\n$$\n\\frac{P_n(7)}{P_n(7) \\cup P_n(8)} = \\frac{P_{1}(7)}{P_{1}(7) \\cup P_{1}(8)}\n$$\nTherefore, whatever value $n$ takes, the probability of a sequence of that length ending in $7$ given that it ends in either $7$ or $8$ is equal to the probability of a sequence of length $1$ ending in $7$ given it is a part of $S$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/71783", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Derivation of $\\frac{\\sum (x - \\bar{x})^2}{N} = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{N}}{N}$ I saw the above equation in an introductory statistics textbook, as a shortcut for evaluating the variance of a population.\nI tried to prove it myself:\n\u00a0\n\u00a0\n$$\\sigma^2 = \\frac{\\sum (x - \\bar{x})^2}{N} \\tag{1}$$\n$$\\sigma^2 = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{N}}{N} \\tag{2}$$\nWe are given that $(1) = (2)$:\n$$\\frac{\\sum (x - \\bar{x})^2}{N} = \\frac{\\sum x^2 - \\frac{(\\sum x)^2}{N}}{N} \\tag{3}$$\nMultiply $(3)$ through by $N$:\n$$\\sum(x - \\bar{x})^2 = \\sum x^2 - \\frac{(\\sum x)^2}{N} \\tag{4}$$\nExpand the LHS in $(4)$:\n$$\\sum\\left(x^2 - 2x\\bar{x} + \\bar{x}^2\\right) = {\\sum x^2 - \\frac{(\\sum x)^2}{N}} \\tag{5}$$\nExpanding both sides in $(5)$:\n$$\\sum x^2 - 2x\\sum\\bar{x} + \\sum\\bar{x}^2 = \\sum x^2 - \\frac{\\sum x\\sum x}{N} \\tag{6}$$\nFrom $(6)$:\n$$\\sum\\bar{x}^2 - 2\\bar{x}\\sum{x} = -\\bar{x}\\sum{x} \\tag{7}$$\nFrom $(7)$:\n$$\\sum\\bar{x}^2 = \\bar{x}\\sum{x} \\tag{8}$$\nI don't know how to make the LHS equal RHS in $(8)$.\n", "A": "Starting from what you know:\n$\\sigma^2 =\\dfrac{\\sum (x - \\bar{x})^2}{N} $\n$= \\dfrac{ \\sum\\left(x^2 - 2x\\bar{x} +\\bar{x}^2 \\right)}{N}$\n$= \\dfrac{\\sum x^2}N  - \\dfrac{2\\sum x\\bar{x}}N  +\\dfrac{\\sum\\bar{x}^2}N  $\n$=\\dfrac{\\sum x^2}N - 2\\bar{x}\\dfrac{\\sum x}{N} + \\bar{x}^2 $\n$=\\dfrac{\\sum x^2}N - {2\\bar{x}^2} + \\bar{x}^2 $\n$= \\dfrac{\\sum x^2}N - {\\bar{x}^2} $\n$= \\dfrac{\\sum x^2}N - \\dfrac {\\sum{\\bar{x}^2}}N $\n$= \\dfrac{\\sum (x^2 - \\bar{x}^2) }N $\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/288517", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 1}}
{"Q": "urn with two colors of balls; probability of selecting specific color 10 balls in an urn, 6 Black and 4 White. Three are removed, color not noted. What is probability that a white ball will be chosen next?\nThe answer is 2/5, so my reasoning below must be faulty.\nAfter the initial three balls are removed, there will be 4 possible configurations:", "A": "You calculated $P(A)$, $P(B)$, $P(C)$ and $P(D)$ incorrectly. A can happen in $\\binom{6}{3} = 20$ ways, B in $\\binom{6}{2} * \\binom{4}{1} = 60$ ways, C can happen in $\\binom{6}{1} * \\binom{4}{2} = 36$ ways, D can happen in $4$ ways. To check, there are $20+60+36+4=120$ total ways of removing $3$ balls at random, which is $\\binom{10}{3}$.\nThe answer is then $\\frac{4}{7} * \\frac{20}{120} + \\frac{3}{7} * \\frac{60}{120} + \\frac{2}{7} * \\frac{36}{120} + \\frac{1}{7} * \\frac{4}{120} = \\frac{2}{5}$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/301320", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 4, "answer_id": 1}}
{"Q": "A simple probability problem There are $n$ applicants for the director of computing. The applicants are interviewed independently by each member of the three-person search committee and ranked from $1$ to $n$. A candidate will be hired if he or she is ranked first by at least two of the three interviewers. Find the probability that a candidate will be accepted if the members of the committee really have no ability at all to judge the candidates and just rank the candidates randomly. \nMy reasoning:\n\n\n*\n\n*There are $n \\cdot (n-1) \\cdot (n-2)$ many ways the interviewers put a different person at the first position.\n\n*So, $n^3 - n \\cdot (n-1) \\cdot (n-2)$ ways that at least two interviewers will put the same person at the first position\n\n*So, $\\frac{n^3 - n \\cdot (n-1) \\cdot (n-2)}{n^3} = \\frac{3n^2-2n}{n^3}$ is the desired probability\n\n\nBut the answer given is $\\frac{3n-2}{n^3}$\nWhere is the problem in my reasoning? thanks.\n", "A": "${3\\choose2} \\cdot 1/n \\cdot 1/n \\cdot (1 - 1/n)$ --this represents two out of the three committee members  selecting the same individual times the third committee member selecting a different person.\n${3\\choose3} \\cdot 1/n \\cdot  1/n \\cdot  1/n$ --this represents all three committee members selecting the same individual.\nAdd the two up:\n$$ \\array{\n&   & 3 \\cdot 1/n^2 \\cdot (n-1)/n + 1/n^3 & = \\\\\n& = & 3(n-1)/n^3 + 1/n^3 & = \\\\\n& = & (3n - 3)/n^3 + 1/n^3 & = \\\\\n& = & (3n - 3 + 1)/n^3 & = \\\\\n& = & (3n - 2)/n^3\n}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/301663", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 1}}
{"Q": "Least square regression weight estimation for $\\beta_1$ and $\\beta_2$? So, I have been looking at this post, and others that are similar and know that the least square estimation of $\\beta_1,\\beta_2$ will be $(X^TX)^{-1}X^TY$, where the model is $Y_i = \\beta_1x_{1i}+ \\beta_2x_{2i} + \\epsilon_i$. But what is the generally derived formula in this case for $\\hat{\\beta_1}$ and $\\hat{\\beta_2}$. How can represent them in a scalar multiplied out value/weight format rather than a matrix/vector format?\n", "A": "Based on the comments above (by @Sycorax):\n$$X^TX = \n\\begin{pmatrix} \nx_{11} & x_{12} & ... &x_{1n} \\\\\nx_{21} & x_{22} & ... &x_{2n}  \n\\end{pmatrix} \n\\begin{pmatrix} \nx_{11} & x_{21}  \\\\\nx_{12} & x_{22} \\\\\n\\vdots &\\vdots\\\\\nx_{1n} & x_{2n}\n\\end{pmatrix} \n= \n\\begin{pmatrix}\nx_{11}^2 + x_{12}^2 + \\:...\\: x_{1n}^2 & x_{11}x_{21} + x_{12}x_{22} + \\:...\\: +x_{1n}x_{2n} \\\\\nx_{11}x_{21} + x_{12}x_{22} + \\:...\\: +x_{1n}x_{2n} & x_{21}^2 + x_{22}^2 + \\:...\\: x_{2n}^2 \\\\ \n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum x_{1i}^2 & \\sum x_{1i}x_{2i}\\\\\n\\sum x_{1i}x_{2i} & \\sum x_{2i}^2 \\\\ \n\\end{pmatrix}$$\n$$(X^TX)^{-1} = \\dfrac{1}{\\sum x_{1i}^2 \\sum x_{2i}^2 - (\\sum x_{1i}x_{2i})^2}\n\\begin{pmatrix}\n\\sum x_{2i}^2 & - \\sum x_{1i}x_{2i} \\\\\n- \\sum x_{1i}x_{2i} & \\sum x_{1i}^2\n\\end{pmatrix}$$\n$$X^T Y = \\begin{pmatrix} \nx_{11} & x_{12} & ... &x_{1n} \\\\\nx_{21} & x_{22} & ... &x_{2n}  \n\\end{pmatrix} \n\\begin{pmatrix} \ny_{1}  \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{n} \n\\end{pmatrix} \n= \n\\begin{pmatrix} \nx_{11}y_1 + x_{12}y_2 + \\: ... \\: + x_{1n}y_n \\\\\nx_{21}y_1 + x_{22}y_2 + \\: ... \\: + x_{2n}y_n\n\\end{pmatrix}\n= \n\\begin{pmatrix} \n\\sum x_{1i}y_i \\\\\n\\sum x_{2i}y_i\n\\end{pmatrix}\n$$\n$$\\therefore \\beta = (X^TX)^{-1}X^TY =\n\\begin{pmatrix} \n\\hat{\\beta_1} \\\\\n\\hat{\\beta_2}\n\\end{pmatrix} \n=\n\\dfrac{1}{\\sum x_{1i}^2 \\sum x_{2i}^2 - (\\sum x_{1i}x_{2i})^2}\\begin{pmatrix} \n\\sum x_{2i}^2 \\sum x_{1i}y_i - \\sum x_{1i}x_{2i} \\sum x_{2i}y_i \\\\\n-\\sum x_{1i}x_{2i} \\sum x_{1i}y_i + \\sum x_{1i}^2 \\sum x_{2i}y_i \n\\end{pmatrix}\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/529529", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
