{"Q": "Convergence of random variables Trying to understand the solution given to this homework problem:\nDefine random variables $X$ and $Y_n$ where $n=1,2\\ldots%$ with probability mass functions:\n$$\nf_X(x)=\\begin{cases} \n\\frac{1}{2} &\\mbox{if } x = -1 \\\\ \n\\frac{1}{2} &\\mbox{if } x = 1  \\\\\n0 &\\mbox{otherwise}\n\\end{cases} and\\; f_{Y_n}(y)=\\begin{cases} \n\\frac{1}{2}-\\frac{1}{n+1} &\\mbox{if } y = -1 \\\\ \n\\frac{1}{2}+\\frac{1}{n+1} &\\mbox{if } y = 1  \\\\\n0 &\\mbox{otherwise}\n\\end{cases}\n$$\nNeed to show whether $Y_n$ converges to $X$ in probability.\nFrom this I can define the probability space $\\Omega=([0,1],U)$ and express the random variables as functions of indicator variables as such:\n$X = 1_{\\omega > \\frac{1}{2}} - 1_{\\omega < \\frac{1}{2}}$\nand\n$Y_n = 1_{\\omega < \\frac{1}{2}+\\frac{1}{n+1}} - 1_{\\omega > \\frac{1}{2}+\\frac{1}{n+1}}$\nAnd from the definition of convergence in probability, we need find to show that\n$P\\{|Y_n-X|>\\epsilon\\}$ does or does not converge to zero. Which can be written as:\n$P\\{|1_{\\omega < \\frac{1}{2}+\\frac{1}{n+1}} - 1_{\\omega > \\frac{1}{2}+\\frac{1}{n+1}} - 1_{\\omega > \\frac{1}{2}} + 1_{\\omega < \\frac{1}{2}}| > \\epsilon \\}\\;\\;(1)$\nNow it's easy to see that $\\epsilon < 2$ for this to hold, but the solution given states that:\n$P\\{|Y_n-X|>\\epsilon\\} = 1 - \\frac{1}{n+1} \\;\\; (2)$\nThus $Y_n$ does not converge in probability to $X$.\nMy problem is that I don't see the reasoning between (1) and (2). Can anyone shed some insight into intermediate steps/reasoning required to make this step?\n", "A": "You're told that\n$$\n  P(X=1)=P(X=-1)=1/2 \\, ,\n$$\nand\n$$\n  P(Y_n=1)=\\frac{1}{2} + \\frac{1}{n+1}  \\;\\;\\;, \\qquad P(Y_n=-1)=\\frac{1}{2} - \\frac{1}{n+1} \\;\\;\\;,\n$$\nfor $n\\geq 1$, and you're asked whether or not $Y_n$ converges to $X$ in probability, which means that\n$$\n  \\lim_{n\\to\\infty} P(|Y_n-X|\\geq \\epsilon) = 0 \\, , \\qquad (*)\n$$\nfor every $\\epsilon>0$.\nI will assume that $X$ is independent of the $Y_n$'s.\nIt is not the case that $Y_n$ converges in probability to $X$, because $(*)$ does not hold for every $\\epsilon>0$. \nFor instance, if we take $\\epsilon=1$, then\n$$\n  P(|Y_n-X|\\geq 1)=P(Y_n=1, X=-1) + P(Y_n=-1,X=1) \n$$\n$$\n  = P(Y_n=1)P(X=-1) + P(Y_n=-1)P(X=1) \n$$\n$$\n  = \\left(\\frac{1}{2} + \\frac{1}{n+1}\\right) \\cdot \\frac{1}{2} + \\left(\\frac{1}{2} - \\frac{1}{n+1}\\right) \\cdot \\frac{1}{2} = \\frac{1}{2} \\, ,\n$$\nfor every $n\\geq 1$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/40701", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 2, "answer_id": 0}}
{"Q": "How can I calculate $\\int^{\\infty}_{-\\infty}\\Phi\\left(\\frac{w-a}{b}\\right)\\phi(w)\\,\\mathrm dw$ Suppose $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ are density function and distribution function of the standard normal distribution.\nHow can one calculate the integral:\n$$\\int^{\\infty}_{-\\infty}\\Phi\\left(\\frac{w-a}{b}\\right)\\phi(w)\\,\\mathrm dw$$\n", "A": "Here is another solution: We define\n\\begin{align*}\nI(\\gamma) & =\\int_{-\\infty}^{\\infty}\\Phi(\\xi x+\\gamma)\\mathcal{N}(x|0,\\sigma^{2})dx,\n\\end{align*}\nwhich we can evaluate $\\gamma=-\\xi\\mu$ to obtain our desired expression.\nWe know at least one function value of $I(\\gamma)$, e.g., $I(0)=0$\ndue to symmetry. We take the derivative wrt to $\\gamma$\n\\begin{align*}\n\\frac{dI}{d\\gamma} & =\\int_{-\\infty}^{\\infty}\\mathcal{N}((\\xi x+\\gamma)|0,1)\\mathcal{N}(x|0,\\sigma^{2})dx\\\\\n & =\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\xi x+\\gamma\\right)^{2}\\right)\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)dx.\n\\end{align*}\nand complete the square\n\\begin{align*}\n\\left(\\xi x+\\gamma\\right)^{2}+\\frac{x^{2}}{\\sigma^{2}} & =\\underbrace{\\left(\\xi^{2}+\\sigma^{-2}\\right)}_{=a}x^{2}+\\underbrace{-2\\gamma\\xi}_{=b}x+\\underbrace{\\gamma^{2}}_{=c} \\\\\n&=a\\left(x-\\frac{b}{2a}\\right)^{2}+\\left(c-\\frac{b^{2}}{4a}\\right)\n\\left(c-\\frac{b^{2}}{4a}\\right)\\\\ \n& =\\gamma^{2}-\\frac{4\\gamma^{2}\\xi^{2}}{4\\left(\\xi^{2}+\\sigma^{-2}\\right)}\\\\\n&=\\gamma^{2}\\left(1-\\frac{\\xi^{2}}{\\xi^{2}+\\sigma^{-2}}\\right)\\\\\n&=\\gamma^{2}\\left(\\frac{1}{1+\\xi^{2}\\sigma^{2}}\\right)\n\\end{align*}\nThus, \n\\begin{align*}\n\\frac{dI}{d\\gamma} & =\\frac{1}{2\\pi\\sigma}\\exp\\left(-\\frac{1}{2}\\left(c-\\frac{b^{2}}{4a}\\right)\\right)\\sqrt{\\frac{2\\pi}{a}}\\int_{-\\infty}^{\\infty}\\sqrt{\\frac{a}{2\\pi}}\\exp\\left(-\\frac{1}{2}a\\left(x-\\frac{b}{2a}\\right)^{2}\\right)dx\\\\\n & =\\frac{1}{2\\pi\\sigma}\\exp\\left(-\\frac{1}{2}\\left(c-\\frac{b^{2}}{4a}\\right)\\right)\\sqrt{\\frac{2\\pi}{a}}\\\\ \n&=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}a}}\\exp\\left(-\\frac{1}{2}\\left(c-\\frac{b^{2}}{4a}\\right)\\right)\\\\\n & =\\frac{1}{\\sqrt{2\\pi\\left(1+\\sigma^{2}\\xi^{2}\\right)}}\\exp\\left(-\\frac{1}{2}\\frac{\\gamma^{2}}{1+\\xi^{2}\\sigma^{2}}\\right)\n\\end{align*}\nand integration yields \n$$\n\\begin{align*}\nI(\\gamma)\n&=\\int_{-\\infty}^{\\gamma}\\frac{1}{\\sqrt{2\\pi\\left(1+\\sigma^{2}\\xi^{2}\\right)}}\\exp\\left(-\\frac{1}{2}\\frac{z^{2}}{1+\\xi^{2}\\sigma^{2}}\\right)dz\\\\\n&=\\Phi\\left(\\frac{\\gamma}{\\sqrt{1+\\xi^{2}\\sigma^{2}}}\\right)\n\\end{align*}\n$$\nwhich implies \n$$\n\\begin{align*}\n\\int_{-\\infty}^{\\infty}\\Phi(\\xi x)\\mathcal{N}(x|\\mu,\\sigma^{2})dx\n&=I(\\xi\\mu)\\\\\n&=\\Phi\\left(\\frac{\\xi\\mu}{\\sqrt{1+\\xi^{2}\\sigma^{2}}}\\right).\n\\end{align*}\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/61080", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "53", "answer_count": 3, "answer_id": 2}}
{"Q": "Conditional Expectaction 3 variables Suppose $X,Y$ and $Z$ are multivariate normal with means and full covariance matrix. The conditional expectation $E(X | Y)$ is well know. What is the conditional expectation of $E(X | Y,Z)$ if $Y$ and $Z$ (and $X$) are correlated? Standard textbooks only seem to cover the case when $Y$ and $Z$ are uncorrelated. \n", "A": "If $\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{y} \\in \\mathbb{R}^m$ are jointly Gaussian,\n\\begin{align}\n\\begin{pmatrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{pmatrix}\n\\sim \n\\mathcal{N}\\left(\n\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix},\n\\begin{pmatrix} \\mathbf{A} & \\mathbf{C} \\\\ \\mathbf{C}^\\top & \\mathbf{B} \\end{pmatrix}\n \\right),\n\\end{align}\nthen (Rasmussen & Williams, 2006, Chapter A.2)\n\\begin{align}\n\\mathbf{x} \\mid \\mathbf{y} \\sim \\mathcal{N}\\left(\\mathbf{a} + \\mathbf{CB}^{-1}(\\mathbf{y} - \\mathbf{b}), \\mathbf{A} - \\mathbf{CB}^{-1}\\mathbf{C}^\\top \\right).\n\\end{align}\nIn your case,\n\\begin{align}\n\\mathbf{x} &= x, \\\\\n\\mathbf{y} &= \\begin{pmatrix} y \\\\ z \\end{pmatrix}, \\\\\n\\mathbf{a} &= \\mu_x, \\\\\n\\mathbf{b} &= \\begin{pmatrix} \\mu_y \\\\ \\mu_z\\end{pmatrix}, \\\\\n\\mathbf{A} &= \\sigma_{xx}^2, \\\\\n\\mathbf{B} &= \\begin{pmatrix} \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\ \\sigma_{zy}^2 & \\sigma_{zz}^2 \\end{pmatrix},\\\\\n\\mathbf{C} &= \\begin{pmatrix} \\sigma_{xy}^2 & \\sigma_{xz}^2 \\end{pmatrix},\n\\end{align}\nwhere $\\sigma_{xz}^2$ is the covariance between $x$ and $z$. Hence,\n$$E[x \\mid y, z] = \\mu_x + \\begin{pmatrix} \\sigma_{xy}^2 & \\sigma_{xz}^2 \\end{pmatrix}\n\\begin{pmatrix} \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\ \\sigma_{zy}^2 & \\sigma_{zz}^2 \\end{pmatrix}^{-1} \\begin{pmatrix} y - \\mu_y \\\\ z - \\mu_z \\end{pmatrix}.$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/68329", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "How to calculate $E[X^2]$ for a die roll? Apparently:\n$$\nE[X^2] = 1^2 \\cdot \\frac{1}{6} + 2^2 \\cdot \\frac{1}{6} + 3^2\\cdot\\frac{1}{6}+4^2\\cdot\\frac{1}{6}+5^2\\cdot\\frac{1}{6}+6^2\\cdot\\frac{1}{6}\n$$\nwhere $X$ is the result of a die roll.\nHow come this expansion?\n", "A": "There are various ways to justify it.\nFor example, it follows from the definition of expectation and the law of the unconscious statistician.\nOr consider the case $Y=X^2$ and computing $E(Y)$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/132996", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 0}}
{"Q": "Finding the Probability of a random variable with countably infinite values So I was working on a problem where I am provided with a PMF $p_X(k)= c/3^k$ for $k=1,2,3....$\nI was able to calculate $c$ using the basic property of PMF and it came to be 2. I am not able to solve the next part which states that \"Find $P(X\\ge k)$ for all $k=1,2,3......$.\nAny suggestions?\nP.S :Here is the actual question:\nLet X be a discrete random variable with probability mass function $p_X(k) = c/3^k$\nfor k = 1, 2, ... for some\n$c > 0$. Find $c$. Find $P(X\\ge k)$ for all $k = 1, 2,3....$\n", "A": "Consider, $$S=\\frac{2}{3}+\\cdots+\\frac{2}{3^{k-2}}+\\frac{2}{3^{k-1}}$$\nmultiply $S$ by $\\frac{1}{3}.$ Thus, \n$$\\frac{1}{3}S=\\frac{2}{3^{2}}+\\cdots+\\frac{2}{3^{k-1}}+\\frac{2}{3^{k}}.$$ \nSubtract $S$ of $\\frac{1}{3}S,$\n$$\\frac{2}{3}S=\\frac{2}{3}-\\frac{2}{3^{k}}.$$ Thus,\n$$S=1-\\frac{1}{3^{k-1}}.$$ Now,\nif $k=1, P(X\\geq k)=1,$ and if $k>1,$\n\\begin{eqnarray}\nP(X\\geq k)&=&1-P(X< k)\\\\\n&=&1-P(X\\leq k-1)\\\\\n&=&1-\\displaystyle \\sum_{n=1}^{k-1}\\frac{2}{3^{n}}\\\\\n&=&1-(1-\\frac{1}{3^{k-1}})\\\\\n&=&\\frac{1}{3^{k-1}}\n\\end{eqnarray}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/262359", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Limiting Sum of i.i.d. Gamma variates Let $X_1,X_2,\\ldots$ be a sequence of independently and identically distributed random variables with the probability density function; $$  f(x) = \\left\\{ \\begin{array}{ll}\n         \\frac{1}{2}x^2 e^{-x} & \\mbox{if $x>0$};\\\\\n         0 & \\mbox{otherwise}.\\end{array} \\right.  $$\nShow that $$\\lim_{n\\to \\infty} P[X_1+X_2+\\ldots+X_n\\ge 3(n-\\sqrt{n})] \\ge \\frac{1}{2}$$\nWhat I attempted\nAt first sight I thought it should use Chebyshev's inequality as the question is asking show a lower bound $X_1+X_2+\\ldots +X_n$. However, I thought about the limit sign which is clearly indicating that the problem can be  somehow related to Central Limit Theorem (CLT)  \nLet $S_n=X_1+X_2+\\ldots +X_n$\n$$E(S_n)=\\sum_{i=0}^{n} E(X_i)=3n \\ (\\text{since } E(X_i)=3) \\\\ V(S_n)=\\sum_{i=0}^{n} V(X_i)=3n \\ (\\text{since } V(X_i)=3 \\text{ and } X_i \\text{ are i.i.d})$$  \nNow,\nUsing CLT, for large $n$, $X_1+X_2+........+X_n \\sim N(3n,3n)$\nOr, $$z=\\frac{S_n-3n}{\\sqrt{3n}} \\sim N(0,1) \\text{ as } n\\to \\infty$$  \nNow, $$\\lim_\\limits{n\\to \\infty} P[X_1+X_2+........+X_n\\ge 3(n-\\sqrt{n})]\n= \\lim_\\limits{n\\to \\infty}P(S_n-3n \\ge -3\\sqrt{n})  \n=   \\lim_\\limits{n\\to \\infty}  P\\left(\\frac{S_n-3n}{\\sqrt{3n}} \\ge -\\sqrt{3}\\right)  \n=P(z\\ge -\\sqrt{3})  \n=P\\left(-\\sqrt{3}\\le z<0\\right)+P(z\\ge 0 ) \n=P\\left(-\\sqrt{3}\\le z<0\\right)+\\frac{1}{2}\\cdots(1)$$  \nSince $P(-\\sqrt{3}\\le z<0) \\ge 0$, thus from $(1)$, \n $$\\lim_\\limits{n\\to \\infty} P[X_1+X_2+........+X_n\\ge 3(n-\\sqrt{n})]\\ge \\frac{1}{2}$$  \nAm I correct?\n", "A": "As an alternative to whuber's excellent answer, I will try to derive the exact limit of the probability in question.  One of the properties of the gamma distribution is that sums of independent gamma random variables with the same rate/scale parameter are also gamma random variables with shape equal to the sum of the shapes of those variables.  (That can be proved using the generating functions of the distribution.)  In the present case we have $X_1,...X_n \\sim \\text{IID Gamma}(3,1)$, so we obtain the sum:\n$$S_n \\equiv X_1 + \\cdots + X_n \\sim \\text{Gamma}(3n, 1).$$\nWe can therefore write the exact probability of interest using the CDF of the gamma distribution.  Letting $a = 3n$ denote the shape parameter and $x = 3(n-\\sqrt{n})$ denote the argument of interest, we have:\n$$\\begin{equation} \\begin{aligned}\nH(n) \n&\\equiv \\mathbb{P}(S_n \\geq 3(n-\\sqrt{n})) \\\\[12pt]\n&= \\frac{\\Gamma(a, x)}{\\Gamma(a)} \\\\[6pt]\n&= \\frac{a \\Gamma(a)}{a \\Gamma(a) + x^a e^{-x}} \\cdot \\frac{\\Gamma(a+1, x)}{\\Gamma(a+1)}. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nTo find the limit of this probability, we first note that we can write the second parameter in terms of the first as $x = a + \\sqrt{2a} \\cdot y$ where $y = -\\sqrt{3/2}$.  Using a result shown in Temme (1975) (Eqn 1.4, p. 1109) we have the asymptotic equivalence:\n$$\\begin{aligned}\n\\frac{\\Gamma(a+1, x)}{\\Gamma(a+1)} \n&\\sim \\frac{1}{2} + \\frac{1}{2} \\cdot \\text{erf}(-y) + \\sqrt{\\frac{2}{9a \\pi}} (1+y^2) \\exp( - y^2).\n\\end{aligned}$$\nUsing Stirling's approximation, and the limiting definition of the exponential number, it can also be shown that:\n$$\\begin{aligned}\n\\frac{a \\Gamma(a)}{a \\Gamma(a) + x^a e^{-x}}\n&\\sim \\frac{\\sqrt{2 \\pi} \\cdot a \\cdot (a-1)^{a-1/2}}{\\sqrt{2 \\pi} \\cdot a \\cdot (a-1)^{a-1/2} + x^a \\cdot e^{a-x-1}} \\\\[6pt]\n&= \\frac{\\sqrt{2 \\pi} \\cdot a \\cdot (1-\\tfrac{1}{a})^{a-1/2}}{\\sqrt{2 \\pi} \\cdot a \\cdot (1-\\tfrac{1}{a})^{a-1/2} + \\sqrt{x} \\cdot (\\tfrac{x}{a})^{a-1/2} \\cdot e^{a-x-1}} \\\\[6pt]\n&= \\frac{\\sqrt{2 \\pi} \\cdot a \\cdot e^{-1}}{\\sqrt{2 \\pi} \\cdot a \\cdot e^{-1} + \\sqrt{x} \\cdot e^{x-a} \\cdot e^{a-x-1}} \\\\[6pt]\n&= \\frac{\\sqrt{2 \\pi} \\cdot a}{\\sqrt{2 \\pi} \\cdot a + \\sqrt{x}} \\\\[6pt]\n&\\sim \\frac{\\sqrt{2 \\pi a}}{\\sqrt{2 \\pi a} + 1}. \\\\[6pt]\n\\end{aligned}$$\nSubstituting the relevant values, we therefore obtain:\n$$\\begin{equation} \\begin{aligned}\nH(n) \n&= \\frac{a \\Gamma(a)}{a \\Gamma(a) + x^a e^{-x}} \\cdot \\frac{\\Gamma(a+1, x)}{\\Gamma(a+1)} \\\\[6pt]\n&\\sim \\frac{\\sqrt{2 \\pi a}}{\\sqrt{2 \\pi a} + 1} \\cdot \\Bigg[ \\frac{1}{2} + \\frac{1}{2} \\cdot \\text{erf} \\Big( \\sqrt{\\frac{3}{2}} \\Big) + \\sqrt{\\frac{2}{9a \\pi}} \\cdot \\frac{5}{2} \\cdot \\exp \\Big( \\frac{3}{2} \\Big) \\Bigg]. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nThis gives us the limit:\n$$\\lim_{n \\rightarrow \\infty} H(n) = \\frac{1}{2} + \\frac{1}{2} \\cdot \\text{erf} \\Big( \\sqrt{\\frac{3}{2}} \\Big) = 0.9583677.$$\nThis gives us the exact limit of the probability of interest, which is larger than one-half.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/342704", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "12", "answer_count": 2, "answer_id": 1}}
{"Q": "Integrating out parameter with improper prior I got this problem while I was reading the book \"Machine Learning: A Probabilistic Perspective\" by Kevin Murphy. It is in section 7.6.1 of the book.\nAssume the likelihood is given by \n$$\n\\begin{split}\np(\\mathbf{y}|\\mathbf{X},\\mathbf{w},\\mu,\\sigma^2) & = \\mathcal{N}(\\mathbf{y}|\\mu+\\mathbf{X}\\mathbf{w}, \\sigma^2\\mathbf{I}_N) \\\\\n    & \\propto (-\\frac{1}{2\\sigma^2}(\\mathbf{y}-\\mu\\mathbf{1}_N - \\mathbf{X}\\mathbf{w})^T(\\mathbf{y}-\\mu\\mathbf{1}_N - \\mathbf{X}\\mathbf{w}))\n\\end{split}\n\\tag{7.53}\n$$\n$\\mu$ and $\\sigma^2$ are scalars. $\\mu$ serves as an offset. $\\mathbf{1}_N$ is a column vector with length $N$.\nWe put an improper prior on $\\mu$ of the form $p(u) \\propto 1$ and then integrate it out to get \n$$\np(\\mathbf{y}|\\mathbf{X},\\mathbf{w},\\sigma^2) \\propto (-\\frac{1}{2\\sigma^2}||\\mathbf{y}-\\bar{y}\\mathbf{1}_N - \\mathbf{X}\\mathbf{w}||_2^2)\n\\tag{7.54}\n$$\nwhere $\\bar{y}=\\frac{1}{N}\\sum_{i=1}^{N}y_i$ is the empirical mean of the output. \nI tried to expand the formula (last line in $7.53$) to integrate directly but failed.\nAny idea or hint on how to derive from $(7.53)$ to $(7.54)$?\n", "A": "This calculation assumes that the columns of the design matrix have been centred, so that:\n$$(\\mathbf{Xw}) \\cdot \\mathbf{1}_N = \\mathbf{w}^\\text{T} \\mathbf{X}^\\text{T} \\mathbf{1}_N = \\mathbf{w}^\\text{T} \\mathbf{0} = 0.$$\nWith this restriction you can rewrite the quadratic form as a quadratic in $\\mu$ plus a term that does not depend on $\\mu$ as follows:\n$$\\begin{equation} \\begin{aligned}\n|| \\mathbf{y} - \\mu \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2 \n&= || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} + (\\bar{y} - \\mu) \\mathbf{1}_N ||^2 \\\\[6pt]\n&= || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2\n+ 2 (\\bar{y} - \\mu) (\\mathbf{y} - \\mu \\mathbf{1}_N - \\mathbf{X} \\mathbf{w}) \\cdot \\mathbf{1}_N\n+ (\\bar{y} - \\mu)^2 || \\mathbf{1}_N ||^2 \\\\[6pt]\n&= || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2\n- 2 n (\\bar{y} - \\mu)^2 + n (\\bar{y} - \\mu)^2 \\\\[6pt]\n&= || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2\n- n (\\mu - \\bar{y})^2. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nHence, with the improper prior $\\pi(\\mu) \\propto 1$ you have:\n$$\\begin{equation} \\begin{aligned}\np(\\mathbf{y}|\\mathbf{X},\\mathbf{w},\\sigma^2) \n&= \\int \\limits_\\mathbb{R} p(\\mathbf{y}|\\mathbf{X},\\mathbf{w},\\mu,\\sigma^2) \\pi(\\mu)  \\ d \\mu \\\\[6pt]\n&\\overset{\\mathbf{y}}{\\propto} \\int \\limits_\\mathbb{R} \\exp \\Big( -\\frac{1}{2\\sigma^2} || \\mathbf{y}-\\mu\\mathbf{1}_N - \\mathbf{X}\\mathbf{w} ||^2 \\Big) \\ d \\mu \\\\[6pt]\n&= \\exp \\Big( -\\frac{1}{2\\sigma^2} || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2 \\Big) \\int \\limits_\\mathbb{R} \\exp \\Big( -\\frac{n}{2\\sigma^2} (\\mu - \\bar{y})^2 \\Big) \\ d \\mu \\\\[6pt]\n&\\overset{\\mathbf{y}}{\\propto} \\exp \\Big( -\\frac{1}{2\\sigma^2} || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2 \\Big) \\int \\limits_\\mathbb{R} \\text{N} \\Big( \\mu \\Big| \\bar{y}, \\frac{\\sigma^2}{n} \\Big) \\ d \\mu \\\\[6pt]\n&= \\exp \\Big( -\\frac{1}{2\\sigma^2} || \\mathbf{y} - \\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w} ||^2 \\Big). \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nThus, your posterior distribution is:\n$$\\mathbf{y}|\\mathbf{X},\\mathbf{w},\\sigma^2 \\sim \\text{N}(\\bar{y} \\mathbf{1}_N - \\mathbf{X} \\mathbf{w}, \\sigma^2).$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/392584", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
