{"Q": "Bounding the difference between square roots I want to compute the value of $\\frac{1}{\\sqrt{a + b + c}}$.  Say I can observe a and b, but not c.  Instead, I can observe d which is a good approximation for c in the sense that $P( |c-d| \\leq 0.001 )$ is large (say 95%), and both c and d are known to have $|c| \\leq 1, |d| \\leq 1$ so a difference of 0.001 is actually small.\nI want to argue that $\\frac{1}{\\sqrt{a + b + d}}$ is a good approximation because d is a good approximation for c.  Is there anything I can say about the difference\n$\\frac{1}{\\sqrt{a + b + c}} - \\frac{1}{\\sqrt{a + b + d}}$?\nMaybe I could say something like $P(\\frac{1}{\\sqrt{a + b + c}} - \\frac{1}{\\sqrt{a + b + d}} \\leq 0.001) = x?$  I'm worried that being under a square root might mess things up.\nWould I need to find out the exact distribution of $c - d$, or anything else before I can make such claims?\n", "A": "Use a Taylor series (or equivalently the Binomial Theorem) to expand around $c$.  This is valid provided $|d-c| \\lt |a+b+c|$:\n$$\\eqalign{\r\n&\\frac{1}{\\sqrt{a+b+c}} - \\frac{1}{\\sqrt{a+b+d}}\\\\\r\n&= (a+b+c)^{-1/2} - (a+b+c + (d-c))^{-1/2} \\\\\r\n&= (a+b+c)^{-1/2} - (a+b+c)^{-1/2}\\left(1 + \\frac{d-c}{a+b+c}\\right)^{-1/2} \\\\\r\n&= (a+b+c)^{-1/2} - (a+b+c)^{-1/2}\\sum_{j=1}^{\\infty}\\binom{-1/2}{j}\\left(\\frac{d-c}{a+b+c}\\right) ^j\\\\\r\n&= \\frac{1}{2}(a+b+c)^{-3/2}(d-c) - \\frac{3}{8}(a+b+c)^{-5/2}O(d-c)^2\r\n}\r\n$$\nThe difference therefore is approximately  $\\frac{1}{2}(a+b+c)^{-3/2}$ times $(d-c)$ and the error is (a) negative (because this is an alternating series when $d-c$ is positive and has all negative terms when $d-c$ is negative), (b) proportional to  $\\frac{3}{8}(a+b+c)^{-5/2}$, and (c) of second order in $d-c$.  That should be enough to complete your analysis.  (This leads essentially to the delta method.)\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/20409", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 3, "answer_id": 0}}
{"Q": "Find the pdf of Y when pdf of X is given $$\nf_{X}(x) = \\frac{3}{8}(x+1)^{2} ,\\ -1 < x < 1\n$$\n$$Y = \\begin{cases} 1 - X^{2}  & X \\leq 0,\\\\\n1- X, & X > 0.\\end{cases}$$\nI started with :\n$$\nF_{Y}(y) = 1 - P(Y \\leq y) \n$$\n$$\n         = 1 - [P(-(1-y)^\\frac {1}{2} < X < (1-y)]\n$$\nFrom here, I can get $F_{Y}(y)$, and differentiating it will give me $f_{x}(x)$.\nBut the answer I am getting for pdf is not the desired answer. Am I doing anything wrong?\nThanks for your help.\n", "A": "The probability density function of $Y$ can be found by:\n$$\nf_{Y}(y)=\\sum_{i}f_{X}(g_{i}^{-1}(y))\\left|\\frac{dg_{i}^{-1}(y)}{dy}\\right|,\\quad \\mathrm{for}\\; y \\in \\mathcal{S}_{Y}\n$$\nwhere $g_{i}^{-1}$ denotes the inverse of the transformation function and $\\mathcal{S}_{Y}$ the support of $Y$. Let's denote our two transformation functions\n$$\n\\begin{align}\n     g_{1}(X) &= 1-X^{2}, & X\\leq 0\\\\\n     g_{2}(X) &= 1-X, & X>0\\\\\n\\end{align}\n$$\nThe support of $Y$ is the set $\\mathcal{S}_{Y}=\\{y=g(x):x\\in\\mathcal{S}_{X}\\}$ where $\\mathcal{S}_{X}$ denotes the support set of $X$. Hence, the support of $Y$ is $y \\in (0,1]$. Further, we need the inverse transformations $g_{1}^{-1}(y)$ and $g_{2}^{-1}(y)$. They are given by:\n$$\n\\begin{align}\n     g_{1}^{-1}(y) &= -\\sqrt{1-y}\\\\\n     g_{2}^{-1}(y) &= 1-y \\\\\n\\end{align}\n$$\nIn the first inverse, we need only the negative signed function because $x\\leq 0$. The derivatives are:\n$$\n\\begin{align}\n     \\left|\\frac{dg_{1}^{-1}(y)}{dy} g_{1}^{-1}(y)\\right| &=\\frac{1}{2\\sqrt{1-y}}\\\\\n     \\left|\\frac{dg_{2}^{-1}(y)}{dy} g_{2}^{-1}(y)\\right| &= \\left|-1\\right| = 1 \\\\\n\\end{align}\n$$\nSo the PDF of $Y$ is given by:\n$$\n\\begin{align}\nf_{Y}(y) &= f_{X}(-\\sqrt{1-y})\\cdot \\frac{1}{2\\sqrt{1-y}} + f_{X}(1-y)\\cdot 1 \\\\\n&= \\frac{3}{8}(1-\\sqrt{1-y})^{2}\\cdot \\frac{1}{2\\sqrt{1-y}} + \\frac{3}{8}(2-y)^{2}\\cdot 1 \\\\\n&= \\begin{cases}\n\\frac{3}{16}\\left(6+\\frac{2}{\\sqrt{1-y}}+y\\cdot\\left(2y-\\frac{1}{\\sqrt{1-y}}-8\\right)\\right), & 0 < y \\leq 1\\\\\n0, &\\mathrm{otherwise}\n\\end{cases}\n\\end{align}\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/70709", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "If f(x) is given, what would be the distribution of Y = 2X + 1? \nIf the random variable X has a continuous distribution with the\n  density $ f(x) = \\frac{1}{x^2}\\Bbb {1}_{(1, \u221e)}(x)$, can you find the\n  distribution of $Y = 2X+1$?\n\nMy attempt:\n$CDF(Y)$\n$\\Rightarrow P(Y\\le y)$\n$\\Rightarrow P(2X + 1 \\le y)$\n$\\Rightarrow P(X \\le \\frac{y-1}{2})$\n$\\Rightarrow \\int_{1}^{(y-1)/2} f(x) dx$\n$\\Rightarrow \\int_{1}^{(y-1)/2} \\frac{1}{x^2} \\Bbb 1_{(1, \\infty)}(x) dx$\n$\\Rightarrow \\int_{1}^{(y-1)/2} \\frac{1}{x^2} dx$\n$\\Rightarrow \\frac{1}{-2+1}[x^{-2+1}]_1^{(y-1)/2}$\n$\\Rightarrow 1 - \\frac{2}{y-1}$ Ans.\n\nIs it a correct approach?\n", "A": "Since the transformation function is monotonic, we can find the CDF by using PDF transformation and integrating the transformed PDF.\nPDF Transformation: \n$$ f_Y(y) = f_X(g^{-1}(y)) \\Bigg|\\frac{dg^{-1}}{dy} \\Bigg|$$\nFor this situation, $g^{-1}(y) = \\frac{y-1}{2}$, and by substitution:\n$$f_X(g^{-1}(y)) = \\frac{1}{((y-1)/2)^2} = \\frac{4}{(y-1)^2}$$ \nThe absolute value of the derivative of $g^{-1}(y)$ with respect to $y$ is easy:\n\\begin{align}\n\\Bigg|\\frac{dg^{-1}}{dy} \\Bigg| = \\frac{1}{2}\n\\end{align}\nPlug components into the PDF Transformation formula above to get a transformed PDF of :\n\\begin{align}\nf_Y(y) = \\frac{2}{(y-1)^2} \\quad \\text{for} \\quad 3 \\leq y \\lt \\infty\n\\end{align}\nRemembering to use transformed lower and upper bounds, we integrate to get the CDF. Line 3 employs u-substitution to simplify the integration:\n\\begin{align} \nF_Y(y) &=\\int_{3}^{y} f_Y(t) \\, dt\\\\  \n&=\\int_{3}^{y} \\frac{2}{(t-1)^2} \\, dt\\\\  \n&=\\int_{2}^{y-1} \\frac{2}{u^2} \\, du\\\\\n&= -\\frac{2}{u} \\; \\Bigg|_2^{y-1}\\\\\n&= -\\frac{2}{y-1} - (-\\frac{2}{2})\\\\ \n&= 1 - \\frac{2}{y-1} \\quad \\text{for} \\quad 3 \\leq y \\lt \\infty\n\\end{align}\nFinally, we'll check the lower and upper bounds just to make sure we've fulfilled CDF validity requirements:\n$$F_Y(3) = 1 - \\frac{2}{2} = 0$$\n$$F_Y(\\infty) = 1 - \\frac{2}{\\infty} = 1$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/245341", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 1}}
{"Q": "Distribution of $\\sqrt{X^2+Y^2}$ when $X,Y$ are independent $U(0,1)$ variables \nAs a routine exercise, I am trying to find the distribution of $\\sqrt{X^2+Y^2}$ where  $X$ and $Y$ are independent $ U(0,1)$ random variables.\n\nThe joint density of $(X,Y)$ is  $$f_{X,Y}(x,y)=\\mathbf 1_{0<x,y<1}$$\nTransforming to polar coordinates $(X,Y)\\to(Z,\\Theta)$ such that $$X=Z\\cos\\Theta\\qquad\\text{ and }\\qquad Y=Z\\sin\\Theta$$ \nSo, $z=\\sqrt{x^2+y^2}$ and $0< x,y<1\\implies0< z<\\sqrt 2$.\nWhen $0< z<1$, we have $0< \\cos\\theta<1,\\,0<\\sin\\theta<1$ so that $0<\\theta<\\frac{\\pi}{2}$.\nWhen $1< z<\\sqrt 2$, we have $z\\cos\\theta<\\implies\\theta>\\cos^{-1}\\left(\\frac{1}{z}\\right)$, as $\\cos\\theta$ is decreasing on $\\theta\\in\\left[0,\\frac{\\pi}{2}\\right]$; and $z\\sin\\theta<1\\implies\\theta<\\sin^{-1}\\left(\\frac{1}{z}\\right)$, as $\\sin\\theta$ is increasing on $\\theta\\in\\left[0,\\frac{\\pi}{2}\\right]$.\nSo, for $1< z<\\sqrt 2$, we have $\\cos^{-1}\\left(\\frac{1}{z}\\right)<\\theta<\\sin^{-1}\\left(\\frac{1}{z}\\right)$.\nThe absolute value of jacobian of transformation is $$|J|=z$$\nThus the joint density of $(Z,\\Theta)$ is given by \n$$f_{Z,\\Theta}(z,\\theta)=z\\mathbf 1_{\\{z\\in(0,1),\\,\\theta\\in\\left(0,\\pi/2\\right)\\}\\bigcup\\{z\\in(1,\\sqrt2),\\,\\theta\\in\\left(\\cos^{-1}\\left(1/z\\right),\\sin^{-1}\\left(1/z\\right)\\right)\\}}$$\nIntegrating out $\\theta$, we obtain the pdf of $Z$ as\n$$f_Z(z)=\\frac{\\pi z}{2}\\mathbf 1_{0<z<1}+\\left(\\frac{\\pi z}{2}-2z\\cos^{-1}\\left(\\frac{1}{z}\\right)\\right)\\mathbf 1_{1<z<\\sqrt 2}$$\n\nIs my reasoning above correct? In any case, I would like to avoid this method and instead try to find the cdf of $Z$ directly. But I couldn't find the desired areas while evaluating $\\mathrm{Pr}(Y\\le \\sqrt{z^2-X^2})$ geometrically.\n\nEDIT.\nI tried finding the distribution function of $Z$ as\n\\begin{align}\nF_Z(z)&=\\Pr(Z\\le z)\n\\\\&=\\Pr(X^2+Y^2\\le z^2)\n\\\\&=\\iint_{x^2+y^2\\le z^2}\\mathbf1_{0<x,y<1}\\,\\mathrm{d}x\\,\\mathrm{d}y\n\\end{align}\nMathematica says this should reduce to\n$$F_Z(z)=\\begin{cases}0 &,\\text{ if }z<0\\\\ \\frac{\\pi z^2}{4} &,\\text{ if } 0< z<1\\\\ \\sqrt{z^2-1}+\\frac{z^2}{2}\\left(\\sin^{-1}\\left(\\frac{1}{z}\\right)-\\sin^{-1}\\left(\\frac{\\sqrt{z^2-1}}{z}\\right)\\right) &,\\text{ if }1< z<\\sqrt 2\\\\ 1 &,\\text{ if }z>\\sqrt 2 \\end{cases}$$\nwhich looks like the correct expression. Differentiating $F_Z$ for the case $1< z<\\sqrt 2$ though brings up an expression which doesn't readily simplify to the pdf I already obtained.\nFinally, I think I have the correct pictures for the CDF:\nFor $0<z<1$ :\n \nAnd for $1<z<\\sqrt 2$ :\n\nShaded portions are supposed to indicate the area of the region $$\\left\\{(x,y):0<x,y< 1\\,,\\,x^2+y^2\\le z^2\\right\\}$$\nThe picture immediately yields \n\\begin{align}\nF_Z(z)&=\\Pr\\left(-\\sqrt{z^2-X^2}\\le Y\\le\\sqrt{z^2-X^2}\\right)\n\\\\&=\\begin{cases}\\frac{\\pi z^2}{4} &,\\text{ if } 0<z<1\\\\\\\\ \\sqrt{z^2-1}+\\int_{\\sqrt{z^2-1}}^1 \\sqrt{z^2-x^2}\\,\\mathrm{d}x &,\\text{ if }1< z<\\sqrt 2 \\end{cases}\n\\end{align}\n, as I had previously found.\n", "A": "\n$f_z(z)$ :\n\nSo, for $1\\le z<\\sqrt 2$, we have\n  $\\cos^{-1}\\left(\\frac{1}{z}\\right)\\le\\theta\\le\\sin^{-1}\\left(\\frac{1}{z}\\right)$\n\nYou can simplify your expressions when you use symmetry and evaluate the expressions for $\\theta_{min} < \\theta < \\frac{\\pi}{4}$. Thus, for half of the space and then double the result.\nThen you get:\n$$P(Z \\leq r) = 2 \\int_0^r z \\left(\\int_{\\theta_{min}}^{\\frac{\\pi}{4}}d\\theta\\right) dz = \\int_0^r z \\left(\\frac{\\pi}{2}-2\\theta_{min}\\right)  dz$$ \nand your $f_z(z)$ is \n$$f_z(z) =  z \\left(\\frac{\\pi}{2}-2\\theta_{min}\\right) = \\begin{cases} z\\left(\\frac{\\pi}{2}\\right) & \\text{ if } 0 \\leq z \\leq 1 \\\\ z \\left(\\frac{\\pi}{2} - 2 \\cos^{-1}\\left(\\frac{1}{z}\\right)\\right) & \\text{ if } 1 < z \\leq \\sqrt{2} \\end{cases}$$\n\n$F_z(z)$ :\nYou can use the indefinite integral:\n$$\\int z \\cos^{-1}\\left(\\frac{1}{z}\\right) = \\frac{1}{2} z \\left( z \\cos^{-1}\\left(\\frac{1}{z}\\right) - \\sqrt{1-\\frac{1}{z^2}} \\right) + C $$\nnote $\\frac{d}{du} \\cos^{-1}(u) = - (1-u^2)^{-0.5}$\nThis leads straightforward to something similar as Xi'ans expression for $Pr(Z \\leq z)$ namely \nif $1 \\leq z \\leq \\sqrt{2}$ then:\n$$F_z(z) = {z^2} \\left(\\frac{\\pi}{4}-\\cos^{-1}\\left(\\frac{1}{z}\\right) + z^{-1}\\sqrt{1-\\frac{1}{z^2}} \\right)$$\n\nThe relation with your expression is seen when we split up the $cos^{-1}$ into two $cos^{-1}$ expressions, and then converted to different $sin^{-1}$ expressions.\nfor $z>1$ we have\n$$\\cos^{-1}\\left(\\frac{1}{z}\\right) = \\sin^{-1}\\left(\\sqrt{1-\\frac{1}{z^2}}\\right) = \\sin^{-1}\\left(\\frac{\\sqrt{z^2-1}}{z}\\right) $$\nand\n$$\\cos^{-1}\\left(\\frac{1}{z}\\right) = \\frac{\\pi}{2} -\\sin^{-1}\\left(\\frac{1}{z}\\right) $$\nso\n$$\\begin{array}\\\\\n\\cos^{-1}\\left(\\frac{1}{z}\\right) & = 0.5 \\cos^{-1}\\left(\\frac{1}{z}\\right)  + 0.5 \\cos^{-1}\\left(\\frac{1}{z}\\right) \\\\ \n & = \\frac{\\pi}{4} - 0.5 \\sin^{-1}\\left(\\frac{1}{z}\\right)  + 0.5 \\sin^{-1}\\left(\\frac{\\sqrt{z^2-1}}{z}\\right) \\end{array} $$\nwhich results in your expression when you plug this into the before mentioned $F_z(z)$ for $1<z<\\sqrt{2}$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/323617", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "11", "answer_count": 3, "answer_id": 0}}
{"Q": "State-space model with contemporaneous effects I have the following system of equations:\n$$\n\\begin{align}\ny_t^{(1)}&=y_t^{(2)}-x_t+\\epsilon_t\\\\\ny_t^{(2)}&=x_t+\\nu_t\\\\\nx_t&=\\alpha x_{t-1}+u_t\n\\end{align}\n$$\nwhere $y_t^{(1)}, y_t^{(2)}$ are observed and $x_t$\u00a0is not.\nI'm having some issues putting this into the state-space formulation. The issue I have is that in order to get $y_t^{(2)}$ in the (measurement) equation for $y_t^{(1)}$ I need to put it in the state vector. But I need $y_t^{(2)}$ in the measurement equation in order to get $x_t$ in there. So what do I do with $y_t^{(2)}$ in the state vector? Can I simply skip that row in the state equation? That would give me:\n$$\n\\begin{align}\n\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}&=\\begin{pmatrix}-1 & 1 \\\\\n0 & 1\\end{pmatrix}\\begin{pmatrix}x_t \\\\ y_t^{(2)}\\end{pmatrix}+\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}\\\\\nx_t&=\\begin{pmatrix}\\alpha & 0 \\end{pmatrix}\\begin{pmatrix}x_{t-1} \\\\ y_{t-1}^{(2)}\\end{pmatrix}+u_t.\n\\end{align}\n$$\n", "A": "Substitute the second equation into the first and you have\n\\begin{align}\ny_t^{(1)}&=y_t^{(2)}-x_t+\\epsilon_t\\\\\n&=x_t+\\nu_t-x_t+\\epsilon_t\\\\\n&=\\nu_t+\\epsilon_t.\n\\end{align}\nSo it's\n\\begin{align}\n\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}&=\\begin{pmatrix}0 \\\\\n1\\end{pmatrix}x_t+\\begin{pmatrix}1 & 1 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}\\\\\nx_t&=\\alpha x_{t-1}+u_t.\n\\end{align}\nAnother way to see it is to rewrite the measurement equations as\n\\begin{align}\ny_t^{(1)}-y_t^{(2)}&=-x_t+\\epsilon_t\\\\\ny_t^{(2)}&=x_t+\\nu_t,\n\\end{align}\nwhich is equivalent to\n$$\\begin{pmatrix}1 & -1 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}=\\begin{pmatrix}-1 \\\\\n1\\end{pmatrix}x_t+\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}.$$\nTo isolate the observables on the left hand side, note that \n$$\\begin{pmatrix}1&-1\\\\0&1\\end{pmatrix}^{-1}=\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}.$$\nMultiply both sides by that and you get\n\\begin{align}\n\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}\\begin{pmatrix}1 & -1 \\\\ 0 & 1\\end{pmatrix}\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}&=\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}\\left[\\begin{pmatrix}-1 \\\\\n1\\end{pmatrix}x_t+\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}\\right]\\\\\n\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}&=\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}\\begin{pmatrix}-1 \\\\\n1\\end{pmatrix}x_t+\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}\\\\\n\\begin{pmatrix}y_t^{(1)}\\\\ y_t^{(2)}\\end{pmatrix}&=\\begin{pmatrix}0 \\\\\n1\\end{pmatrix}x_t+\\begin{pmatrix}1&1\\\\0&1\\end{pmatrix}\\begin{pmatrix}\\epsilon_t\\\\\\nu_t\\end{pmatrix}.\n\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/373080", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Understanding KL divergence between two univariate Gaussian distributions I'm trying to understand KL divergence from this post on SE. I am following @ocram's answer, I understand the following :\n$\\int \\left[\\log( p(x)) - log( q(x)) \\right] p(x) dx$\n$=\\int \\left[ -\\frac{1}{2} \\log(2\\pi) - \\log(\\sigma_1) - \\frac{1}{2} \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 + \\frac{1}{2}\\log(2\\pi) + \\log(\\sigma_2) + \\frac{1}{2} \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2  \\right] \\times \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2\\right] dx$\n$=\\int \\left\\{\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2} \\left[ \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2 - \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\right] \\right\\} \\times \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2\\right] dx$\nBut not the following:\n$=E_{1} \\left\\{\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2} \\left[ \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2 - \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\right]\\right\\}$\n$=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2\\sigma_1^2} E_1 \\left\\{(X-\\mu_1)^2\\right\\}$\n$=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2}$\nNow noting :\n$(X - \\mu_2)^2 = (X-\\mu_1+\\mu_1-\\mu_2)^2 = (X-\\mu_1)^2 + 2(X-\\mu_1)(\\mu_1-\\mu_2) + (\\mu_1-\\mu_2)^2$\n$=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2}\n\\left[E_1\\left\\{(X-\\mu_1)^2\\right\\} + 2(\\mu_1-\\mu_2)E_1\\left\\{X-\\mu_1\\right\\} + (\\mu_1-\\mu_2)^2\\right] - \\frac{1}{2}$\n$=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$\nFirst off what is $E_1$?\n", "A": "$E_1$ is the expectation with respect to the first distribution $p(x)$. Denoting it with $E_p$ would be better, I think. \u2013 Monotros\n\nI've created this answer from a comment so that this question is answered. Better to have a short answer than no answer at\nall.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/406221", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Sample standard deviation is a biased estimator: Details in calculating the bias of $s$ In this post Why is sample standard deviation a biased estimator of $\\sigma$?\nthe last step is shown as:\n$$\\sigma\\left(1-\\sqrt\\frac{2}{n-1}\\frac{\\Gamma\\frac{n}{2}}{\\Gamma\\frac{n-1}{2}}\\right)\n= \\sigma\\left(1-\\sqrt\\frac{2}{n-1}\\frac{((n/2)-1)!}{((n-1)/2-1)!}\\right)$$\nHow is this equal to $\\frac{\\sigma}{4n}$?\n", "A": "Making the substitution $x = \\frac{n}{2}-1$, you essentially want to control\n$$1 - \\frac{\\Gamma(x+1)}{\\Gamma(x+\\frac{1}{2}) \\sqrt{x + \\frac{1}{2}}}$$\nas $x \\to \\infty$.\nGautschi's inequality (applied with $s=\\frac{1}{2}$) implies\n$$\n1 - \\sqrt{\\frac{x+1}{x+\\frac{1}{2}}}\n<1 - \\frac{\\Gamma(x+1)}{\\Gamma(x+\\frac{1}{2}) \\sqrt{x + \\frac{1}{2}}}\n< 1 - \\sqrt{\\frac{x}{x+\\frac{1}{2}}}$$\nThe upper and lower bounds can be rearranged as\n$$\n\\left|1 - \\frac{\\Gamma(x+1)}{\\Gamma(x+\\frac{1}{2}) \\sqrt{x + \\frac{1}{2}}}\\right|\n< \\frac{1}{2x+1} \\cdot \\frac{1}{1 + \\sqrt{1 - \\frac{1}{2x+1}}}\n\\approx \\frac{1}{2(2x+1)}.$$\nPlugging in $x=\\frac{n}{2}-1$ gives a bound of $\\frac{1}{2(n-1)}$. This is weaker than the author's claim of asymptotic equivalence with $\\frac{1}{4n}$, but at least it is of the same order.\n\nResponses to comments:\nWhen $x=\\frac{n}{2}-1$ you have $x+1 = \\frac{n}{2}$ and $x + \\frac{1}{2} = \\frac{n}{2} - 1 + \\frac{1}{2} = \\frac{n}{2} - \\frac{1}{2} = \\frac{n-1}{2}$. So $\\frac{\\Gamma(x+1)}{\\Gamma(x+\\frac{1}{2}) \\sqrt{x + \\frac{1}{2}}} = \\frac{\\Gamma(n/2)}{\\Gamma((n-1)/2) \\sqrt{(n-1)/2}}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/494489", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Calculate the consistency of an Estimator I need to determine whether the following estimator $T$ is asymptotically unbiased and consistent for an i.i.d. sample of Gaussian distributions with $X_{i} \\sim N(\\mu, \\sigma)$:\n\\begin{equation*}\nT = \\frac{1}{2} X_{1} + \\frac{1}{2n} \\sum\\limits_{i = 2}^{n} X_{i}\n\\end{equation*}\nKeep in mind that $n$ denotes the sample size.\nI was able to figure out that the estimator $T$ is asymptotically unbiased.\nFirst, I determined the expected value of the estimator.\n\\begin{align*}\n    E[T] &= \\frac{1}{2} \\cdot E[X_{1}] + \\frac{1}{2 \\cdot n} \\sum\\limits_{i = 1}^{n} E[X_{i}] \\\\\n    &= \\frac{E[X]}{2} + \\frac{1}{2 \\cdot n} \\cdot (n - 1) \\cdot E[X] \\\\\n    &= \\frac{E[X]}{2} + \\frac{(n - 1)}{2 \\cdot n} \\cdot E[X] \\\\\n    &= \\frac{\\mu}{2} + \\frac{(n - 1) \\cdot \\mu}{2 \\cdot n} \\\\\n    &= \\frac{\\mu}{2} + \\frac{n \\cdot \\mu - \\mu}{2 \\cdot n} \\\\\n\\end{align*}\nSince the expected value does not equal $\\mu$, one can conclude that the estimator $T$ is biased.\nHowever, if we calculate the estimator's bias $b(T)$ and check if it converges to 0, we can see that the estimator is asymptotically unbiased (the calculation of the limit was done using WolframAlpha):\n\\begin{equation*}\n  b(T) = E[T] - \\mu = \\left(\\frac{\\mu}{2} + \\frac{n \\cdot \\mu - \\mu}{2 \\cdot n} \\right) - \\mu\n\\end{equation*}\n\\begin{equation*}\n  \\text{lim}_{n \\rightarrow +\\infty}\\left(\\frac{\\mu}{2} + \\frac{n \\cdot \\mu - \\mu}{2 \\cdot n} - \\mu \\right) = 0\n\\end{equation*}\nUnfortunately, I have not been able to find out whether the estimator $T$ is consistent.\nFrom my understanding we can find out if a biased estimator is consistent by verifying if the mean squared error $MSE$ of the error approaches 0 when the sample size $n$ gets infinitely large.\nIn order to calculate the $MSE$, we need to calculate the variance $VAR$ of the estimator and then subtract the square of the bias $b$ from the variance $VAR$:\n\\begin{equation*}\n\\text{MSE}(T) = \\text{VAR}(T) - b^{2}(T)\n\\end{equation*}\n\\begin{equation*}\n\\text{lim}_{n \\rightarrow +\\infty}\\left(\\text{MSE}(T)\\right) = 0 \\Rightarrow T \\text{ is consistent}\n\\end{equation*}\nThe issue is that I am not able to correctly calculate the MSE.\nI tried many approaches, but I could not figure out what's wrong.\nMy current approach is the following:\n\\begin{align*}\n    \\text{VAR}(T) &= \\frac{1}{2^{2}} \\cdot \\text{VAR}(X_{1}) + \\frac{1}{(2 \\cdot n)^{2}} \\sum\\limits_{i = 1}^{n} \\text{VAR}(X_{i}) \\\\\n    &= \\frac{1}{2^{2}} \\cdot \\text{VAR}(X) + \\frac{1}{(2 \\cdot n)^{2}} \\cdot (n - 1) \\cdot \\text{VAR}(X) \\\\\n    &= \\frac{\\sigma^{2}}{4} + \\frac{(n - 1) \\cdot \\sigma^{2}}{4 \\cdot n^{2}} \\\\\n    &= \\frac{\\sigma^{2}}{4} + \\frac{n \\sigma^{2} - \\sigma^{2}}{4 \\cdot n^{2}} \\\\\n    &= \\frac{n^{2} \\cdot \\sigma^{2}}{4 \\cdot n^{2}} + \\frac{n \\sigma^{2} - \\sigma^{2}}{4 \\cdot n^{2}} \\\\\n    &= \\frac{n^{2} \\cdot \\sigma^{2} + n \\sigma^{2} - \\sigma^{2}}{4 \\cdot n^{2}} \\\\\n\\end{align*}\nThe particular issue lies in finding the value for the square of the bias $b^{2}(T)$.\nI tried many different approaches, but I could not find an equation which makes my calculation work.\nTherefore, my issue is how can I find a sensible equation for $b^{2}(T)$?\nJust for reference, here is my current approach:\nSee WolframAlpha for the expansion of the bias\n\\begin{align*}\nb^{2}(T) &= \\left( \\frac{\\sigma}{2} + \\frac{n \\cdot \\sigma - \\sigma}{2 \\cdot n} - \\sigma \\right)^{2} \\\\\n&= \\frac{\\sigma^{2}}{4} \\\\\n\\end{align*}\n\\begin{align*}\n\\text{MSE}(T) &= \\frac{n^{2} \\cdot \\sigma^{2} + n \\sigma^{2} - \\sigma^{2}}{4 \\cdot n^{2}} - \\frac{\\sigma^{2}}{4}\\\\\n&= \\frac{n^{2} \\cdot \\sigma^{2} + n \\sigma^{2} - 2 \\cdot \\sigma^{2}}{4 \\cdot n^{2}}\n\\end{align*}\n\\begin{equation*}\n\\text{lim}_{n \\rightarrow +\\infty} \\left( \\frac{n^{2} \\cdot \\sigma^{2} + n \\sigma^{2} - 2 \\cdot \\sigma^{2}}{4 \\cdot n^{2}} \\right) = \\frac{\\sigma^{2}}{4}\n\\end{equation*}\nThank you for your help! Grazie mille!\n", "A": "By definition, a consistent estimator converges in probability to a constant as the sample grows larger.\nTo be explicit, let's subscript $T$ with the sample size.  Note that\n$$\\operatorname{Var}(T_n) = \\operatorname{Var}\\left(\\frac{X_1}{2}\\right) + \\operatorname{Var}\\left(\\frac{1}{2n}\\sum_{i=2}^n X_i\\right) \\ge \\operatorname{Var}\\left(\\frac{X_1}{2}\\right) = \\frac{\\sigma^2}{4}.$$\nBecause $T_n,$ being a linear combination of independent Normal variables, has a Normal distribution, it cannot possibly converge to a constant and therefore is not consistent.\nOne quick rigorous proof is to suppose it does converge in probability to a number $\\theta$ and then observe that $\\Pr(|T_n-\\theta|\\ge \\sigma) \\ge \\Phi(1)-\\Phi(-1) \\gt 0$ (where $\\Phi$ is the standard Normal distribution function), demonstrating that it does in fact not converge.\n(If you're unfamiliar with this inequality, use Calculus to minimize the function $\\theta\\to \\Pr(|Z-\\theta|\\ge 1)$ (for a standard normal variable $Z$) by finding the zeros of its derivative.  You will discover the finite critical points occur where the densities at $\\theta\\pm 1$ are equal, immediately giving $\\theta=0.$)\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/495867", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Fourth moment of arch(1) process I have an ARCH(1) process\n\\begin{align*}\nY_t &= \\sigma_t \\epsilon_t, \\\\\n\\sigma_t^2 &= \\omega + \\alpha Y_{t-1}^2,\n\\end{align*}\nand I am trying to express the fourth moment $\\mathbb{E}[Y_t^4]$ in terms of $\\omega$, $\\alpha$ and $\\mathbb{E}[\\epsilon_t^4]$.\n", "A": "For \\begin{align*}\nY_t = \\sigma_t \\epsilon_t, \\qquad \\sigma^2_t = \\omega + \\alpha Y^2_{t-1}, \\qquad \\omega>0, \\alpha \\geq 0,\n\\end{align*}\nwe assume $\\sigma_t$ and $\\epsilon_t$ to be independent. I also assume standard normality for $\\epsilon_t$, so that $E(\\epsilon_t^4)=3$. (You will see from the proof what needs to happen for convergence when the fourth moment is different.)\nConsider a recursion for the 4th moment.\n\\begin{align*}\nE[Y^4_t] &= E[\\sigma^4_t \\epsilon^4_t] = E[\\sigma^4_t] E[\\epsilon^4_t] \\\\\n&= 3 E[\\sigma^4_t] = 3 E[(w + \\alpha Y^2_{t-1})^2] \\\\\n&= 3 E[\\omega^2 + 2\\omega \\alpha Y^2_{t-1} + \\alpha^2 Y^4_{t-1}] \\\\\n&= 3 \\omega^2 + 6 \\omega \\alpha E[Y^2_{t-1}] + 3 \\alpha^2 E[Y^4_{t-1}] \\\\\n&= \\underbrace{ 3 \\omega^2 + \\frac{6 \\omega^2 \\alpha}{1 - \\alpha}}_{=:c} + 3 \\alpha^2 E[Y^4_{t-1}], \\\\\n\\end{align*}\nwhere the last line uses results for the variance of an ARCH(1)-process.\nRepeated substitution yields\n\\begin{align*}\nE[Y^4_t] &= c + 3 \\alpha^2 E[Y^4_{t-1}] \\\\\n&= c + 3 \\alpha^2 (c + 3 \\alpha^2 E[Y^4_{t-2}]) \\\\\n&= c + 3 \\alpha^2 c + (3 \\alpha^2)^2 E[Y^4_{t-2}] \\\\\n&= c + 3 \\alpha^2c + (3 \\alpha^2)^2 (c + 3 \\alpha^2 E[Y^4_{t-3}]) \\\\\n&= c + 3 \\alpha^2 c + (3 \\alpha^2)^2 c + (3 \\alpha^2)^3 E[Y^4_{t-3}]\\\\\n& \\qquad \\qquad \\qquad \\qquad \\vdots \\\\\n&= c \\sum^n_{i=0} (3 \\alpha^2)^i + (3 \\alpha^2)^{n+1} E[Y^4_{t-(n+1)}] \\\\\n\\end{align*}\nFor $E[Y^4_t]$ to be finite we hence need $3 \\alpha^2 < 1$. In this case, we obtain\n\\begin{align*}\nE[Y^4_t] &= c \\sum^\\infty_{i=0} (3 \\alpha^2)^i \\quad\\overset{x:=3 \\alpha^2}{=} c \\sum^\\infty_{i=0} x^i= \\frac{c}{1 - x} \\\\\n&= \\frac{c}{1 - 3 \\alpha^2} \\\\\n& = \\frac{3 w^2 (1 + \\alpha)}{(1 - \\alpha) (1 - 3 \\alpha^2)}. \\\\\n\\end{align*}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/550022", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 0}}
