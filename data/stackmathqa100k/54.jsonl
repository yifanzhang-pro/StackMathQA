{"Q": "Parametric Solvable Septics? Known parametric solvable septics are, \n$$x^7+7ax^5+14a^2x^3+7a^3x+b=0\\tag{1}$$\n$$x^7 + 21x^5 + 35x^3 + 7x + a(7x^6 + 35x^4 + 21x^2 + 1)=0\\tag{2}$$\n$$x^7 - 2x^6 + x^5 - x^4 - 5x^2 - 6x - 4 + n(x - 1)x^2(x + 1)^2=0\\tag{3}$$\n$$x^7 + 7x^6 - 7\\beta x^2 + 28\\beta x + 2\\beta(n - 13)=0\\tag{4}$$\n$$x^7 + 14x^4 + 7(n - 2)x^3 + 14(n - 5)x^2 - 28x - (n^2 + n + 3)=0\\tag{5}$$\nwhere $\\beta = 4(n^2 + 27)$. The first generalizes Demoivre's quintic to 7th powers, the third can be derived from Kluener's database, while the fifth is a variation of the one in this post.  \nIn contrast, many parametric solvable quintics are known, such as the multi-variable,\n$$x^5+10cx^3+10dx^2+5ex+f=0$$\nwhere the coefficients obey the simple quadratic in $f$,\n$$(c^3 + d^2 - c e) \\big((5 c^2 - e)^2 + 16 c d^2\\big) = (c^2 d + d e - c f)^2$$\nQuestion: Surely there are other parametric solvable septics, also simple in form, known by now? Can someone give a sixth (without using transformations on the known ones)?\n", "A": "There is a parametric family of cyclic septics that obey\n$$x_1 x_2 + x_2 x_3 + \\dots + x_7 x_1 - (x_1 x_3 + x_3 x_5 + \\dots + x_6 x_1) = 0\\tag1$$\nas the Hashimoto-Hoshi septic,\n$$\\small x^7 - (a^3 + a^2 + 5a + 6)x^6 + \n  3(3a^3 + 3a^2 + 8a + 4)x^5 + (a^7 + a^6 + 9a^5 - 5a^4 - 15a^3 - 22a^2 - \n        36a - 8)x^4 - a(a^7 + 5a^6 + 12a^5 + 24a^4 - 6a^3 + 2a^2 - 20a - 16)x^3 + a^2(2a^6 + 7a^5 + 19a^4 + 14a^3 + 2a^2 + 8a - 8)x^2 - a^4(a^4 + 4a^3 + 8a^2 + 4)x + a^7=0$$\nFor example, let $a=1$ so,\n$$1 - 17 x + 44 x^2 - 2 x^3 - 75 x^4 + 54 x^5 - 13 x^6 + x^7=0$$\nwhich is the equation involved in $\\cos\\frac{\\pi k}{43}$. If we order its roots as,\n$$x_1,\\,x_2,\\,x_3,\\,x_4,\\,x_5,\\,x_6,\\,x_7 =\\\\\nr_1,\\,r_2,\\,r_5,\\,r_6,\\,r_3,\\,r_7,\\,r_4 =\n\\\\ -0.752399,\\; 0.0721331,\\; 2.63744,\\; 3.62599,\\; 0.480671,\\; 6.29991,\\; 0.636246$$\nwhere the $r_i$ is the root numbering in Mathematica, then it satisfies $(1)$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/145278", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Go I Know Not Whither and Fetch I Know Not What Next day: apparently my original question is harder, by far, than the other bits. So: it is a finite check, I was able to confirm by computer that, if the polynomial below satisfies $$ f(a,b,c,d) \\equiv 0 \\pmod {27}, \\;\\; \\mbox{THEN} \\; \\; a,b,c,d \\equiv 0 \\pmod 3,  $$ and if\n$$ f(a,b,c,d) \\equiv 0 \\pmod {125}, \\;\\; \\mbox{THEN} \\; \\; a,b,c,d \\equiv 0 \\pmod 5,  $$ \nORIGINAL:\n$f$ is a polynomial in four variables. Take matrices\n$$  \n1 =\n\\left( \n\\begin{array}{rrrr}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \n\\end{array}\n\\right),  \n$$\n$$\ni =\n\\left( \n\\begin{array}{rrrr}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \n\\end{array}\n\\right),\n$$\n$$  \nj =\n\\left( \n\\begin{array}{rrrr}\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \n\\end{array}\n\\right),  \n$$\n$$\nk =\n\\left( \n\\begin{array}{rrrr}\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \n\\end{array}\n\\right),\n$$\nThen take\n$$ f(a,b,c,d) = \\det (a \\cdot 1  + b \\sqrt 3 i + c \\sqrt 5 j + d \\sqrt{15} k),   $$\n$$ =a^4-6 a^2 b^2+9 b^4-10 a^2 c^2-30 b^2 c^2+25 c^4+120 a b c d-30 a^2 d^2-90 b^2 d^2-150 c^2 d^2+225 d^4$$.  Note that everything is commutative; \n$$ i^2 = 1, j^2 = 1, k^2 = 1; \\; ij=ji=k, ki=ik=j,jk=kj=i.  $$\nIt is also possible to re-write this with the square roots absorbed into the definitions of $i,j,k.$\nSo, questions include: does it make sense to anyone that, as I checked by brute force, that if\n$$  f(a,b,c,d) \\equiv 0 \\pmod {81}  $$\nthen $a,b,c,d \\equiv 0 \\pmod 3?$ Same for $625$ and $5.$ Need to think about how to check $5$ completely.\nFinally, is it true that this thing represents the same numbers as $x^2 - 15 y^2,$ and what is such a thing called anyway? It might be a field norm, I dunno.\nOh, from a closed question at https://math.stackexchange.com/questions/931769/integer-solution-to-diophantine-equations which I found interesting.\nhttp://en.wikipedia.org/wiki/Go_I_Know_Not_Whither_and_Fetch_I_Know_Not_What\nEDIT: It turns out we may use $27$ in place of $81.$ Evidently explaining this is the hard part. Confirmed, anyway. See what I can do with $125$ instead of $625.$\nEDIT 2: Figured out how to program it; if the polynomial is divisible by $125,$ each variable is indeed divisible by $5.$\n", "A": "Yes, this is a field norm; it is the norm of $a + b \\sqrt{3} + c \\sqrt{5} + d \\sqrt{15}$, from $K = \\mathbb{Q}(\\sqrt{3}, \\sqrt{5})$ down to $\\mathbb{Q}$. Note that $a+b \\sqrt{3} + c \\sqrt{5} + d \\sqrt{15}$ acts on the basis $(1, \\sqrt{3}, \\sqrt{5}, \\sqrt{15})$ by\n$$a \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} + \nb \\begin{pmatrix} 0 & 3 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 5 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} + \nc \\begin{pmatrix} 0 & 0 & 5 & 0 \\\\ 0 & 0 & 0 & 5 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix} + \nd \\begin{pmatrix} 0 & 0 & 0 & 15 \\\\ 0 & 0 & 5 & 0 \\\\ 0 & 3 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\end{pmatrix}.$$\nNow conjugate by the matrix whose diagonal entries are $(1, \\sqrt{3}, \\sqrt{5}, \\sqrt{15})$ to get your matrix. The entries are no longer rational, so I can't think of the result as describing the action on $K$, but the determinant is the same.\n$\\mathbb{Q}(\\sqrt{15})$ has class number $2$ and $K$ is the class field. So, for a prime $p$ other than $2$, $3$, $5$, we have that $\\pm p$ is a value of $x^2-15 y^2$ if and only if $p$ splits principally in $\\mathbb{Q}(\\sqrt{15})$ if and only if $p$ splits in $K$ if and only if $\\pm p$ is a value of $f$. Also, neither $x^2-15 y^2$ nor $f$ can be $3 \\bmod 4$, so the sign is the same in the two cases.\nHowever, they don't take the same set of composite values. Look at $-119 = 7 \\times 17$. We have $61^2 - 15 \\cdot 16^2 = -119$, but, if $7 | f(a,b,c,d)$ then $7^2 | f(a,b,c,d)$. \nI found this by hunting for two primes which are non-principally split in $\\mathbb{Q}(\\sqrt{15})$. In terms of quadratic forms, which I know you love, I needed primes of the form $3 x^2 - 5 y^2$, and I found $7=3 \\cdot 2^2 - 5$ and $-17 = 3 \\cdot 6^2 - 5 \\cdot 5^2$.  Then their product was of the form $x^2-15 y^2$. \nSince these primes split non principally in $\\mathbb{Q}(\\sqrt{15})$, they don't split further in the class field. (We can also directly compute $\\left( \\frac{3}{7} \\right) = \\left( \\frac{3}{17} \\right) = -1$.) So things divisible by one power of $7$ or $17$ are not norms from $K$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/180987", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "9", "answer_count": 1, "answer_id": 0}}
{"Q": "Find all solution $a,b,c$ with $(1-a^2)(1-b^2)(1-c^2)=8abc$ Two years ago, I made a conjecture on stackexchange:\nToday, I tried to find all solutions in integers $a,b,c$ to\n$$(1-a^2)(1-b^2)(1-c^2)=8abc,\\quad a,b,c\\in \\mathbb{Q}^{+}.$$  \nI have found some solutions, such as\n$$(a,b,c)=(5/17,1/11,8/9),(1/7,5/16,9/11),(3/4,11/21,1/10),\\cdots$$\n$$(a,b,c)=\\left(\\dfrac{4p}{p^2+1},\\dfrac{p^2-3}{3p^2-1},\\dfrac{(p+1)(p^2-4p+1)}{(p-1)(p^2+4p+1)}\\right),\\quad\\text{for $p>2+\\sqrt{3}$ and $p\\in\\mathbb {Q}^{+}$}.$$\nHere is another simple solution:\n$$(a,b,c)=\\left(\\dfrac{p^2-4p+1}{p^2+4p+1},\\dfrac{p^2+1}{2p^2-2},\\dfrac{3p^2-1}{p^3-3p}\\right).$$\n\nMy question is: are there solutions of another form (or have we found all solutions)?\n\n", "A": "The original proposer asks for \"simple methods\". Simplicity, like beauty, is in the eye of the beholder. I am sure\nthat Noam Elkies and Joe Silverman feel their answers are extremely simple. The following discussion is, in my humble opinion,\nsimpler.\nWe can express the underlying equation as a quadratic in $a$,\n\\begin{equation*}\na^2+\\frac{8bc}{(b^2-1)(c^2-1)}a-1\n\\end{equation*}\nwith the obvious condition that $|b| \\ne 1$ and $|c| \\ne 1$.\nFor $a$ to be rational, the discriminant must be a rational square, so there exists $D \\in \\mathbb{Q}$ such that\n\\begin{equation*}\nD^2=(c^2-1)^2b^4-2(c^4-10c^2+1)b^2+(c^2-1)^2\n\\end{equation*}\nThis quartic has an obvious rational point when $b=0$, and so is birationally equivalent to an elliptic curve. We find the curve\n\\begin{equation*}\nv^2=u(u+(c^2-1)^2)(u+4c^2)\n\\end{equation*}\nwith the reverse transformation\n\\begin{equation*}\nb=\\frac{v}{(c^2-1)(u+4c^2)}\n\\end{equation*}\nThe elliptic curve has $3$ points of order $2$, which give $b=0$ or $b$ undefined. There are also $4$ points of order $4$ at\n\\begin{equation*}\nu=2c(c^2-1) \\hspace{1cm} v= \\pm 2c(c+1)(c-1)(c^2+2c-1)\n\\end{equation*}\nand\n\\begin{equation*}\nu=-2c(c^2-1) \\hspace{1cm} v= \\pm 2c(c+1)(c-1)(c^2-2c-1)\n\\end{equation*}\nall of which give $|b|=1$. \nThus, to get a non-trivial solution we need the elliptic curve to have rank at least $1$. Numerical investigations suggest that the rank is often $0$, so\nsolutions do not exist for all $c$.\nWe can derive parametric solutions by finding points of the curve, subject to certain conditions.\nFor example, $u=c^2-1$ would give a point if $5c^2-1=\\Box$. We can parametrize this quadric using the solution when $c=1$, to give\n\\begin{equation*}\na=\\frac{(p-2)(p-5)(3p-5)}{p(p-1)(p-3)(2p-5)} \\hspace{1cm} b=\\frac{p^2-4p+5}{2(p^2-5p+5)} \\hspace{1cm} c=\\frac{p^2-4p+5}{p^2-5}\n\\end{equation*}\nwhich gives strictly positive solutions when $p > 5$.\nAnother simple point to consider could be $u=2c^2(c-1)(c+3)$ which gives a rational point when $(c+3)(3c+1)=\\Box$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/208485", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "14", "answer_count": 4, "answer_id": 3}}
{"Q": "sum, integral of certain functions While working on some research, I have encountered an infinite series and its improper integral analogue:\n\\begin{align}\\sum_{m=1}^{\\infty}\\frac1{\\sqrt{m(m+1)(m+2)+\\sqrt{m^3(m+2)^3}}}&=\\frac12+\\frac1{\\sqrt{2}}, \\\\\n\\int_0^{\\infty}\\frac{dx}{\\sqrt{x(x+1)(x+2)+\\sqrt{x^3(x+2)^3}}}&=2.\\end{align}\nThe evaluations were guessed using numerical evidence.\nCan you provide proofs, or any reference (if available)?\n", "A": "For the integral, notice that the expression under the square root is\n  $$ x(x+1)(x+2)+x(x+2)\\sqrt{x(x+2)} = \\frac12\\,x(x+2)(\\sqrt x+\\sqrt{x+2})^2. $$\nConsequently,\n\\begin{align*} \n  \\frac1{\\sqrt{x(x+1)(x+2)+x(x+2)\\sqrt{x(x+2)}}} \n       &= \\frac{\\sqrt 2}{(\\sqrt x+\\sqrt{x+2}) \\sqrt{x(x+2)}} \\\\\n       &= \\frac1{\\sqrt 2}\\,\\frac{\\sqrt{x+2}-\\sqrt{x}}{\\sqrt{x(x+2)}} \\\\\n       &= \\frac1{\\sqrt 2} \\left( \\frac1{\\sqrt x}-\\frac1{\\sqrt{x+2}}\\right);\n\\end{align*}\nthus, the indefinite integral is\n  $$ \\sqrt{2}\\, (\\sqrt x-\\sqrt{x+2})+C $$\nand the result follows easily.\nAs Antony Quas noticed, this also works for the sum showing that the partial sum over $m\\in[1,M]$ is\n  $$ \\frac1{\\sqrt 2} \\sum_{m=1}^M \\frac1{\\sqrt m} - \\frac1{\\sqrt 2} \\sum_{m=3}^{M+2} \\frac1{\\sqrt m} = \\frac1{\\sqrt 2} \\left( 1+\\frac1{\\sqrt 2}\\right) + o(1). $$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/257982", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "Question on a generalisation of a theorem by Euler We call an integer $k\\geq 1$ good if for all $q\\in\\mathbb{Q}$ there are $a_1,\\ldots, a_k\\in \\mathbb{Q}$ such that $$q = \\prod_{i=1}^k a_i \\cdot\\big(\\sum_{i=1}^k a_i\\big).$$\nEuler showed that $k=3$ is good.\nIs the set of good positive integers infinite?\n", "A": "I suspect that $k = 4$ is good, but am not sure how to prove it. However, every positive integer $k \\geq 5$ is good. This follows from the fact (see the proof of Theorem 1 from this preprint) that for any rational number $x$, there are rational numbers $a$, $b$, $c$, $d$ so that $a+b+c+d = 0$ and $abcd = x$. In particular, one can take\n$$\na(x) = \\frac{2(1-4x)^{2}}{3(1+8x)}, b(x) = \\frac{-(1+8x)}{6}, c(x) = \\frac{-(1+8x)}{2(1-4x)}, d(x) = \\frac{18x}{(1-4x)(1+8x)},\n$$\nas long as $x \\not\\in \\{1/4, -1/8\\}$. (For $x = 1/4$ one can take $(a,b,c,d) = (-1/2,1/2,-1,1)$ and for $x = -1/8$ one can take $(a,b,c,d) = (-2/3,25/12,-1/15,-27/20)$.)\nNow, fix $k \\geq 5$, let $q \\in \\mathbb{Q}$ and take $a_{1} = a(q/(k-4))$, $a_{2} = b(q/(k-4))$, $a_{3} = c(q/(k-4))$, $a_{4} = d(q/(k-4))$ and $a_{5} = a_{6} = \\cdots = a_{k} = 1$. We have that\n$$\na_{1} + a_{2} + a_{3} + a_{4} + \\cdots + a_{k} = 0 + a_{5} + \\cdots + a_{k} = k-4\n$$\nand $a_{1} a_{2} a_{3} a_{4} \\cdots = \\frac{q}{k-4} \\cdot 1 \\cdot 1 \\cdots 1 = \\frac{q}{k-4}$. Thus\n$$\n  \\left(\\prod_{i=1}^{k} a_{i}\\right) \\left(\\sum_{i=1}^{k} a_{i}\\right) = \\frac{q}{k-4} \\cdot (k-4) = q.\n$$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/302933", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "23", "answer_count": 2, "answer_id": 0}}
{"Q": "How to find the analytical representation of eigenvalues of the matrix $G$? I have the following matrix arising when I tried to discretize the Green function\uff0c now to show the convergence of my algorithm I need to find the eigenvalues of the matrix $G$ and show it has absolute value less than 1 for certain choices of $N$. \nNote that the explicit formula for entry $(i,j)$ is $-i(N+1-j)$ when $i\\le j$ and it is symmetric, so we can get the formulas for $i>j$ by interchanging $i$ and $j$ in the $i\\le j$ case. \nAny one has any ideas about how to find the analytical representation of eigenvalues of the matrix $G$, i,e, the eigenvalues represented by $N$? Thank you so much for any help!\n$\\begin{pmatrix}\n - N & - N + 1 & -N+2 & -N+3 &\\ldots & 1(-2) & 1(-1) \\\\\n - N + 1 & 2( - N + 1) & 2(-N+2) & 2(-N+3) &\\ddots & 2(-2) & 2(-1) \\\\\n - N + 2 & 2( - N + 2) & 3(-N+2) & 3(-N+3) &\\ddots & 3(-2) & 3(-1) \\\\\n - N + 3 & 2( - N + 3) & 3(-N+3) & 4(-N+3) &\\ddots & 4(-2) & 4(-1) \\\\\n \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n - 2 & 2(-2) & 3(-2) & 4(-2) &\\ddots & ( - 1 + N)( - 2) & ( - 1 + N)( - 1) \\\\\n - 1 & 2(-1) & 3(-1) & 4(-1) &\\ldots & ( - 1 + N)( - 1) & N( - 1) \\\\\n\\end{pmatrix}$\n", "A": "It's straightforward to show that this is the inverse of $1/(N+1)$ times the tridiagonal matrix $T_N$ with $-2$ on its main diagonal and $1$ on its super- and sub-diagonals.\nLet $t_N$ be the characteristic polynomial of $T_N$. We have $t_0(x)=1$, $t_1(x)=x+2$, and by cofactor expansion $t_N(x)=(x+2)t_{N-1}(x)-t_{N-2}(x)$. That is, $t_N$ is related to the Chebyshev polynomials of the second kind by $t_N(x)=U_N(x/2+1)$. The roots of the Chebyshev polynomials are $\\cos(\\frac{k\\pi}{N+1})$ for $k=1,\\dots,N$, so the eigenvalues of the inverse of your $G$ are $\\frac{2}{N+1}(\\cos(\\frac{k\\pi}{N+1})-1)$. These have absolute value less than $1$ for $N\\ge 3$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/308835", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Matrix rescaling increases lowest eigenvalue? Consider the set $\\mathbf{N}:=\\left\\{1,2,....,N \\right\\}$ and let $$\\mathbf M:=\\left\\{ M_i; M_i \\subset \\mathbf N \\text{ such that } \\left\\lvert M_i \\right\\rvert=2 \\text{ or }\\left\\lvert M_i \\right\\rvert=1 \\right\\}$$\nbe the set of all subsets of $\\mathbf{N}$ that are of cardinality $1$ or $2.$\nThe cardinality of the set $\\mathbf M$ itself is $\\binom{n}{1}+\\binom{n}{2}=:K$\nWe can then study for $y \\in (0,1)$ the $K \\times K$ matrix\n$$A_N = \\left(  \\frac{\\left\\lvert M_i \\cap M_j \\right\\rvert}{\\left\\lvert M_i \\right\\rvert\\left\\lvert M_j \\right\\rvert}y^{-\\left\\lvert M_i \\cap M_j \\right\\rvert} \\right)_{i,j}$$\nand \n$$B_N = \\left(  \\left\\lvert M_i \\cap M_j \\right\\rvert y^{-\\left\\lvert M_i \\cap M_j \\right\\rvert} \\right)_{i,j}.$$\nQuestion\nI conjecture that $\\lambda_{\\text{min}}(A_N)\\le \\lambda_{\\text{min}}(B_N)$ for any $N$ and would like to know if one can actually show this?\nAs a first step, I would like to know if one can show that $$\\lambda_{\\text{min}}(A_N)\\le C\\lambda_{\\text{min}}(B_N)$$ for some $C$ independent of $N$?\nIn fact, I am not claiming that $A_N \\le B_N$ in the sense of matrices. But it seems as if the eigenvalues of $B_N$ are shifted up when compared with $A_N.$\nNumerical evidence:\nFor $N=2$ we can explicitly write down the matrices \n$$A_2 =\\left(\n\\begin{array}{ccc}\n \\frac{1}{y} & 0 & \\frac{1}{2 y} \\\\\n 0 & \\frac{1}{y} & \\frac{1}{2 y} \\\\\n \\frac{1}{2 y} & \\frac{1}{2 y} & \\frac{1}{2 y^2} \\\\\n\\end{array}\n\\right) \\text{ and }B_2 = \\left(\n\\begin{array}{ccc}\n \\frac{1}{y} & 0 & \\frac{1}{y} \\\\\n 0 & \\frac{1}{y} & \\frac{1}{y} \\\\\n \\frac{1}{y} & \\frac{1}{y} & \\frac{2}{y^2} \\\\\n\\end{array}\n\\right)$$\nWe obtain for the lowest eigenvalue of $A_2$ (orange) and $B_2$(blue) as a function of $y$\n\nFor $N=3$ we get qualitatively the same picture, i.e. the lowest eigenvalue of $A_3$ remains below the lowest one of $B_3$:\nIn this case: \n$$A_3=\\left(\n\\begin{array}{cccccc}\n \\frac{1}{y} & 0 & 0 & \\frac{1}{2 y} & 0 & \\frac{1}{2 y} \\\\\n 0 & \\frac{1}{y} & 0 & \\frac{1}{2 y} & \\frac{1}{2 y} & 0 \\\\\n 0 & 0 & \\frac{1}{y} & 0 & \\frac{1}{2 y} & \\frac{1}{2 y} \\\\\n \\frac{1}{2 y} & \\frac{1}{2 y} & 0 & \\frac{1}{2 y^2} & \\frac{1}{4 y} & \\frac{1}{4 y} \\\\\n 0 & \\frac{1}{2 y} & \\frac{1}{2 y} & \\frac{1}{4 y} & \\frac{1}{2 y^2} & \\frac{1}{4 y} \\\\\n \\frac{1}{2 y} & 0 & \\frac{1}{2 y} & \\frac{1}{4 y} & \\frac{1}{4 y} & \\frac{1}{2 y^2} \\\\\n\\end{array}\n\\right)\\text{ and } B_3=\\left(\n\\begin{array}{cccccc}\n \\frac{1}{y} & 0 & 0 & \\frac{1}{y} & 0 & \\frac{1}{y} \\\\\n 0 & \\frac{1}{y} & 0 & \\frac{1}{y} & \\frac{1}{y} & 0 \\\\\n 0 & 0 & \\frac{1}{y} & 0 & \\frac{1}{y} & \\frac{1}{y} \\\\\n \\frac{1}{y} & \\frac{1}{y} & 0 & \\frac{2}{y^2} & \\frac{1}{y} & \\frac{1}{y} \\\\\n 0 & \\frac{1}{y} & \\frac{1}{y} & \\frac{1}{y} & \\frac{2}{y^2} & \\frac{1}{y} \\\\\n \\frac{1}{y} & 0 & \\frac{1}{y} & \\frac{1}{y} & \\frac{1}{y} & \\frac{2}{y^2} \\\\\n\\end{array}\n\\right)$$\n\n", "A": "\nClaim. $\\lambda_\\min(A_N) \\le 4\\lambda_\\min(B_N)$.\n\nProof.\nLet $C_N:=\\bigl[\\tfrac{1}{|M_i||M_j|}\\bigr]$. Then, $B_N = A_N \\circ C_N$, where $\\circ$ denotes the Hadamard product. Observe that by construction both $A_N$ and $C_N$ are positive semidefinite, so $B_N$ is also psd. Let's drop the subscript $N$ for brevity. Define $c = \\text{diag}(C)$ sorted in decreasing order, so in particular, $c_\\min = \\min_{1\\le i \\le N} 1/|M_i|^2=1/4$.\nNow, from Theorem 3(ii) of Bapat and Sunder, it follows that:\n\\begin{equation*}\n\\lambda_\\min(B)=\\lambda_\\min(A \\circ C) \\ge \\lambda_\\min(A)c_\\min = \\lambda_\\min(A_N)/4.\n\\end{equation*}\n\nNote: The result of Bapat and Sunder is more general. For psd matrices $A$ and $C$ it states that\n\\begin{equation*}\n\\prod_{j=k}^n \\lambda_j(A\\circ C) \\ge \\prod_{j=k}^n\\lambda_j(A)c_j,\n\\end{equation*}\nwhere $1\\le k \\le n$, and $\\lambda_1(\\cdot)\\ge \\lambda_2(\\cdot) \\ge \\cdots \\ge \\lambda_n(\\cdot)$, while $c$ is as above.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/313470", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 2, "answer_id": 1}}
{"Q": "Asymptotic Expansion of Bessel Function Integral I have an integral:\n$$I(y) = \\int_0^\\infty \\frac{xJ_1(yx)^2}{\\sinh(x)^2}\\ dx $$\nand would like to asymptotically expand it as a series in $1/y$. Does anyone know how to do this? By numerically computing the integral it appears that\n$$I(y) = \\frac 12 - \\frac 1 {\\pi y}+ \\frac {3\\zeta(3)}{4y^3\\pi^3} + O(y^{-5}) $$\nbut this is just (high precision) guesswork and I would like to understand the series analytically.\n", "A": "Inserting the Mellin-Barnes representation for the square of the Bessel function (DLMF),\n\\begin{equation}\n J_{1}^2\\left(xy\\right)=\\frac{1}{2\\pi i}\\int_{c-i\\infty}^{c+i\n\\infty}\\frac{\\Gamma\\left(-t\\right)\\Gamma\\left(2t+3\\right)}{\\Gamma^2\\left(t+2\\right)\\Gamma%\n\\left(t+3\\right)}\\left(\\frac{xy}{2}\\right)^{2t+2}\\,dt\n\\end{equation} \nwhere $-3/2<\\Re (c)<0$, and changing the order of integration, one obtains\n\\begin{equation}\n I(y)=\\frac{1}{2\\pi i}\\int_{c-i\\infty}^{c+i\n\\infty}\\frac{\\Gamma\\left(-t\\right)\\Gamma\\left(2t+3\\right)}{\\Gamma^2\\left(t+2\\right)\\Gamma%\n\\left(t+3\\right)}\\left(\\frac{y}{2}%\n\\right)^{2t+2}\\,dt\\int_0^\\infty \\frac{x^{2t+3}}{\\sinh^2x}\\,dx\n\\end{equation} \nFrom G. & R. (3.527.1)\n\\begin{equation}\n\\int_0^\\infty \\frac{x^{2t+3}}{\\sinh^2x}\\,dx=\\frac{1}{2^{2t+2}}\\Gamma\\left( 2t+4 \\right)\\zeta\\left( 2t+3 \\right)\n\\end{equation} \nvalid for $t>-1$. Thus we choose $-1<\\Re(c)<0$\nand thus\n\\begin{equation}\n I(y)=\\frac{1}{2\\pi i}\\int_{c-i\\infty}^{c+i\n\\infty}\\frac{\\Gamma\\left(-t\\right)\\Gamma\\left(2t+3\\right)\\Gamma\\left( 2t+4 \\right)\\zeta\\left( 2t+3 \\right)}{\\Gamma^2\\left(t+2\\right)\\Gamma\\left(t+3\\right)}\\left(\\frac{y}{4}\\right)^{2t+2}\\,dt\n\\end{equation} \nTo evaluate asymptotically this integral, one can close the contour by the large left half-circle. Poles are situated at $t=-1$ and $t=-\\frac{2n+1}{2}$, with $n=1,2,3\\ldots$. With the help of a CAS, the first corresponding residues are:\n\\begin{equation}\n R_{-1}=\\frac{1}{2}\\quad ;\\quad R_{-3/2}=-\\frac{1}{4\\pi}\\quad ;\\quad R_{-5/2}=-\\frac{3}{64\\pi}\\zeta'(-2)\\quad ;\\quad R_{-7/2}=\\frac{15}{8192\\pi}\\zeta'(-4)\n\\end{equation} \n(General expressions can probably be found, if necessary). The derivative of the Riemann Zeta function at even integer values are involved and can be simply expressed. We obtain finally\n\\begin{equation}\n I(y)=\\frac{1}{2}-\\frac{1}{\\pi}y^{-1}+\\frac{3\\zeta(3)}{4\\pi^3}y^{-3}+\\frac{45\\zeta(5)}{32\\pi^5}y^{-5}+O\\left( y^{-7} \\right)\n\\end{equation} \n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/315264", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 1, "answer_id": 0}}
{"Q": "eigenvalues of a symmetric matrix I have a special $N\\times N$ matrix with the following form. It is symmetric and zero row (and column) sums.\n$$K=\\begin{bmatrix}\nk_{11} & -1 & \\frac{-1}{2} & \\frac{-1}{3} & \\frac{-1}{4} & \\ldots &\n\\frac{-1}{N-2} & \\frac{-1}{N-1} & \\\\\n-1 & k_{22} & \\frac{-1}{2} & \\frac{-1}{3} & \\frac{-1}{4} & \\ldots &\n\\frac{-1}{N-2} & \\frac{-1}{N-1} & \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots &\n\\\\\n\\frac{-1}{N-2} & \\frac{-1}{N-2} & \\frac{-1}{N-2} & \\frac{-1}{N-2} & \\frac{-1}{N-2} & \\ldots & k_{N-1,N-1} & \\frac{-1}{N-1} & \\\\\n\\frac{-1}{N-1} & \\frac{-1}{N-1} & \\frac{-1}{N-1} & \\frac{-1}{N-1} & \\frac{-1}{N-1} & \\ldots & \\frac{-1}{N-1} & 1 & \\\\\n\\end{bmatrix}\n$$\nwhere\n$K_{ii}=\\sum_{j=1, j\\ne i}^{N}{(-k_{ij})}$  for $i=1, 2,3,\\ldots , N\n$\nFor example if N=4, we have:\n$$K = \\begin{bmatrix}\n11/6 & -1 & -1/2 & -1/3 & \\\\\n-1 & 11/6 & -1/2 & -1/3 & \\\\\n-1/2 & -1/2 & 4/3 & -1/3 & \\\\\n-1/3 & -1/3 & -1/3 & 1 & \\\\\n\\end{bmatrix}\n$$\nHow can I find an explicit equation for its eigenvalues?  \n", "A": "Phillip Lampe seems to be correct. Here are the eigenvalues and eigenvectors computed by hand:\nLet $k_1 = 2 + \\tfrac12 + \\cdots + \\tfrac{1}{N-1}$, then:\n$\\lambda_0 = 0$ with eigenvector all ones (by construction).\n$\\lambda_1 = k_{1}$ with eigenvector $\\begin{bmatrix}-1& 1&  0&\\cdots& 0\\end{bmatrix}^T$\n$\\lambda_2 = k_1-1$ with eigenvector $\\begin{bmatrix}-\\tfrac12& -\\tfrac12& 1& 0 &\\cdots& 0\\end{bmatrix}^T$\n$\\lambda_3 = k_1 -1- \\tfrac12$ with eigenvector $\\begin{bmatrix}-\\tfrac13& -\\tfrac13& -\\tfrac13& 1& 0&\\cdots& 0\\end{bmatrix}^T$\n$\\lambda_4 = k_1 - 1-\\tfrac12 - \\tfrac13$ with eigenvector $\\begin{bmatrix}-\\tfrac14& \\cdots& -\\tfrac14& 1& 0&\\cdots &0\\end{bmatrix}^T$\nand so on until\n$\\lambda_{N-1} =  k_1 -1-\\tfrac12-\\cdots-\\tfrac{1}{N-2} = 1 + \\tfrac{1}{N-1} = \\tfrac{N}{N-1}$ with eigenvector $\\begin{bmatrix}-\\tfrac1{N-1}& \\cdots& -\\tfrac{1}{N-1}& 1\\end{bmatrix}^T$.\nSo in short: The eigenvalues are $0$ and the values\n$\\lambda_j = 1+\\sum_{i=j}^{N-1}\\tfrac1i$ for $j=1,\\dots,N-1$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/324165", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "Representation of $4\\times4$ matrices in the form of $\\sum B_i\\otimes C_i$ Every matrix $A\\in M_4(\\mathbb{R})$\ncan be represented in the form of $$A=\\sum_{i=1}^{n(A)} B_i\\otimes C_i$$ for $B_i,C_i\\in M_2(\\mathbb{R})$.\nWhat is the least uniform upper bound $M$ for such $n(A)$? In other words, what is the least integer $M$ such that every $A$  admit such a representation with $n(A)\\leq M$?\nIs this least upper bound equal to the corresponding least upper bound for all matrices $A$ which are a matrix representation of quaternions $a+bi+cj+dk$?\nAs another question about tensor product representation: What is a sufficient condition for a $4\\times 4$ matrix $A$ to be represented in the form of $A=B\\otimes C -C\\otimes B$?\n", "A": "Because $M_4(\\mathbb R) = M_2(\\mathbb R) \\otimes M_2(\\mathbb R)$ as vector spaces (and as algebras, but we won't use this), we can replace $M_2(\\mathbb R)$ by an arbitrary $4$-dimensional vector space $V$ and $M_4(\\mathbb R)$ by $V \\otimes V$. We can represent elements of $V\\otimes V$ conveniently as $4\\times 4$ matrices, where simple tensors are rank one matrices. The question is then equivalent to asking the minimum number of rank $1$ matrices it takes to write a $4 \\times 4$ matrix as a sum of rank $1$ matrices. The answer is obviously $4$.\nFor quaternions acting by left multiplication by quaternions in the standard basis:\n$$1= \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\0 & 1 \\end{pmatrix} \\otimes\\begin{pmatrix} 1 & 0 \\\\0 & 1 \\end{pmatrix}$$\n$$i=  \\begin{pmatrix} 0 & -1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 \\\\1 & 0 \\end{pmatrix} \\otimes\\begin{pmatrix} 1 & 0 \\\\0 & 1 \\end{pmatrix}$$\n$$j=  \\begin{pmatrix} 0 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\otimes\\begin{pmatrix} 0 & -1 \\\\1 & 0 \\end{pmatrix}$$\n$$k=  \\begin{pmatrix} 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\otimes\\begin{pmatrix} 0 & -1 \\\\1 & 0 \\end{pmatrix}$$\nBecause only two terms appear on the right side, the matrices clearly have rank at most $2$ at tensor product, and some obtain rank exactly $2$, so the answer is two.\nFor the last question, the answer is isomorphic to the analogous answer for writing an element of $V \\otimes V$ as $v_1 \\otimes v_2 - v_2 \\otimes v_1$ for $V$ a four-dimensional vector space. The condition is given by a skew-symmetry condition (i.e. 10 linear conditions) plus a Pfaffian condition (a quadratic condition). More precisely the general such matrix can be written as\n$$ \\begin{pmatrix} 0 & a & -a & 0 \\\\ b & c & d & e \\\\ -b & -d & -c & -e \\\\ 0 & f & -f & 0 \\\\ \\end{pmatrix}$$\nsuch that $af-be+cd=0$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/331525", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Primality test for specific class of $N=8kp^n-1$ My following question  is related to my  question here\nCan you provide a proof or a counterexample for the following claim :\n\nLet $P_m(x)=2^{-m}\\cdot \\left(\\left(x-\\sqrt{x^2-4}\\right)^{m}+\\left(x+\\sqrt{x^2-4}\\right)^{m}\\right)$ . Let $N=8kp^n-1$ such that $k>0$ , $3 \\not\\mid k$ , $p$ is a prime number, $p \\neq 3$ , $n > 2$ and $8k<p^n$ . Let $S_i=P_p(S_{i-1})$ with $S_0=P_{2kp^2}(4)$ , then: \n  $$N \\text{ is a prime iff } S_{n-2} \\equiv 0\\pmod{N}$$\n\nYou  can run this test here.\nEDIT \nI have verified this claim for $k \\in [1,500]$ with $p \\leq 139$ and $n \\in [3,50]$ .\n", "A": "This is a partial answer.\nThis answer proves that if $N$ is a prime, then $S_{n-2}\\equiv 0\\pmod N$.\nProof : \nIt can be proven by induction that\n$$S_i=(2-\\sqrt 3)^{2kp^{i+2}}+(2+\\sqrt 3)^{2kp^{i+2}}\\tag1$$\nUsing $(1)$ and $2\\pm\\sqrt 3=\\bigg(\\frac{\\sqrt{6}\\pm\\sqrt 2}{2}\\bigg)^2$, we get\n$$\\begin{align}&2^{N+1}S_{n-2}^2-2^{N+2}\n\\\\\\\\&=(\\sqrt 6-\\sqrt 2)(\\sqrt 6-\\sqrt 2)^{N}+(\\sqrt 6+\\sqrt 2)(\\sqrt 6+\\sqrt 2)^{N}\n\\\\\\\\&=\\sqrt 6\\bigg((\\sqrt 6+\\sqrt 2)^{N}+(\\sqrt 6-\\sqrt 2)^{N}\\bigg)\n\\\\&\\qquad\\qquad +\\sqrt 2\\bigg((\\sqrt 6+\\sqrt 2)^{N}-(\\sqrt 6-\\sqrt 2)^{N}\\bigg)\n\\\\\\\\&=\\sqrt 6\\sum_{i=0}^{N}\\binom Ni(\\sqrt 6)^{N-i}\\bigg((\\sqrt 2)^i+(-\\sqrt 2)^i\\bigg)\n\\\\&\\qquad\\qquad +\\sqrt 2\\sum_{i=0}^{N}\\binom Ni(\\sqrt 6)^{N-i}\\bigg((\\sqrt 2)^i-(-\\sqrt 2)^i\\bigg)\n\\\\\\\\&=\\sum_{j=0}^{(N-1)/2}\\binom N{2j}6^{(N+1-2j)/2}\\cdot 2^{j+1}+\\sum_{j=1}^{(N+1)/2}\\binom N{2j-1}6^{(N-2j+1)/2}\\cdot 2^{1+j}\n\\\\\\\\&\\equiv 6^{(N+1)/2}\\cdot 2+2^{(N+3)/2}\\pmod N\n\\\\\\\\&\\equiv 12\\cdot 2^{(N-1)/2}\\cdot 3^{(N-1)/2}+4\\cdot 2^{(N-1)/2}\\pmod N\n\\\\\\\\&\\equiv 12\\cdot (-1)^{(N^2-1)/8}\\cdot \\frac{(-1)^{(N-1)/2}}{\\bigg(\\frac N3\\bigg)}+4\\cdot (-1)^{(N^2-1)/8}\\pmod N\n\\\\\\\\&\\equiv 12\\cdot 1\\cdot \\frac{-1}{1}+4\\cdot 1\\pmod N\n\\\\\\\\&\\equiv -8\\pmod N\\end{align}$$\nSo, we get\n$$2^{N+1}S_{n-2}^2-2^{N+2}\\equiv -8\\pmod N$$\nIt follows from $2^{N-1}\\equiv 1\\pmod N$ that\n$$S_{n-2}\\equiv 0\\pmod N$$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/361489", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "How small the radical of $xy(x+y)uv(u+v)$ can be infinitely often? Let $x,y,u,v$ be positive integers with $x,y$ coprime and $u,v$ coprime\n( $xy,uv$ not necessarily coprime). Assume $x+y \\ne u+v$.\nHow small the radical of $xy(x+y)uv(u+v)$ can be infinitely often?\nCan we get $O(|(x+y)(u+v)|^{1-C})$ for $C>0$?\nThese are just two pairs of good $abc$ triples so we can get $C=0$\nwith pairwise coprimality.\n", "A": "Here is a solution where the radical is $O(k^9)$ and $(x+y)(u+v)=O(k^{12})$\nThe idea is that $x,y,z=a^2,b^2,c^2$ for a Pythagorean triple and $u,v,u+v=A^2,B^2,C^2$ for another with $C=c^2.$ I used the most familiar type of triple (hypotenuse and long leg differ by $1$), there might be others that do better, or special values of $k.$\n\n*\n\n*$x=(2k+1)^2$\n\n*$y=\\left(2k(k+1)\\right)^2$\n\n*$u=(y^2-x^2)^2=((2k^2-1)(2k^2+4k+1))^2$\n\n*$v=(2xy)^2=(4k(k+1)(2k+1))^2$\nthen\n\n*\n\n*$x+y=(2k^2+2k+1)^2$\n\n*$u+v=(2k^2+2k+1)^4$\nThus $(x+y)(u+v)=O(k^{12})$\nBut the radical of $xy(x+y)uv(u+v)$ is at most\n$$k(k+1)(2k+1)(2k^2-1)(2k^2+2k+1)(2k^2+4k+1)=O(k^9)$$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/377124", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 0}}
{"Q": "Is there a nonzero solution to this infinite system of congruences? Is there a triple of nonzero even integers $(a,b,c)$ that satisfies the following infinite system of congruences?\n$$\na+b+c\\equiv 0 \\pmod{4} \\\\\na+3b+3c\\equiv 0 \\pmod{8} \\\\\n3a+5b+9c\\equiv 0 \\pmod{16} \\\\\n9a+15b+19c\\equiv 0 \\pmod{32} \\\\\n\\vdots \\\\\ns_na + t_nb + s_{n+1}c \\equiv 0 \\pmod{2^{n+1}} \\\\\n\\vdots\n$$\nwhere $(s_n)$ and $(t_n)$ are weighted tribonacci sequences defined by\n$$\ns_1=s_2=1, \\\\\ns_3=3, \\\\\ns_n = s_{n-1} +2s_{n-2} + 4s_{n-3} \\text{ for } n>3,\n$$\nand\n$$\nt_1=1, \\\\\nt_2=3, \\\\\nt_3=5, \\\\\nt_n = t_{n-1} +2t_{n-2} + 4t_{n-3} \\text{ for } n>3.\n$$\nI think there are no nonzero solutions, but I haven't been able to prove this. Computationally, I found there are no nonzero solutions for integers $a$, $b$, and $c$ up to $1000$.\nNote the $s_n$ and $t_n$ are always odd, and that the ratios $\\frac{s_n}{s_{n-1}}$ and $\\frac{t_n}{t_{n-1}}$ approach $2.4675...$.\n", "A": "$u_n=s_na + t_nb + s_{n+1}c$ satisfies the same recurrence relation as $s_n$ and $t_n$: $u_n = u_{n-1} +2u_{n-2} + 4u_{n-3}$. The question is whether $2^{n+1}\\mid u_n$.\nSince $v_n=u_n/2^{n+1}$ satisfies\n$v_n = \\displaystyle\\frac{v_{n-1} +v_{n-2} + v_{n-3}}{2}$\nthe answer is affirmative only if there are $v_0, v_1, v_2$ (not all 0) such that $v_n$ is always integral.\nEDIT.\nAs sharply noticed by the OP, the attempt below was wrong, since a matrix I claimed to be invertible (mod $2$) is in fact singular. A similar, more computational argument does work (mod $5$).\n\nIt's clear that (mod $2$) such a sequence $v_n$ must follow either one of the 3-periodic patterns $000$ and $110$ (up to shifts). In the $000$ case keep dividing entire sequence $(v_n)$ by $2$ until one term is odd, and then shift the sequence to start with that term, so it's back to the $110$ case.  Therefore it must be that \n$$\\require{cancel}\\cancel{\\det\\left (\\begin{smallmatrix}v_1 & v_2 & v_3\\\\ v_2 & v_3 & v_4\\\\ v_3 & v_4 & v_5 \\end{smallmatrix}\\right )\\equiv \\det\\left (\\begin{smallmatrix}1 & 1 & 0\\\\ 1 & 0 & 1\\\\ 0 & 1 & 1 \\end{smallmatrix}\\right )\\!\\!\\!\\!\\pmod{2} \\ne0}$$\nCORRECTED ARGUMENT.\nNotice that\n$$D=\\det\\left (\\begin{matrix}v_1 & v_2 & v_3\\\\ v_2 & v_3 & v_4\\\\ v_3 & v_4 & v_5 \\end{matrix}\\right )=\\det\\left (\\begin{matrix}v_1 & v_2 & v_3\\\\ v_2 & v_3 & \\frac{v_1+v_2+v_3}{2}\\\\ v_3 & \\frac{v_1+v_2+v_3}{2} & \\frac{v_1+3v_2+3v_3}{4} \\end{matrix}\\right )\\\\=\n\\frac{-4v_3^3+4v_2 v_3^2+2v_1 v_3^2+v_2^2 v_3+5v_1 v_2 v_3-v_1^2 v_3-3v_2^3-2v_1 v_2^2-2v_1^2 v_2-v_1^3}{4}$$\nis $\\equiv 0 \\pmod{5}$ if and only if $v_1\\equiv v_2\\equiv v_3\\equiv 0 \\pmod{5}$. This is proved by the following snippet of code:\nawk -vp=5 'BEGIN {\n    for(a=0; a<p; a++)\n        for(b=0; b<p; b++)\n            for(c=0; c<p; c++) {\n                d=4*c^3-4*b*c^2-2*a*c^2-b^2*c-5*a*b*c+a^2*c+3*b^3+2*a*b^2+2*a^2*b+a^3;\n                if(d%p==0) print a, b, c;\n            }\n}'\n\nNow divide the entire sequence $(v_n)$ by a power of $5$ so that at least one term is is not $\\equiv 0$, and shift it to start with that term, thus $v_1\\not\\equiv 0$ and therefore $D\\ne0$.\nNext this implies that there is an integral linear combination $(z_n)$ of $(v_n)$ and its shifts $(v_{n+1})$, $(v_{n+2})$ such that $z_1=z_2=0$, $z_3\\ne 0$, and still $z_n=(z_{n-1} +z_{n-2} + z_{n-3})/2$ holds.\nFinally write $z_3=2^m d$, with $d$ odd, and start the recursion from $0, 0, 2^m d$ to easily notice that it runs into a half-integer in $m+1$ steps.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/381057", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 2, "answer_id": 1}}
{"Q": "An explicit equation for $X_1(13)$ and a computation using MAGMA By a general theory $X_1(13)$ is smooth over $\\mathbb{Z}[1/13]$, and so is its Jacobian $J$.\nAnd the hyperelliptic curve given by an affine model $y^2 = x^6 - 2x^5 + x^4 -2x^3 + 6x^2 -4x + 1$ is $X_1(13)$.\nHowever, according to MAGMA, $J$ is bad at $2$.\n\nWhat is wrong with my argument?\n\nHere is my code:\nP<x> := PolynomialRing(RationalField());\nC := HyperellipticCurve(x^6 - 2 * x^5 + x^4 - 2 * x^3 + 6 * x^2 - 4 * x +1);\nJ := Jacobian(C);\nBadPrimes(J);\n\n", "A": "To get a model with good reduction at $2$, take $y = 2Y + x^3 + x^2 + 1$,\nsubtract $(x^3+x^2+1)^2$ from both sides, and divide by $4$ to get\n$$ Y^2 + (x^3+x^2+1) \\, Y = -x^5-x^3+x^2-x. $$\n(A similar tactic of un-completing the square\nis well-known for elliptic curves.)\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/385038", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Improper integral $\\int_0^\\infty\\frac{x^{2n+1}}{(1+x^2)^2(2+x^2)^N}dx,\\ \\ \\ n\\le N$ How can I evaluate this integral?\n$$\\int_0^\\infty\\frac{x^{2n+1}}{(1+x^2)^2(2+x^2)^N}dx,\\ \\ \\ n\\le N$$\nMaybe there is a recurrence relation for the integral?\n", "A": "Let us renumber $N=n+L$ and let $K_{n,L} = I_{n,n+L} = \\frac{1}{2} \\int_0^\\infty \\frac{y^n}{(1+y)^2 (2+y)^{n+L}} \\, dy$, which is the desired integral after the variable change $y=x^2$. Let $K(s,t) = \\sum_{L=0}^\\infty \\sum_{n=0}^\\infty K_{n,L} s^L t^n$. For small enough $s$ and $t$, the integrands converge uniformly over the interval of integration, so we can exchange the summation with the integration to get\n\\begin{align*}\n  K(s,t) &= \\frac{1}{2} \\int_0^\\infty \\frac{(2+y)^2}{(1+y)^2 (2-s+y) (2+(1-t)y)} \\, dy \\\\\n  &= \\frac{1}{2\\pi i} \\frac{1}{2} \\oint_\\gamma \\frac{\\log(-y) (2+y)^2}{(1+y)^2 (2-s+y) (2+(1-t)y)} \\, dy,\n\\end{align*}\nwhere the complex contour $\\gamma$ tightly encircles the positive real line clockwise. Deforming the contour to counter-clockwise encircle the poles at $y=-1,-2+s,-2/(1-t)$, we get\n\\begin{align*}\n  K(s,t) &= \\sum_{z=-1,-2+s,-\\frac{2}{(1-t)}} \\operatorname{Res}_{y=z}\n    \\frac{1}{2} \\frac{\\log(-y) (2+y)^2}{(1+y)^2 (2-s+y) (2+(1-t)y)} \\\\\n  &= \\frac{1}{2} \\frac{1}{(1-s)(1+t)}\n    + \\frac{1}{2} \\frac{s^2 \\log(2-s)}{(1-s)^2 (s+2t-st)}\n    - \\frac{1}{2} \\frac{4t^2 \\log(\\frac{2}{1-t})}{(1+t)^2 (s+2t-st)} \\\\\n  &= \\frac{1}{2} \\frac{1}{(1-s)(1+t)}\n\\\\ & \\quad {}\n    + \\frac{1}{2(1-t)(s+\\frac{2t}{1-t})} \\left( \\frac{s^2 \\log(2-s)}{(1-s)^2}\n      - \\frac{(\\frac{2t}{1-t})^2 \\log(2+\\frac{2t}{1-t})}{(1+\\frac{2t}{1-t})^2} \\right) .\n\\end{align*}\nNote that the last term is of the form $(f(x)-f(y))/(x-y)$ for $x=s$ and $y=-\\frac{2t}{1-t}$, so that this ratio is expressible as some $g(x,y)$ that is analytic at $x=y$, which tells you how an expansion in $s$ and $t$ is possible despite the pesky factor of $1/(s+\\frac{2t}{1-t})$.\nThis answer is the same as Neil Strickland's. But he didn't specify how to do the expansion in the presence of the pesky denominator mentioned above.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/393753", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 4, "answer_id": 2}}
{"Q": "General formulas for derivative of $f_n(x)=\\dfrac{ax^n+bx^{n-1}+cx^{n-2}+\\cdots}{a'x^n+b'x^{n-1}+c'x^{n-2}+\\cdots},\\quad a'\\neq0$ For the function $f_1(x)=\\dfrac{ax+b}{a'x+b'},\\quad a'\\neq0$ , we have $$f_1'(x)=\\dfrac{\\begin{vmatrix}{a} && {b} \\\\ {a'} && {b'}\\end{vmatrix}}{(a'x+b')^2}$$\nFor $f_2(x)=\\dfrac{ax^2+bx+c}{a'x^2+b'x+c'},\\quad a'\\neq0$, we have\n$$f_2'(x)=\\dfrac{{ \\begin{vmatrix}{a} && {b} \\\\ {a'} && {b'}\\end{vmatrix} }x^2+2{ \\begin{vmatrix}{a} && {c} \\\\ {a'} && {c'}\\end{vmatrix} }x+{ \\begin{vmatrix}{b} && {c} \\\\ {b'} && {c'}\\end{vmatrix} }}{(a'x^2+b'x+c')^2}$$\nCan we generalize the formula containing determinants to find $f_n'(x)$?\n", "A": "If $f_n=\\frac{P_n(x)}{Q_n(x)}$ and $P_n=\\sum a_kx^k$,\n$Q_n=\\sum b_kx^k$, then\n$$f'_n=\\frac{\\begin{vmatrix}{P'} && {Q'} \\\\ {P} && {Q}\\end{vmatrix} }{Q^2}$$\nBreaking the determinant on the numerator gives $$\\sum_{j\\geq 0} \\left(\\sum_{k+r=j+1} k\\begin{vmatrix}{a_{k}} && {b_k} \\\\ {a_{j+1-k}} && {b_{j+1-k}}\\end{vmatrix} \\right)x^j=\\sum_{j=0}^{2n-1}c_jx^j$$\nNow, $k,r \\leq n$ implies $ n\\geq k,r \\geq (j+1)-n ; k>0$.\nAlso, we can further simplify $c_j$ as $$c_j=\\sum_{k=j+1-n}^{\\lfloor{\\frac{j+1}{2}}\\rfloor} k\\begin{vmatrix}{a_k} && {b_k} \\\\ {a_{j+1-k}} && {a_{j+1-k}}\\end{vmatrix}+ (j+1-k)\\begin{vmatrix}{a_{j+1-k}} && {b_{j+1-k}} \\\\ {a_k} && {b_k}\\end{vmatrix}$$\nHence, $$c_j=\\sum_{k=j+1-n}^{\\lfloor{\\frac{j+1}{2}}\\rfloor} (j+1-2k)\\begin{vmatrix}{a_{j+1-k}} && {b_{j+1-k}} \\\\ {a_{k}} && {b_{k}}\\end{vmatrix}$$\nThis gives $c_{2n-1}=0$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/396250", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 2, "answer_id": 1}}
{"Q": "Any hints on how to prove that the function $\\lvert\\alpha\\;\\sin(A)+\\sin(A+B)\\rvert - \\lvert\\sin(B)\\rvert$ is negative over the half of the total area? I have this inequality with $0<A,B<\\pi$ and a real $\\lvert\\alpha\\rvert<1$:\n$$   f(A,B):=\\bigl|\\alpha\\;\\sin(A)+\\sin(A+B)\\bigr| - \\bigl| \\sin(B)\\bigr| < 0$$\nNumerically, I see that regardless of the value of $\\alpha$, the area in which $f(A,B)<0$ is always half of the total area $\\pi^2$.\nI appreciate any hints and comments on how I can prove this.\n", "A": "Let us assume $\\alpha\\in[0,1)$ (the case of $\\alpha\\in(-1,0]$ is similar). As $\\sin B>0$ for $B\\in (0,\\pi)$,  the inequality $f(A,B)<0$ amounts to\n$$\n\\alpha\\sin A<\\sin B-\\sin(A+B),\\quad -[\\sin B+\\sin(A+B)]<\\alpha\\sin A.\\quad (\\star)\n$$\nNotice that\n$\\sin A=2\\sin\\left(\\frac{A}{2}\\right)\\cos\\left(\\frac{A}{2}\\right)$,\n$\\sin B-\\sin(A+B)=-2\\sin\\left(\\frac{A}{2}\\right)\\cos\\left(\\frac{A}{2}+B\\right)$ and\n$\\sin B+\\sin(A+B)=2\\cos\\left(\\frac{A}{2}\\right)\\sin\\left(\\frac{A}{2}+B\\right)$.\nSubstituting in $(\\star)$ and cancelling the positive terms\n$2\\sin\\left(\\frac{A}{2}\\right)$ and $2\\cos\\left(\\frac{A}{2}\\right)$, we obtain the equivalent inequalities\n$$\n\\alpha\\cos\\left(\\frac{A}{2}\\right)<-\\cos\\left(\\frac{A}{2}+B\\right), \\quad \n-\\sin\\left(\\frac{A}{2}+B\\right)<\\alpha\\sin\\left(\\frac{A}{2}\\right).\\quad (\\star\\star)\n$$\nIn $(\\star\\star)$, the LHS of the first inequality and the RHS of the second are non-negative. Hence $\\frac{A}{2}+B$  - which belongs to $\\left(0,\\frac{3\\pi}{2}\\right)$ - must be in the second or the third quadrant; otherwise, the first inequality in $(\\star\\star)$ does not hold. Let us analyze these cases separately:\n\n*\n\n*If $\\frac{\\pi}{2}\\leq\\frac{A}{2}+B\\leq\\pi$, then the second inequality in $(\\star\\star)$ holds automatically (its RHS is always non-negative); and the first one can be written as\n$$\\alpha\\cos\\left(\\frac{A}{2}\\right)<\\cos\\left(\\pi-\\frac{A}{2}-B\\right).$$ Applying the strictly decreasing function $\\cos^{-1}:[0,1]\\rightarrow\\left[0,\\frac{\\pi}{2}\\right]$ yields:\n$$\\pi-\\frac{A}{2}-B<\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right).$$\nThis of course implies $\\frac{A}{2}+B\\geq\\frac{\\pi}{2}$. But we also need $\\frac{A}{2}+B\\leq\\pi$. Combining these, the bounds for $B$ in terms of $A\\in(0,\\pi)$ are given by\n$$\n\\pi-\\frac{A}{2}-\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right)\\leq B\\leq\\pi-\\frac{A}{2}.\n$$\nThe difference of the two bounds is $\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right)$. Consequently, the contribution to the area of\n$\\{(A,B)\\mid f(A,B)<0\\}$ is\n$$\n\\int_{\\{(A,B)\\mid f(A,B)<0,\\, \\frac{\\pi}{2}\\leq\\frac{A}{2}+B\\leq\\pi\\}}\\mathbf{1}=\n\\int_{0}^\\pi\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right){\\rm{d}}A.\\quad (1)\n$$\n\n*If $\\pi\\leq\\frac{A}{2}+B\\leq\\frac{3\\pi}{2}$, all terms appearing in $(\\star\\star)$ are non-negative. We first rewrite these inequalities as\n$$\n\\alpha\\cos\\left(\\frac{A}{2}\\right)<\\cos\\left(\\frac{A}{2}+B-\\pi\\right), \\quad \n\\sin\\left(\\frac{A}{2}+B-\\pi\\right)<\\alpha\\sin\\left(\\frac{A}{2}\\right). \n$$\nNext applying strictly monotonic functions\n$\\cos^{-1}:[0,1]\\rightarrow\\left[0,\\frac{\\pi}{2}\\right]$\nand $\\sin^{-1}:[0,1]\\rightarrow\\left[0,\\frac{\\pi}{2}\\right]$ to them results in:\n$$\n\\frac{A}{2}+B-\\pi<\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\quad \n\\frac{A}{2}+B-\\pi<\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right).\n$$\nHence the upper bound\n$$\nB<\\pi+\\min\\left\\{\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\\right\\}-\\frac{A}{2}\n$$\nwhich of course implies $\\frac{A}{2}+B\\leq\\frac{3\\pi}{2}$. But $\\pi\\leq\\frac{A}{2}+B$ is also required. We therefore arrive at the bounds for $B$ in terms of $A\\in(0,\\pi)$:\n$$\n\\pi-\\frac{A}{2}\\leq B\\leq \n\\pi+\\min\\left\\{\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\\right\\}-\\frac{A}{2}.\n$$\nThe difference of the bounds is $\\min\\left\\{\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\\right\\}$. Therefore, the contribution to the area of\n$\\{(A,B)\\mid f(A,B)<0\\}$ is\n$$\n\\int_{\\{(A,B)\\mid f(A,B)<0,\\, \\pi\\leq\\frac{A}{2}+B\\leq\\frac{3\\pi}{2}\\}}\\mathbf{1}\\\\\n=\\int_{0}^\\pi\\min\\left\\{\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\\right\\}{\\rm{d}}A.\\quad (2)\n$$\nAdding $(1)$ and $(2)$, the area of $\\{(A,B)\\mid f(A,B)<0\\}$ turns out to be\n$$\n\\int_{0}^\\pi\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right){\\rm{d}}A\\\\+\n\\int_{0}^\\pi\\min\\left\\{\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right),\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\\right\\}{\\rm{d}}A.\\quad \n(\\star\\star\\star)$$\nSo the question is if the quantity above coincides with $\\frac{\\pi^2}{2}$ for all $\\alpha\\in [0,1)$. First, we claim that the minimum above is\n$\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)$. Notice that:\n$$\n\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)\n\\leq\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right)\\\\\n\\Leftrightarrow\n\\cos^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right)+\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right)\\geq\\frac{\\pi}{2}; \n$$\nand the cosine of the last angle appearing above is\n$$\n\\left[\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right]\\left[\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right]\n-\\sqrt{1-\\alpha^2\\sin^2\\left(\\frac{A}{2}\\right)}\n\\sqrt{1-\\alpha^2\\cos^2\\left(\\frac{A}{2}\\right)};$$\nwhich is negative as\n$\\alpha\\sin\\left(\\frac{A}{2}\\right)<\\sqrt{1-\\alpha^2\\cos^2\\left(\\frac{A}{2}\\right)}$ and\n$\\alpha\\cos\\left(\\frac{A}{2}\\right)<\\sqrt{1-\\alpha^2\\sin^2\\left(\\frac{A}{2}\\right)}$ due to $|\\alpha|<1$. We conclude that $(\\star\\star\\star)$ is equal to\n$$\n\\int_{0}^\\pi\\cos^{-1}\\left(\\alpha\\cos\\left(\\frac{A}{2}\\right)\\right){\\rm{d}}A+\n\\int_{0}^\\pi\\sin^{-1}\\left(\\alpha\\sin\\left(\\frac{A}{2}\\right)\\right){\\rm{d}}A.\n$$\nCall the expression above $h(\\alpha)$. The goal is to establish $h(\\alpha)=\\frac{\\pi^2}{2}$ for any $\\alpha\\in[0,1]$. This is clear when $\\alpha=0$, and so it suffices to show $\\frac{{\\rm{d}}h}{{\\rm{d}}\\alpha}\\equiv 0$. One has\n$$\n\\frac{{\\rm{d}}h}{{\\rm{d}}\\alpha}=\n-\\int_0^{\\pi}\\frac{\\cos\\left(\\frac{A}{2}\\right)}{\\sqrt{1-\\alpha^2\\cos^2\\left(\\frac{A}{2}\\right)}}{\\rm{d}}A\n+\\int_0^{\\pi}\\frac{\\sin\\left(\\frac{A}{2}\\right)}{\\sqrt{1-\\alpha^2\\sin^2\\left(\\frac{A}{2}\\right)}}{\\rm{d}}A;\n$$\nwhich is clearly zero because the change of variable $A\\mapsto\\pi-A$ indicates\n$$\\int_0^{\\pi}\\frac{\\cos\\left(\\frac{A}{2}\\right)}{\\sqrt{1-\\alpha^2\\cos^2\\left(\\frac{A}{2}\\right)}}{\\rm{d}}A\n=\\int_0^{\\pi}\\frac{\\sin\\left(\\frac{A}{2}\\right)}{\\sqrt{1-\\alpha^2\\sin^2\\left(\\frac{A}{2}\\right)}}{\\rm{d}}A.$$\nThis concludes the proof.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/401878", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "11", "answer_count": 3, "answer_id": 0}}
