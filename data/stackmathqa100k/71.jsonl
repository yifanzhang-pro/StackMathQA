{"Q": "Cauchy distribution (likelihood and Fisher information) I have a three part question:\n1) If I have a $Cauchy(\\theta, 1)$ with density:\n$p(x-\\theta) = \\frac{1}{\\pi\\{1+{(x-\\theta)^2}\\}}$\nand $x_1, ..., x_n$ forms i.i.d sample. I see on Wiki that this will make a likelihood function of:\n$l(\\theta) = -nlog\\pi - \\sum_{i=1}^{n} log(1+{(x_i-\\theta)^2})$\nBut how is that derived?\n2) What derivative rules would I use to show that:\n$l'(\\theta) = -2 \\sum_{i=1}^{n} \\frac{x_i-\\theta}{1+(x_i-\\theta)^2}$\n(I feel this is probably straight-forward, but I am missing it)\n3) How would I obtain that the Fisher information is equal to $I(\\theta)=\\frac{n}{2}$?\nI started by:\n$\\frac{\\partial^2f(x;\\theta)}{2\\theta^2} = \\frac{8(x-\\theta)^2}{\\pi[1+(x-\\theta)^2]^3} - \\frac{2}{\\pi[1+(x-\\theta)^2]^2}$\nI think that I next need to find the integral of:\n$I(\\theta) = -E[\\frac{\\partial^2f(x;\\theta)}{2\\theta^2}]$\nBut I cannot get this last step of reducing this information to $\\frac{n}{2}$.\n", "A": "The Fisher information for one observation is given by\\begin{align*}I(\\theta) &= -\\mathbb{E}_\\theta\\left[\\frac{\\partial^2 \\log f(X;\\theta)}{\\partial\\theta^2}\\right]\\\\ &=\\mathbb{E}_\\theta\\left[ \\frac{\\partial^2 \\log \\{1+(X-\\theta)^2\\}}{\\partial\\theta^2}\\right]\\\\\n&=2\\mathbb{E}_\\theta\\left[ -\\frac{\\partial }{\\partial\\theta}\\frac{(X-\\theta)}{1+(X-\\theta)^2}\\right]\\\\\n&=2\\mathbb{E}_\\theta\\left[\\frac{1}{1+(X-\\theta)^2}-\\frac{2(X-\\theta)^2}{[1+(X-\\theta)^2]^2}\\right]\\\\\n&= \\frac{2}{\\pi}\\int_\\mathbb{R} \\frac{1}{[1+(x-\\theta)^2]^2}-\\frac{2(x-\\theta)^2}{[1+(x-\\theta)^2]^3} \\text{d}x\\\\\n&= \\frac{2}{\\pi}\\int_\\mathbb{R} \\frac{1}{[1+x^2]^2}-\\frac{2x^2}{[1+x^2]^3} \\text{d}x\\\\\n&= \\frac{2}{\\pi}\\int_\\mathbb{R} \\frac{1}{[1+x^2]^2}-\\frac{2}{[1+x^2]^2}+\\frac{2}{[1+x^2]^3} \\text{d}x\\\\\n&= \\frac{2}{\\pi}\\int_\\mathbb{R} \\frac{-1}{[1+x^2]^2}+\\frac{2}{[1+x^2]^3} \\text{d}x\n\\end{align*}\nbecause the integral (and the information) is translation invariant.\nNow it is easy to establish a recurrence relation on$$I_k=\\int_\\mathbb{R} \\frac{1}{[1+x^2]^k}\\text{d}x$$Indeed\n\\begin{align*}\nI_k &= \\int_\\mathbb{R} \\frac{1+x^2}{[1+x^2]^{k+1}}\\text{d}x\\\\\n&= I_{k+1} + \\int_\\mathbb{R} \\frac{2kx}{[1+x^2]^{k+1}}\\frac{x}{2k}\\text{d}x\\\\\n&= I_{k+1} + \\frac{1}{2k} \\int_\\mathbb{R} \\frac{1}{[1+x^2]^{k}}\\text{d}x\n= I_{k+1} + \\frac{1}{2k} I_k\n\\end{align*}\nby an integration by parts. Hence\n$$I_1=\\pi\\quad\\text{and}\\quad I_{k+1}=\\frac{2k-1}{2k}I_k\\quad k>1$$\nwhich implies\n$$I_1=\\pi\\quad I_2=\\frac{\\pi}{2}\\quad I_3=\\frac{3\\pi}{8}$$\nand which leads to the Fisher information:\n$$I(\\theta)=\\frac{2}{\\pi}\\left\\{-I_2+2I_3\\right\\}=\\frac{2}{\\pi}\\left\\{\\frac{-\\pi}{2}+\\frac{3\\pi}{4}\\right\\}=\\frac{1}{2}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/145017", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 1, "answer_id": 0}}
{"Q": "Correlation when adding balls to urn It is a pretty simple question, but I wanted to know if I got it right (and I had a weird result in the end):\nThere are 3 White balls and 1 Green ball in a urn. We take one ball from it, write down the colour then put it back with +c balls of the same colour. Now we take a second ball. $X_{i} = 1$ if the ith ball is green and $X_{i} = 0$ if it is white, for $i = 1,2$.\na) What is the joint probability distribution of $X_{1}$ and $X_{2}$\nb) What is the correlation between $X_{1}$ and $X_{2}$. What whappens when c goes to inf?\nHere's what I did:\na)\n$P(0,0) =  \\frac{3}{4} \\cdot \\frac{3+c}{4+c} = \\frac{9+3c}{16+4c}$\n$P(0,1) =  \\frac{3}{4} \\cdot \\frac{1}{4+c} = \\frac{3}{16+4c}$\n$P(1,0) =  \\frac{1}{4} \\cdot \\frac{3}{4+c} = \\frac{3}{16+4c}$\n$P(1,1) =  \\frac{1}{4} \\cdot \\frac{1+c}{4+c} = \\frac{1+c}{16+4c}$\nb)\n$E[X_{1}] = E[(X_{1})^2] =  1 \\cdot \\frac{1}{4} = \\frac{1}{4}$ \n$E[X_{2}] = E[(X_{1})^2] =  1 \\cdot \\frac{1+c}{4+c} + 1 \\cdot \\frac{1}{4+c} = \\frac{2+c}{4+c}$ (all other sum terms are 0)\n$E[X_{1}X_{2}] = 1 \\cdot 1 \\cdot \\frac{1+c}{4c+16}$ (all other sum terms are 0)\n$Var(X_{1}) = \\frac{1}{4} - \\frac{1}{16} = \\frac{3}{16}$\n$Var(X_{2}) = \\frac{2+c}{4+c} - (\\frac{2+c}{4+c})^2 = \\frac{4+2c}{(4+c)^2}$\n$Cov(X_{1},X_{2}) = \\frac{1+c}{4c+16} - \\frac{1}{4} \\cdot \\frac{2+c}{4+c} = -1$\n$Cor(X_{1},X_{2}) = \\frac{-1}{\\sqrt{\\frac{3}{16} \\cdot \\frac{4+2c}{(4+c)^2}}} = \\frac{-c-16}{\\sqrt{12+6c}}$\nAnd then when c goes to inf, the correlation goes to -inf, which doesn't seem to make sense...\nDid I go wrong somewhere?\n", "A": "I'm having trouble following your logic, but yes, you've made some mistakes (a correlation cannot exceed one in absolute value, for example).  $\\text{E}(X_1)$ is easy enough to find so let's start by calculating $\\text{E}(X_2)$.  The key is to condition on $X_1$ and then calculate the expectation in pieces.\n\\begin{align}\n\\text{E}(X_2) &= \\text{E} [ \\text{E} (X_2 \\mid X_1) ] \\\\\n&= P(X_1 = 1) \\text{E} (X_2 \\mid X_1 = 1) + P(X_1 = 0) \\text{E}(X_2 \\mid X_1 = 0) \\\\\n&= \\frac{1}{4} \\cdot \\frac{1 + c}{4 + c} + \\frac{3}{4} \\cdot \\frac{1}{4 + c} \\\\\n&= \\frac{4 + c}{4 (4 + c)} \\\\\n&= \\frac{1}{4} .\n\\end{align}\nThis is interesting as it says that on average $X_2$ behaves just like $X_1$.  Now since these are Bernoulli random variables with the same expectation the variances are easy:\n\\begin{align}\n\\text{Var}(X_i) &=  \\text{E}(X_i^2) - \\text{E}(X_i)^2 \\\\\n&= \\frac{1}{4} - \\frac{1}{16} \\\\\n&= \\frac{3}{16} .\n\\end{align}\nThe only thing left to calculate is the covariance and we can use the identity $\\text{Cov}(X_1, X_2) = \\text{E}(X_1 X_2) - \\text{E}(X_1) \\text{E}(X_2)$.  We already know the rightmost term so for the other we have\n\\begin{align}\n\\text{E}(X_2 X_2) &= P(X_1 = 1 \\cap X_2 = 1) \\\\\n&= P(X_1 = 1) P(X_2 = 1 \\mid X_1 = 1) \\\\\n&= \\frac{1 + c}{4 (4 + c)}\n\\end{align}\nyielding\n\\begin{align}\n\\text{Cov}(X_1, X_2) &= \\frac{1 + c}{4 (4 + c)} - \\frac{1}{16} \\\\\n&= \\frac{3c}{16 (4 + c)} .\n\\end{align}\nIf we now divide by this $\\sqrt{\\text{Var}(X_1) \\text{Var}(X_2)}$ we get\n\\begin{align}\n\\text{Corr}(X_1, X_2) &= \\frac{c}{4 + c} .\n\\end{align}\nThis makes sense since as $c \\to \\infty$ we have $X_1 = X_2$ with high probability so the correlation should approach one.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/237277", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Prove using correlation to do t-test is equivalent to the standard t-test formula in linear regression? $\\newcommand{\\Cor}{\\operatorname{Cor}} \\newcommand{\\Cov}{\\operatorname{Cov}}$My question is why the following expression holds?\n$$t = \\frac{\\hat{\\beta_1}}{\\operatorname{se}(\\beta_1)} = \\frac{\\Cor(Y,X)\\sqrt{n-2}}{\\sqrt{1 - \\Cor^2(Y,X)}}$$\nHere is what I got so far:\n\\begin{align}\n\\frac{\\hat{\\beta_1}}{\\operatorname{se}(\\beta_1)} &= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\operatorname{se}(\\beta_1)}\\\\\n&= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\sqrt{\\frac{\\sum(\\hat{Y_i} - Y_i)^2}{n-2}\\cdot\\frac{1}{\\sum(X_i - \\overline{X})^2}}}\\\\\n&= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}\\sqrt{n-2}}{\\sqrt{\\frac{\\sum(\\hat{Y_i} - Y_i)^2}{\\sum(X_i - \\overline{X})^2}}}\\label{a}\\tag{1}\n\\end{align} \n\\begin{align}\n\\frac{\\Cor(Y,X)\\sqrt{n-2}}{\\sqrt{1 - \\Cor^2(Y,X)}}\n&= \\frac{\\frac{\\Cov(Y, X)}{S_xS_y}\\sqrt{n-2}}{\\sqrt{1 - \\left(\\frac{\\Cov(Y, X)}{S_xS_y}\\right)^2}} \\label{b}\\tag{2}\n\\end{align}\nFrom $\\ref{a}$ and $\\ref{b}$, we then need to show:\n\\begin{align}\n\\frac{1}{S_x\\sqrt{\\frac{\\sum(\\hat{Y_i} - Y_i)^2}{\\sum(X_i - \\overline{X})^2}}} &= \\frac{1}{S_y\\sqrt{1 - \\left(\\frac{\\Cov(Y, X)}{S_xS_y}\\right)^2}}\\\\\n&=\\frac{1}{S_y\\frac{\\sqrt{S_x^2S_y^2 -\\Cov^2(Y, X)}}{S_xS_y}}\\\\\n&=\\frac{1}{\\frac{\\sqrt{S_x^2S_y^2 -\\Cov^2(Y, X)}}{S_x}}\\label{c}\\tag{3}\n\\end{align}\nI tried to expand the $\\Cov(Y, X)$ term and left $SSE$ and $SSX$ terms in $\\ref{c}$, but there is no any further process.\nI am wondering how to continue from $\\ref{c}$, or my initial direction of the proof is not correct?\n", "A": "The trick is the following equation:$$\\sum(\\hat{Y_i} - Y_i)^2 = SSE = (1 - R^2)SST$$\nThis is why there is a $\\sqrt{1 - \\Cor^2(Y,X)}$ term in $(2)$. The whole proof is below:\n$$\n\\begin{align}\nt &= \\frac{\\hat{\\beta_1}}{\\operatorname{se}(\\beta_1)}\\\\\n  &= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\sqrt{\\frac{\\sum(\\hat{Y_i} - Y_i)^2}{n-2}\\cdot\\frac{1}{\\sum(X_i - \\overline{X})^2}}}\\\\\n  &= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\sqrt{\\frac{(1 - R^2)SST}{n-2}\\cdot\\frac{1}{\\sum(X_i - \\overline{X})^2}}}\\\\\n  &= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\sqrt{\\frac{(1 - R^2)S_y^2(n-1)}{n-2}\\cdot\\frac{1}{S_x^2(n-1)}}}\\\\\n  &= \\frac{\\frac{\\Cov(Y,X)}{S_x^2}}{\\frac{S_y}{S_x}\\frac{\\sqrt{1 - R^2}}{\\sqrt{n - 2}}}\\\\\n  &= \\frac{\\frac{\\Cov(Y,X)}{S_yS_x}\\sqrt{n-2}}{\\sqrt{1 - R^2}}\\\\\n  &= \\frac{\\Cor(Y,X)\\sqrt{n-2}}{\\sqrt{1 - \\Cor^2(Y,X)}}\n\\end{align}\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/267023", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "Showing that the OLS estimator is scale equivariant? I don't have a formal definition of scale equivariance, but here's what Introduction to Statistical Learning says about this on p. 217:\n\nThe standard least squares coefficients... are scale equivariant: multiplying $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$.\n\nFor simplicity, let's assume the general linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon$, where $\\mathbf{y} \\in \\mathbb{R}^N$, $\\mathbf{X}$ is a $N \\times (p+1)$ matrix (where $p+1 < N$) with all entries in $\\mathbb{R}$, $\\boldsymbol\\beta \\in \\mathbb{R}^{p+1}$, and $\\boldsymbol\\epsilon$ is a $N$-dimensional vector of real-valued random variables with $\\mathbb{E}[\\boldsymbol\\epsilon] = \\mathbf{0}_{N \\times 1}$.\nFrom OLS estimation, we know that if $\\mathbf{X}$ has full (column) rank, \n$$\\hat{\\boldsymbol\\beta}_{\\mathbf{X}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\\text{.}$$\nSuppose we multiplied a column of $\\mathbf{X}$, say $\\mathbf{x}_k$ for some $k \\in \\{1, 2, \\dots, p+1\\}$, by a constant $c \\neq 0$. This would be equivalent to the matrix\n\\begin{equation}\n\\mathbf{X}\\underbrace{\\begin{bmatrix}\n1 & \\\\\n  & 1 \\\\\n  &  & \\ddots \\\\\n  &   &      & 1 \\\\\n  &   &      & & c\\\\\n  &   &      &  & & 1 \\\\\n  &   &      &  & & &\\ddots \\\\\n  &   &      &  & & & & 1\n  \\end{bmatrix}}_{\\mathbf{S}} = \n  \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & c\\mathbf{x}_{k} & \\cdots & \\mathbf{x}_{p+1}\\end{bmatrix} \\equiv \\tilde{\\mathbf{X}}\n\\end{equation}\nwhere all other entries of the matrix $\\mathbf{S}$ above are $0$, and $c$ is in the $k$th entry of the diagonal of $\\mathbf{S}$. Then, $\\tilde{\\mathbf X}$ has full (column) rank as well, and the resulting OLS estimator using $\\tilde{\\mathbf X}$ as the new design matrix is \n$$\\hat{\\boldsymbol\\beta}_{\\tilde{\\mathbf{X}}} = \\left(\\tilde{\\mathbf{X}}^{T}\\tilde{\\mathbf{X}}\\right)^{-1}\\tilde{\\mathbf{X}}^{T}\\mathbf{y}\\text{.}$$\nAfter some work, one can show that\n$$\\tilde{\\mathbf{X}}^{T}\\tilde{\\mathbf{X}} = \\begin{bmatrix}\n\\mathbf{x}_1^{T}\\mathbf{x}_1 & \\mathbf{x}_1^{T}\\mathbf{x}_2 & \\cdots & c\\mathbf{x}_1^{T}\\mathbf{x}_k & \\cdots & \\mathbf{x}_1^{T}\\mathbf{x}_{p+1} \\\\\n\\mathbf{x}_2^{T}\\mathbf{x}_1 & \\mathbf{x}_2^{T}\\mathbf{x}_2 & \\cdots & c\\mathbf{x}_2^{T}\\mathbf{x}_k & \\cdots & \\mathbf{x}_2^{T}\\mathbf{x}_{p+1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\cdots & \\vdots \\\\\nc\\mathbf{x}_k^{T}\\mathbf{x}_1 & c\\mathbf{x}_k^{T}\\mathbf{x}_2 & \\cdots & c^2\\mathbf{x}_k^{T}\\mathbf{x}_k & \\cdots & c\\mathbf{x}_k^{T}\\mathbf{x}_{p+1} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n\\mathbf{x}_{p+1}^{T}\\mathbf{x}_1 & \\mathbf{x}_{p+1}^{T}\\mathbf{x}_2 & \\cdots & c\\mathbf{x}_{p+1}^{T}\\mathbf{x}_{p+1} & \\cdots & \\mathbf{x}_{p+1}^{T}\\mathbf{x}_{p+1} \\\\\n\\end{bmatrix}$$\nand\n$$\\tilde{\\mathbf{X}}^{T}\\mathbf{y} = \\begin{bmatrix}\n\\mathbf{x}_1^{T}\\mathbf{y} \\\\\n\\mathbf{x}_2^{T}\\mathbf{y} \\\\\n\\vdots \\\\\nc\\mathbf{x}_k^{T}\\mathbf{y} \\\\\n\\vdots \\\\\n\\mathbf{x}_{p+1}^{T}\\mathbf{y}\n\\end{bmatrix}$$\nHow do I go from here to show the claim quoted above (i.e., that $\\hat{\\boldsymbol\\beta}_{\\tilde{\\mathbf{X}}} = \\dfrac{1}{c}\\hat{\\boldsymbol\\beta}_{\\mathbf{X}}$)? It's not clear to me how to compute $(\\tilde{\\mathbf{X}}^{T}\\tilde{\\mathbf{X}})^{-1}$.\n", "A": "I figured this out after posting the question. If my work is correct, however, I misinterpreted the claim. The $\\dfrac{1}{c}$ scaling only occurs on the one component of $\\boldsymbol\\beta$ corresponding to the column of $\\mathbf{X}$ being multiplied by $c$.\nNotice that $\\mathbf{S}$, in the notation above, is a diagonal, symmetric $(p+1) \\times (p+1)$ matrix and has inverse (because it is diagonal)\n$$\\mathbf{S}^{-1} = \\begin{bmatrix}\n1 & \\\\\n  & 1 \\\\\n  &  & \\ddots \\\\\n  &   &      & 1 \\\\\n  &   &      & & \\frac{1}{c}\\\\\n  &   &      &  & & 1 \\\\\n  &   &      &  & & &\\ddots \\\\\n  &   &      &  & & & & 1\n  \\end{bmatrix}\\text{.}$$\nNote that $(\\tilde{\\mathbf{X}}^{T}\\tilde{\\mathbf{X}})^{-1}$ is a $(p+1)\\times(p+1)$ matrix. Let's suppose that \n$$(\\mathbf{X}^{T}\\mathbf{X})^{-1} = \\begin{bmatrix}\n\\mathbf{z}_1 & \\mathbf{z}_2 & \\cdots & \\mathbf{z}_k & \\cdots & \\mathbf{z}_{p+1}\n\\end{bmatrix}\\text{.}$$\nThen it follows that \n$$(\\tilde{\\mathbf{X}}^{T}\\tilde{\\mathbf{X}})^{-1} = [(\\mathbf{X}\\mathbf{S})^{T}\\mathbf{X}\\mathbf{S}]^{-1} = (\\mathbf{S}^{T}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{S})^{-1} = (\\mathbf{S}\\mathbf{X}^{T}\\mathbf{X}\\mathbf{S})^{-1}=\\mathbf{S}^{-1}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{S}^{-1}\\text{.}$$\nHence,\n$$\\mathbf{S}^{-1}(\\mathbf{X}^{T}\\mathbf{X})^{-1} = \\begin{bmatrix}\n\\mathbf{z}_1 \\\\\n& \\mathbf{z}_2 \\\\\n& & \\ddots \\\\\n& &  & \\frac{1}{c}\\mathbf{z}_k \\\\\n& &  & & \\ddots \\\\\n& & & & & \\mathbf{z}_{p+1}\n\\end{bmatrix}$$\nand multiplying this by $\\mathbf{S}^{-1}$ has a similar effect to what multiplying $\\mathbf{X}$ by $\\mathbf{S}$ did - it remains the same, except $\\frac{1}{c}\\mathbf{z}_k$ is multiplied by $\\frac{1}{c}$:\n$$\\mathbf{S}^{-1}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{S}^{-1} = \\begin{bmatrix}\n\\mathbf{z}_1 \\\\\n& \\mathbf{z}_2 \\\\\n& & \\ddots \\\\\n& &  & \\frac{1}{c^2}\\mathbf{z}_k \\\\\n& &  & & \\ddots \\\\\n& & & & & \\mathbf{z}_{p+1}\n\\end{bmatrix}\\text{.}$$\nTherefore,\n$$\\begin{align}\n\\hat{\\boldsymbol\\beta}_{\\tilde{\\mathbf{X}}}&=\\mathbf{S}^{-1}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{S}^{-1}(\\mathbf{X}\\mathbf{S})^{T}\\mathbf{y} \\\\\n&= \\begin{bmatrix}\n\\mathbf{z}_1 \\\\\n& \\mathbf{z}_2 \\\\\n& & \\ddots \\\\\n& &  & \\frac{1}{c^2}\\mathbf{z}_k \\\\\n& &  & & \\ddots \\\\\n& & & & & \\mathbf{z}_{p+1}\n\\end{bmatrix}\\begin{bmatrix}\n\\mathbf{x}_1^{T}\\mathbf{y} \\\\\n\\mathbf{x}_2^{T}\\mathbf{y} \\\\\n\\vdots \\\\\nc\\mathbf{x}_k^{T}\\mathbf{y} \\\\\n\\vdots \\\\\n\\mathbf{x}_{p+1}^{T}\\mathbf{y}\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\mathbf{z}_1\\mathbf{x}_1^{T}\\mathbf{y} \\\\\n\\mathbf{z}_2\\mathbf{x}_2^{T}\\mathbf{y} \\\\\n\\vdots \\\\\n\\frac{1}{c}\\mathbf{z}_k\\mathbf{x}_k^{T}\\mathbf{y} \\\\\n\\vdots \\\\\n\\mathbf{z}_{p+1}\\mathbf{x}_{p+1}^{T}\\mathbf{y}\n\\end{bmatrix}\n\\end{align}$$\nas desired.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/311198", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "13", "answer_count": 5, "answer_id": 1}}
{"Q": "Using Chebyshev's inequality to obtain lower bounds Let $X_1$ and $X_2$ be i.i.d. continuous random variables with pdf $f(x) = 6x(1-x), 0<x<1$ and $0$, otherwise.\nUsing Chebyshev's inequality, find the lower bound of $P\\left(|X_1 + X_2-1| \\le\\frac{1}{2}\\right)$\nWhat I did:\nUsing as Chebyshev's inequality,\n$P(|X-\\mu|\\ge a)\\le \\frac{\\sigma^2}{a^2}$\nWhere $a=\\frac{1}{2}$\nFinding the variance: $E[X^2] - (E[X])^2$\n$  E[X] = \\int_{0}^{1} x6x(1-x) dx$ $=6\\left[\\frac{x^  3}{3}-\\frac{x^4}{4}\\right]_0^1=6\\left[\\frac{1}{3}-\\frac{1}{4}\\right]=  \\frac{6}{12}=\\frac{1}{2}$\n$  E[X^2]= \\int_{0}^{1}x^{2}6x(1-x)dx=6\\left[\\frac{x^  4}{4}-\\frac{x^5}{5}\\right]_0^1=6\\left[\\frac{1}{4}-\\frac{1}{5}\\right]=\\frac{6}{20}$\nTherefore, $\\sigma^2=\\frac{6}{20}-\\left(\\frac{1}{2}\\right)^2=\\frac{1}{20}$\nPutting in Chebyshev's inequality, \n$\\frac{\\sigma^2}{a^2} $= $\\left[\\frac{\\frac{1}{20}}{\\left(\\frac{1}{2}\\right)^2}\\right]$=$\\frac{4}{20}=\\frac{1}{5} $\nBut what we need is $\\le \\frac{1}{2}$ which we get by $1-\\frac{1}{5}=\\frac{4}{5}$, \nBut the answer is $\\frac{3}{5}$\nWhere am I going wrong?\n", "A": "Comparing the given equation with Chebyshev's inequality, we get $\\mu=1$\nSince $X$ here has $X_1$ and $X_2$\u00a0\u00a0i.i.d. continuous random variables,\u00a0so both have same pdf and $E[X]=  E[X_1] + E[X_2]$\nThe mistake I did was to not calculate both $X_1$ and $X_2$ separately.\nSo the calculated $E[X]$ is actually$ E[X_1]=\\frac{1}{2}$ and similarly $E[X_2]=\\frac{1}{2} $\n$E[X]=  E[X_1] + E[X_2]=\\frac{1}{2}+ \\frac{1}{2}=1$\nAnd in the very same manner(recognising the same mistake everywhere) \n$E[X^2]=E[X_1^2] + E[X_2^2] = \\frac{6}{20} + \\frac{6}{20} = \\frac{6}{10}$\nTherefore variance $E[X^2] - (E[X])^2 =\\frac{1}{10}$\nPutting in Chebyshev's inequality, \n$\\frac{\\sigma^2}{a^2} $= $\\left[\\frac{\\frac{1}{10}}{\\left(\\frac{1}{2}\\right)^2}\\right]$=$\\frac{4}{10}=\\frac{2}{5} $\nSo the lower bound which we get by $1-\\frac{2}{5}=\\frac{3}{5}$ which is the required answer.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/324862", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 1}}
{"Q": "Proving the maximum possible sample variance for bounded data There is a fascinating question here looking at the distribution of the sample mean and sample variance for the uniform distribution and other bounded distributions.  This led me to wonder about the maximum possible value of the sample variance in these cases.  Suppose we have data values $x_1,...,x_n$ that are known to fall within the bounds $a \\leqslant x_i \\leqslant b$.  What is the maximum possible value of the sample variance?\nIntuitively, it seems to me that the answer should be that you have half of the data points at each boundary.  In the case where there is an even number of data points this gives the sample variance:\n$$\\begin{align}\ns^2 \n&= \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x}_n)^2 \\\\[6pt]\n&= \\frac{1}{n-1} \\Bigg[ \\frac{n}{2} \\Big( a - \\frac{a+b}{2} \\Big)^2 + \\frac{n}{2} \\Big( b - \\frac{a+b}{2} \\Big)^2  \\Bigg] \\\\[6pt]\n&= \\frac{n}{n-1} \\cdot \\frac{1}{2} \\Bigg[ \\Big( \\frac{a-b}{2} \\Big)^2 + \\Big( \\frac{b-a}{2} \\Big)^2  \\Bigg] \\\\[6pt]\n&= \\frac{n}{n-1} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2. \\\\[6pt]\n\\end{align}$$\nAs $n \\rightarrow \\infty$ this converges to the square of the half-length between the boundaries.  Is there a proof that this result is correct?\n", "A": "Proving the bound: The sample variance is not affected by location shifts in the data, so we will use the values $0 \\leqslant y_i \\leqslant b-a$ and compute the sample variance from these values.  As a preliminary observation we first note that the bound $y_i \\leqslant b-a$ implies that we have $\\sum y_i^2 \n\\leqslant \\sum (b-a) y_i = (b-a) n \\bar{y}_n$, which gives the upper bound:\n$$\\begin{align}\ns_n^2 \n&= \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar{y}_n)^2 \\\\[6pt]\n&= \\frac{1}{n-1} \\Bigg[ \\sum_{i=1}^n y_i^2 - n \\bar{y}_n^2 \\Bigg] \\\\[6pt]\n&\\leqslant \\frac{1}{n-1} \\Bigg[ (b-a) n \\bar{y}_n - n \\bar{y}_n^2 \\Bigg] \\\\[6pt]\n&= \\frac{n}{n-1} \\cdot \\bar{y}_n (b-a  - \\bar{y}_n). \\\\[6pt]\n\\end{align}$$\nNow, using the fact that $0 \\leqslant \\bar{y}_n \\leqslant b-a$ this upper bound is maximised when $\\bar{y}_n = \\tfrac{b-a}{2}$, which gives:\n$$\\begin{align}\ns_n^2 \n&\\leqslant \\frac{n}{n-1} \\cdot \\bar{y}_n (b-a  - \\bar{y}_n) \\quad \\ \\ \\ \\\\[6pt]\n&\\leqslant \\frac{n}{n-1} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2. \\\\[6pt]\n\\end{align}$$\nThis gives an upper bound for the sample variance.  Splitting the points between the two bounds $a$ and $b$ (for an even number of points) achieves this upper bound, so it is the maximum possible sample variance for bounded data.\n\nMaximum possible sample variance with an odd number of data points: It is also possible to find the upper bound when we have an odd number of data points.  One way to do this is to use the iterative updating formula for the sample variance (see e.g., O'Neill 2014).  If we consider the maximising case for an even number $n-1$ of values and then take the final value to be $x_n = x$, then we have:\n$$\\begin{align}\n\\quad \\quad \\quad \\quad \\quad \\quad s_n^2 \n&= \\frac{1}{n-1} \\Bigg[ (n-2) s_{n-1} + \\frac{n-1}{n} \\cdot (x-\\bar{x}_{n-1})^2 \\Bigg] \\\\[6pt]\n&\\leqslant \\frac{1}{n-1} \\Bigg[ (n-1) \\cdot \\Big( \\frac{b-a}{2} \\Big)^2 + \\frac{n-1}{n} \\cdot \\Big( x-\\frac{b-a}{2} \\Big)^2 \\Bigg] \\\\[6pt]\n&= \\Big( \\frac{b-a}{2} \\Big)^2 + \\frac{1}{n} \\cdot \\Big( x-\\frac{b-a}{2} \\Big)^2. \\\\[6pt]\n\\end{align}$$\nThis quantity is maximised by taking either $x=a$ or $x=b$ (i.e., the last point is also on the boundary), which gives:\n$$\\begin{align}\ns_n^2 \n&= \\Big( \\frac{b-a}{2} \\Big)^2 + \\frac{1}{n} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2 \\quad \\quad \\quad \\quad \\quad \\ \\\\[6pt]\n&= \\Big( 1 + \\frac{1}{n} \\Big) \\cdot \\Big( \\frac{b-a}{2} \\Big)^2 \\\\[6pt]\n&= \\frac{n+1}{n} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2 \\\\[6pt]\n&= \\frac{(n-1)(n+1)}{n^2} \\cdot \\frac{n}{n-1} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2 \\\\[6pt]\n&= \\frac{n^2-1}{n^2} \\cdot \\frac{n}{n-1} \\cdot \\Big( \\frac{b-a}{2} \\Big)^2. \\\\[6pt]\n\\end{align}$$\nThis is a slightly lower sample variance value than when we have an even number of data points, but the two cases converge when $n \\rightarrow \\infty$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/511110", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Geometric conditional Probability In order to start a game, each player takes turns throwing a fair six-sided dice until a $6$ is obtained. Let $X$ be the number of turns a player takes to start the game. Given that $X=3$, find the probability that the total score on all three of the dice is less than $10$.\nSo initially I thought this was straight forward. Total number of outcomes that three dice can sum up to less than $10$ is equal to $60$. Total amount of outcomes is $6^3=216$.\nTherefore probability of the total summing to less than $10$ is $$P(\\text{sum} < 10)=\\frac{60}{6^3}=\\frac{5}{18}$$.\nHowever, this did not match the provided solution of $1/12$ and so I thought it had something to do with conditional probability. i.e. $P(\\text{sum} < 10 | X = 3)$.\nSo naturally, $$P(\\text{sum} < 10 | X = 3)=\\frac{P(X = 3 | \\text{sum} < 10) P(\\text{sum} < 10)}{P(X=3)}$$\nOut of the $60$ outcomes that sum less than $10$, only $6$ of those outcomes contain sixes. So that when we have to calculate the probability of taking three turns to get one $6$ we have to fail twice and succeed once and hence$$P(X = 3 | \\text{sum} < 10) = \\left(\\frac{9}{10}\\right)^2\\left(\\frac{1}{10}\\right)=\\frac{81}{1000}$$\n$P(X=3)$ is simply $\\left(\\frac{5}{6}\\right)^2\\left(\\frac{1}{6}\\right)=\\frac{25}{216}$ and so finally\n$$P(\\text{sum} < 10 | X = 3)=\\frac{P(X = 3 | \\text{sum} < 10) P(\\text{sum} < 10)}{P(X=3)}=\\frac{\\frac{81}{1000} \\frac{5}{18}}{\\frac{25}{216}}=\\frac{243}{1250}$$\nWhere did I go wrong? Alternatively, maybe the textbook is incorrect?\n", "A": "For a more brute force approach, consider the following. You have three numbers, you know the last one is 6. There are $6 \\cdot 6=36$ possible combinations for the first two numbers (assuming they can be any number from 1 to 6, which your problem seems to state that they cannot). Asking that the sum of these three numbers is $<10$ is equivalent to asking that the sum of the first two numbers is $<4$. Counting it all out you find there are 3 such combinations. Therefore $3/36=1/12$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/554239", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 3, "answer_id": 1}}
