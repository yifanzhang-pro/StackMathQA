{"Q": "What is the P(A|C) if we know B depends on A and C depends on B? Given a Bayesian network that looks like the following:\nA->B->C\n\nHow do we compute P(A|C)?  My initial guess would be:\nP(A|C) = P(A|B) * P(B|C) + P(A|not B) * P(not B|C)\n\n", "A": "I would prefer $\\Pr(A|C) = \\Pr(A|C,B)  \\Pr(B|C) + \\Pr(A|C, \\text{not } B) \\Pr(\\text{not } B|C)$ and the following counterexample shows why there is a difference.\nProb A B C\n0.1  T T T\n0.1  F T T\n0.1  T F T\n0.2  F F T \n0.2  T T F\n0.1  F T F\n0.1  T F F\n0.1  F F F\n\nThen in your formulation $\\Pr(A|C)=\\frac{2}{5}$, $\\Pr(A|B)=\\frac{3}{5}$, $\\Pr(B|C)=\\frac{2}{5}$, $\\Pr(A|\\text{not } B)= \\frac{2}{5}$, $\\Pr(\\text{not } B|C)= \\frac{3}{5}=6$  but $\\frac{2}{5} \\not = \\frac{3}{5} \\times \\frac{2}{5} + \\frac{2}{5} \\times \\frac{3}{5}$.\nIn my formulation $\\Pr(A|C,B) = \\frac{1}{2}$ and $\\Pr(A|C, \\text{not } B)=\\frac{1}{3}$ and we have the equality $\\frac{2}{5} = \\frac{1}{2} \\times \\frac{2}{5} + \\frac{1}{3} \\times \\frac{3}{5}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/19024", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 0}}
{"Q": "Distribution of $XY$ if $X \\sim$ Beta$(1,K-1)$ and $Y \\sim$ chi-squared with $2K$ degrees Suppose that $X$ has the beta distribution Beta$(1,K-1)$ and $Y$  follows a chi-squared with $2K$ degrees. In addition, we assume that $X$ and $Y$ are independent.\nWhat is the distribution of the product $Z=XY$ .\nUpdate\nMy attempt:\n\\begin{align}\nf_Z &= \\int_{y=-\\infty}^{y=+\\infty}\\frac{1}{|y|}f_Y(y) f_X \\left (\\frac{z}{y} \\right ) dy \\\\ &= \\int_{0}^{+\\infty}  \\frac{1}{B(1,K-1)2^K \\Gamma(K)}  \\frac{1}{y} y^{K-1} e^{-y/2} (1-z/y)^{K-2} dy \\\\ &= \\frac{1}{B(1,K-1)2^K \\Gamma(K)}\\int_{0}^{+\\infty} e^{-y/2} (y-z)^{K-2} dy \\\\ &=\\frac{1}{B(1,K-1)2^K \\Gamma(K)} [-2^{K-1}e^{-z/2}\\Gamma(K-1,\\frac{y-z}{2})]_0^\\infty \\\\ &= \\frac{2^{K-1}}{B(1,K-1)2^K \\Gamma(K)} e^{-z/2} \\Gamma(K-1,-z/2)\n\\end{align}\nIs it correct? if yes, how we call this distribution? \n", "A": "After some valuable remarks, I was able to find the solution:\nWe have $f_X(x)=\\frac{1}{B(1,K-1)} (1-x)^{K-2}$ and $f_Y(y)=\\frac{1}{2^K \\Gamma(K)} y^{K-1} e^{-y/2}$.\nAlso, we have $0\\le x\\le 1$. Thus, if $x=\\frac{z}{y}$, we get  $0 \\le \\frac{z}{y}  \\le 1$ which implies that $z\\le y \\le \\infty$.\nHence:\n\\begin{align}\nf_Z &= \\int_{y=-\\infty}^{y=+\\infty}\\frac{1}{|y|}f_Y(y) f_X \\left (\\frac{z}{y} \\right ) dy \\\\ &= \\int_{z}^{+\\infty}  \\frac{1}{B(1,K-1)2^K \\Gamma(K)}  \\frac{1}{y} y^{K-1} e^{-y/2} (1-z/y)^{K-2} dy \\\\ &= \\frac{1}{B(1,K-1)2^K \\Gamma(K)}\\int_{z}^{+\\infty} e^{-y/2} (y-z)^{K-2} dy \\\\ &=\\frac{1}{B(1,K-1)2^K \\Gamma(K)} \\left[-2^{K-1}e^{-z/2}\\Gamma(K-1,\\frac{y-z}{2})\\right]_z^\\infty \\\\ &= \\frac{2^{K-1}}{B(1,K-1)2^K \\Gamma(K)} e^{-z/2} \\Gamma(K-1) \\\\ &= \\frac{1}{2} e^{-z/2}\n\\end{align}\nwhere the last equality holds since $B(1,K-1)=\\frac{\\Gamma(1)\\Gamma(K-1)}{\\Gamma(K)}$.\nSo $Z$ follows an exponential distribution of parameter $\\frac{1}{2}$; or equivalently, $Z \\sim\\chi_2^2$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/183574", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "9", "answer_count": 3, "answer_id": 0}}
{"Q": "The expected long run proportion of time the chain spends at $a$ , given that it starts at $c$ Consider the transition matrix:\n$\\begin{bmatrix} \\frac{1}{5} & \\frac{4}{5} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\ \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{bmatrix}$\n\nWhat is the expected long run proportion of time the chain spends at\n  $a$, given that it starts at $b$.\n\nI know that I must use the stationary distributions of each $\\pi(j)$ in question. Since $a$ and $b$ only communicate with each other, I get the system of simulataneous equations:  \n\n\n*\n\n*$\\pi(a) = \\frac{1}{2} \\cdot \\pi(b) + \\frac{1}{5} \\cdot \\pi(a)$\n\n*$\\pi(b) = \\frac{4}{5} \\cdot \\pi(a) + \\frac{1}{2} \\cdot \\pi(b)$ \nwith these I am getting a distribution $\\pi = (\\frac{5}{13}, \\frac{8}{13})$, Is this correct?\nif the distribution started at $c$, as in the title of the post, would my equations now be 3 simulatenous equations which look like:  \n\n\n*\n\n*$\\pi(a) = \\frac{1}{5} \\cdot \\pi(a) + \\frac{1}{2} \\cdot \\pi(b) + \\frac{1}{5} \\cdot \\pi(c)$\n\n*$\\pi(a) = \\frac{4}{5} \\cdot \\pi(a) + \\frac{1}{2} \\cdot \\pi(b) + \\frac{1}{5} \\cdot \\pi(c)$\n\n*$\\pi(c) = \\frac{1}{5} \\cdot \\pi(c)$  \nI am uncertain about the last equation. What I am confused about is $c$ leads to every state, but if I include all of them then I will have a system of 6 equations. Since the question is asking specifically about $a$ which can only be reached by states $a,b,c$, shouldn't we only be considering the equations I wrote?\n", "A": "\nWhat is the expected long run proportion of time the chain spends at $a$, given that it starts at $b$?\n\nThis exercise, technically, asks for the limiting probability value $\\ell_b(a)$. You can note that the limiting distribution $\\ell_b= \\left(\\frac{5}{13}, \\frac{8}{13}, 0, 0, 0\\right)$ that you correctly evaluated is also a stationary distribution of the given matrix -- a limiting distribution will always be stationary. That matrix although is a reducible one and so it can have more than one stationary distribution. In fact a second one is $(0, 0, 0, 0, 1)$.\nNow, the question that you made in the title is about the limiting distribution $\\ell_c$, and, of course, specifically about its first value $\\ell_c(a)$:\n$$\\ell_c(a) = \\lim_{n \\to \\infty}P(X_n=a | X_0= c) = \\frac{5}{7}\\frac{5}{13} = \\frac{25}{91}$$\nIf I didn't get it wrong this is a self-study question, so I will leave to you to find the middle steps of this solution. Consider that $\\ell_c$ has non-zero values only for $a$, $b$ and $e$, it is, in fact, a weighted sum of the two stationary distributions above (and therefore, it is a stationary distribution as well).\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/262912", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 1, "answer_id": 0}}
{"Q": "Full-Rank design matrix from overdetermined linear model I'm trying to create a full-rank design matrix X for a randomized block design model starting from something like the example from page 3/8 of this paper (Wayback Machine) .\nIt's been suggested that I can go about this by eliminating one of each of the columns for treatment (column 5) and block (column 8) as shown in the example on the page marked 45 in this paper.\nI'm not sure how this results in a full-rank design matrix though, for a 12x8 matrix wouldn't making it 12x6 mean that by definition there are still linear dependencies?\nMy understanding of the definition of a full-rank matrix would be that there are no linear dependencies, which would require a matrix to be square among other criteria. Perhaps this is a misconception on my part?\nThe goal is to end up with an invertible matrix from $X^T X$ which I can use to determine the least squares, I'm not sure if that's relevant to the issue.\n", "A": "The matrix $\\mathbf{X}^\\text{T} \\mathbf{X}$ is called the Gramian matrix of the design matrix $\\mathbf{X}$.  It is invertible if and only if the columns of the design matrix are linearly independent ---i.e., if and only if the design matrix has full rank (see e.g., here and here).  (So yes, these two things are closely related, as you suspected.)  Before considering why removing columns gives you full rank, it is useful to see why the present design matrix is not of full rank.  In its current form, you have the linear dependencies:\n$$\\begin{equation} \\begin{aligned}\n\\text{col}_5(\\mathbf{X}) &= \\text{col}_1(\\mathbf{X}) - \\text{col}_2(\\mathbf{X}) - \\text{col}_3(\\mathbf{X}) - \\text{col}_4(\\mathbf{X}), \\\\[6pt]\n\\text{col}_8(\\mathbf{X}) &= \\text{col}_1(\\mathbf{X}) - \\text{col}_6(\\mathbf{X}) - \\text{col}_7(\\mathbf{X}). \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nIf you remove columns $5$ and $8$ you remove these linear dependencies, and it turns out that there are no linear dependencies remaining.  (If you're not sure, just try to find one.)  To confirm this we can look at the reduced design matrix and :\n$$\\mathbf{X}_- = \\begin{bmatrix}\n1 & 1 & 0 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 1 \\\\\n1 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 1 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 1 & 0 \\\\\n1 & 0 & 0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 1 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n\\end{bmatrix} \\quad \\quad \\quad \n\\mathbf{X}_-^\\text{T} \\mathbf{X}_- = \\begin{bmatrix}\n12 & 3 & 3 & 3 & 4 & 4 \\\\\n3 & 3 & 0 & 0 & 1 & 1 \\\\\n3 & 0 & 3 & 0 & 1 & 1 \\\\\n3 & 0 & 0 & 3 & 1 & 1 \\\\\n4 & 1 & 1 & 1 & 4 & 0 \\\\\n4 & 1 & 1 & 1 & 0 & 4 \\\\\n\\end{bmatrix}.$$\nThe Gram determinant of this reduced design matrix is $\\det (\\mathbf{X}_-^\\text{T} \\mathbf{X}_-) = 432 \\neq 0$, so the reduced design matrix has linearly independent columns and is of full rank.  The Gramian matrix for the reduced design matrix is invertible, with inverse:\n$$(\\mathbf{X}_-^\\text{T} \\mathbf{X}_-)^{-1} = \\begin{bmatrix}\n\\tfrac{1}{2} & -\\tfrac{1}{3} & -\\tfrac{1}{3} & -\\tfrac{1}{3} & -\\tfrac{1}{4} & -\\tfrac{1}{4} \\\\\n-\\tfrac{1}{3} & \\tfrac{2}{3} & \\tfrac{1}{3} & \\tfrac{1}{3} & 0 & 0 \\\\\n-\\tfrac{1}{3} & \\tfrac{1}{3} & \\tfrac{2}{3} & \\tfrac{1}{3} & 0 & 0 \\\\\n-\\tfrac{1}{3} & \\tfrac{1}{3} & \\tfrac{1}{3} & \\tfrac{2}{3} & 0 & 0 \\\\\n-\\tfrac{1}{4} & 0 & 0 & 0 & \\tfrac{1}{2} & \\tfrac{1}{4} \\\\\n-\\tfrac{1}{4} & 0 & 0 & 0 & \\tfrac{1}{4} & \\tfrac{1}{2} \\\\\n\\end{bmatrix}.$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/314022", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 1}}
{"Q": "How to show this matrix is positive semidefinite? Let \n$$K=\\begin{pmatrix}\nK_{11} & K_{12}\\\\ \nK_{21} & K_{22}\n\\end{pmatrix}$$ \nbe a symmetric positive semidefinite real matrix (PSD) with $K_{12}=K_{21}^T$. Then, for $|r| \\le 1$, \n$$K^*=\\begin{pmatrix}\nK_{11} & rK_{12}\\\\ \nrK_{21} & K_{22}\n\\end{pmatrix}$$\nis also a PSD matrix. Matrices $K$ and $K^*$ are $2 \\times 2$ and $K_{21}^T$ denotes the transpose matrix. How do I prove this? \n", "A": "There is already a great answer by @whuber, so I will try to give an alternative, shorter proof, using a couple theorems.\n\n\n*\n\n*For any $A$ - PSD and any $Q$ we have $Q^TAQ$ - PSD\n\n*For $A$ - PSD and $B$ - PSD also $A + B$ - PSD\n\n*For $A$ - PSD and $q > 0$ also $qA$ - PSD\n\n\nAnd now:\n\\begin{align*} \nK^* &= \n\\begin{pmatrix}\n  K_{1,1} & rK_{1,2} \\\\\n  rK_{2,1} & K_{2,2} \\\\\n \\end{pmatrix} \\\\\n&= \n\\begin{pmatrix}\n  K_{1,1} & rK_{1,2} \\\\\n  rK_{2,1} & r^2K_{2,2} \\\\\n \\end{pmatrix} \n+\n\\begin{pmatrix}\n  0 & 0 \\\\\n  0 & qK_{2,2} \\\\\n \\end{pmatrix}, \\text{ where $q = 1 - r^2 > 0$} \\\\\n&= \n\\begin{pmatrix}\n  I & 0 \\\\\n  0 & rI \\\\\n \\end{pmatrix}^T\n\\begin{pmatrix}\n  K_{1,1} & K_{1,2} \\\\\n  K_{2,1} & K_{2,2} \\\\\n \\end{pmatrix} \n\\begin{pmatrix}\n  I & 0 \\\\\n  0 & rI \\\\\n \\end{pmatrix} \n+\nq\\begin{pmatrix}\n  0 & 0 \\\\\n  0 & K_{2,2} \\\\\n \\end{pmatrix}\n\\end{align*}\nMatrix $K$ is PSD by definition and so is its submatrix $K_{2, 2}$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/322207", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 2, "answer_id": 0}}
{"Q": "Variance of X+Y+XY? Assuming that random variables X and Y are independent, what is $\\displaystyle Var((1+X)(1+Y)-1)=Var(X+Y+XY)$?\nShould I start as follows\n\\begin{equation}\nVar((1+X)(1+Y)-1)\\\\\n=Var((1+X)(1+Y))\\\\\n=(E[(1+X)])^2 Var(1+Y)+(E[(1+Y])^2 Var(1+X)+Var(1+X)Var(1+Y)\n\\end{equation}\nor maybe as follows\n\\begin{equation}\n\\\\\nVar((1+X)(1+Y)-1)\\\\\n=Var(1+Y+X+XY-1)\\\\\n=Var(X+Y+XY)\\\\\n=Var(X)+Var(Y)+Var(XY)+2Cov(X,Y)+2Cov(X,XY)+2Cov(Y,XY)\n\\end{equation}\nI'm considering could I express the problem in terms of covariances (and variances) between individual random variables. I would like to forecast the variance by individual covariances in my model if its possible. Does the solution simplify if expected values of the variables are zero?\nEdit:\nMoving on from the first alternative\n\\begin{equation}\n=(E[(1+X)])^2 Var(1+Y)+(E[(1+Y])^2 Var(1+X)+Var(1+X)Var(1+Y)\\\\\n=(E[(1+X)])^2 Var(Y)+(E[(1+Y])^2 Var(X)+Var(X)Var(Y)\\\\\n=(1+E[X])^2 Var(Y)+(1+E[Y])^2 Var(X)+Var(X)Var(Y)\\\\\n\\text{ }\\\\\n\\text{if E[X] = 0 and E[Y] = 0, then }\\\\\n=Var(Y) + Var(X) + Var(X)Var(Y)\\\\\n\\text{ }\\\\\n\\end{equation}\n", "A": "For independent random variables $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ respectively, and variances $\\sigma_X^2$ and $\\sigma_Y^2$ respectively,\n\\begin{align}\\require{cancel}\n\\operatorname{var}(X+Y+XY) &= \\operatorname{var}(X)+\\operatorname{var}(Y)+\\operatorname{var}(XY)\\\\\n&\\quad +2\\cancelto{0}{\\operatorname{cov}(X,Y)}+2\\operatorname{cov}(X,XY)+\\operatorname{cov}(Y,XY)\\\\\n&=\\sigma_X^2+\\sigma_Y^2+\\big(\\sigma_X^2\\sigma_Y^2+\\sigma_X^2\\mu_Y^2+\\sigma_Y^2\\mu_X^2\\big)\\\\\n&\\quad +2\\operatorname{cov}(X,XY)+\\operatorname{cov}(Y,XY).\n\\end{align}\nNow,\n\\begin{align}\n\\operatorname{cov}(X,XY) &= E[X\\cdot XY] - E[X]E[XY]\\\\\n&=E[X^2Y]-E[X]\\big(E[X]E[Y]\\big)\\\\\n&= E[X^2]E[Y]-\\big(E[X]\\big)^2E[Y]\\\\\n&= \\sigma_X^2\\mu_Y\n\\end{align}\nand similarly, $\\operatorname{cov}(Y,XY) = \\sigma_Y^2 \\mu_X$.\nConsequently,\n\\begin{align}\\operatorname{var}(X+Y+XY) &=\\sigma_X^2+\\sigma_Y^2+\\sigma_X^2\\sigma_Y^2+\\sigma_X^2\\mu_Y^2+\\sigma_Y^2\\mu_X^2 +2\\sigma_X^2\\mu_Y + 2\\sigma_Y^2 \\mu_X\\\\\n&= \\sigma_X^2\\big(1 + \\mu_Y^2 + 2\\mu_Y\\big) + \\sigma_Y^2\\big(1 + \\mu_X^2 + 2\\mu_X\\big) + \\sigma_X^2\\sigma_Y^2\\\\\n&= \\sigma_X^2\\big(1 + \\mu_Y\\big)^2 + \\sigma_Y^2\\big(1 + \\mu_X\\big)^2 + \\sigma_X^2\\sigma_Y^2. \n\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/323905", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 1}}
{"Q": "A Pairwise Coupon Collector Problem This is a modified version of the coupon collector problem, where we are interested in making comparisons between the \"coupons\". There are a number of constraints which have been placed in order to make this applicable to the application of interest (not relevant here, but related to clustering).\n\n\n*\n\n*There exists $M$ unique coupons.\n\n*Coupons come in packets of size $K$.\n\n*Each packet contains $K$ unique units, sampled uniformly without replacement from the total set of $M$ units.\n\n*The contents of a packet is independent of all other packets.\n\n*All units in a packet are \"compared\" to all other units in the packet.\n\n*Units may not be compared across packets.\n\nQuestion 1. Let $X$ be the number of unique comparisons that have been made after $T$ packets have been acquired. What is the expected value and variance of $X$?\nQuestion 2: Let $T_\\star$ be the smallest number of packets required to make all of the $\\binom{M}{2}$ comparisons. What is the expected value and variance of $T_\\star$?\n", "A": "A Solution for Question 1\n\\begin{align*}\nE(X) &= \\binom{M}{2}\\left(1 - (1-p)^T\\right) \\\\\nV(X) &= \\binom{M}{2}(1-p)^T\\left(1 -\\binom{M}{2}(1-p)^T\\right)  + 6\\binom{M}{3}(1-q)^T + 6\\binom{M}{4}(1-r)^T\n\\end{align*}\nwhere\n\\begin{align*}\np &= \\frac{K(K-1)}{M(M-1)} \\\\\nq &= \\frac{2\\binom{M-2}{K-2} - \\binom{M-3}{K-3}}{\\binom{M}{K}} \\\\\nr &= \\frac{2\\binom{M-2}{K-2} - \\binom{M-4}{K-4}}{\\binom{M}{K}} \n\\end{align*}\nThe following plot shows $E(X)$ and $E(X) - 2\\sqrt{V(X)}$ as a function of $T$ for the case where $M=50$ and $K=10$.\n\n\nDerivation of Results\nNotation and Preliminaries\nLet $A_{ij,t}$ be the event that units $i$ and $j$ are compared in packet $t$. It is easy to see that $P(A_{ij, t}) = \\frac{K(K-1)}{M(M-1)}$. Now let $B_{ij} = \\cap_{t=1}^TA^c_{ij,t}$ be the event that units $i$ and $j$ are not compared in any packet.\n\\begin{align*}\nP(A_{ij}) &= P\\left(\\bigcap_{t=1}^TA^c_{ij,t}\\right) \\\\ \n&= P(A^c_{ij,t})^T \\\\\n&= \\left(1 - \\frac{K(K-1)}{M(M-1)}\\right)^T\n\\end{align*}\nFinally, let $X_{ij}$ be an indicator variable which equals $1$ when $B_{ij}^c$ holds and $0$ otherwise. Note that $$X_{ij} \\sim \\text{Bern}\\left(1 -\\left(1 - \\frac{K(K-1)}{M(M-1)}\\right)^T\\right).$$ Then the total number of comparisons made can be denoted\n$$X = \\sum_{i < j}X_{ij}$$\nExpected Value of $X$\nBy linearity of expectation we have\n$$E(X) = E\\left(\\sum_{i < j}X_{ij}\\right) = \\sum_{i < j}E\\left(X_{ij}\\right) = \\binom{M}{2}\\left(1 -\\left(1 - \\frac{K(K-1)}{M(M-1)}\\right)^T\\right)$$\nVariance of $X$\nThe variance is slightly tricker. We begin by finding $E(X^2)$. First note that\n\\begin{align*}\nX^2 &= \\left(\\sum_{i < j}X_{ij}\\right)^2 \\\\\n&= \\underbrace{\\sum X_{ij}^2}_{\\binom{M}{2} \\text{ terms}} + \\underbrace{\\sum X_{ij}X_{kl}}_{\\binom{M}{4}\\binom{4}{2} \\text{ terms}} + \\underbrace{\\sum X_{ij}X_{ik} + \\sum X_{ij}X_{jk} + \\sum X_{ik}X_{jk}}_{\\binom{M}{3}\\cdot 3 \\cdot 2 \\text{ terms}}\n\\end{align*}\nWe group the sum in this way so that the terms in each group have the same expected value. Note that the total number of terms is $\\binom{M}{2} + 6\\binom{M}{3} + 6\\binom{M}{4} = \\binom{M}{2}^2$, as expected. We will now look at each of the three cases individually.\nCase One: $E(X_{ij}^2)$\nSince $X_{ij}$ is binary, we have that $X_{ij}^2 = X_{ij}$, thus\n$$E\\left(\\sum X_{ij}^2\\right) = E(X) = \\binom{M}{2}\\left(1 -\\left(1 - \\frac{K(K-1)}{M(M-1)}\\right)^T\\right)$$\nCase Two: $E\\left(X_{ij}X_{kl}\\right)$\nUsing standard facts about products of Bernoulli random variables, we have $E\\left(X_{ij}X_{kl}\\right) = P(X_{ij} = 1, X_{kl} = 1)$ where $i$, $j$, $k$ and $l$ are all distinct units. There are $\\binom{M}{4}$ ways to choose these distinct units, and then $\\binom{4}{2} = 6$ ways to choose a valid index assignment.\nThe event $\\{X_{ij} = 1, X_{kl} = 1\\}$ is equivalent to the event $B_{ij}^c \\cap B_{kl}^c$.\n\\begin{align*}\nE(X_{ij}X_{kl}) &= P(B_{ij}^c \\cap B_{kl}^c) \\\\\n&= 1 - P(B_{ij} \\cup B_{kl}) && \\text{DeMorgans Law} \\\\\n&= 1 - \\left[P(B_{ij} + P(B_{kl}) - P(B_{ij} \\cap P(B_{kl})\\right] \\\\\n&= 1 - \\left[2\\left(1-\\frac{K(K-1)}{M(M-1)}\\right) - P\\left(\\bigcap_{t=1}^T\\{B_{ij,t}\\cap B_{kl,t}\\}\\right)\\right] \\\\\n&= 1 - 2\\left(1-\\frac{K(K-1)}{M(M-1)}\\right) +  P(B_{ij,1} \\cap B_{kl, 1})^T && \\text{independence across packets} \\\\\n&= 1 - 2\\left(1-\\frac{K(K-1)}{M(M-1)}\\right) +  \\left(1 - \\frac{2\\binom{M-2}{K-2} - \\binom{M-4}{K-4}}{\\binom{M}{K}} \\right)^T\n\\end{align*}\nCase Three: $E\\left(X_{ij}X_{ik}\\right)$\nThis case is very similar to the the previous case. There are $6\\binom{M}{3}$ terms with this expected value because there are $\\binom{M}{3}$ ways to choose three distinct units, $3$ ways to choose which unit is shared between both indicators and $2$ ways to assign the remaining indices. The probability calculation proceeds in the exact same way as case two, up until the last line which becomes.\n$$E(X_{ij}X_{ik}) = 1 - 2\\left(1-\\frac{K(K-1)}{M(M-1)}\\right) +  \\left(1 - \\frac{2\\binom{M-2}{K-2} - \\binom{M-3}{K-3}}{\\binom{M}{K}} \\right)^T$$\nPutting Everything Together\nTo simplify notation, lets define\n\\begin{align*}\np &= \\frac{K(K-1)}{M(M-1)} \\\\\nq &= \\frac{2\\binom{M-2}{K-2} - \\binom{M-3}{K-3}}{\\binom{M}{K}} \\\\\nr &= \\frac{2\\binom{M-2}{K-2} - \\binom{M-4}{K-4}}{\\binom{M}{K}} \n\\end{align*}\nThen we have\n\\begin{align*}\nE(X^2) &= \\binom{M}{2}\\left(1 -(1 -p)^T\\right) + \\\\\n&\\quad\\quad 6\\binom{M}{3}\\left(1 - 2(1-p)^T + (1-q)^T\\right) + \\\\\n&\\quad\\quad 6\\binom{M}{4}\\left(1 - 2(1-p)^T + (1-r)^T\\right) \\\\\n&= \\binom{M}{2}^2 - \\left(\\binom{M}{2} + 12\\binom{M}{3} + 12\\binom{M}{4}\\right)(1-p)^T + \\\\\n&\\quad\\quad 6\\binom{M}{3}(1-q)^T + 6\\binom{M}{4}(1-r)^T\n\\end{align*}\nAnd the variance can be calculated using the identity $\\text{Var}(X) = E(X^2) - E(X)^2$ which gives (after simplification)\n$$\\text{Var}(X) = \\binom{M}{2}(1-p)^T\\left(1 -\\binom{M}{2}(1-p)^T\\right)  + 6\\binom{M}{3}(1-q)^T + 6\\binom{M}{4}(1-r)^T $$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/541014", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
