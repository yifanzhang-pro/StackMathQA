{"Q": "Conditional vs unconditional expectation I'm having trouble understanding the calculation of conditional versus unconditional expectations in this case:\n\\begin{array}{c c c c}\n& \\quad    &X=1\\quad   &X=-1\\quad  &X=2  \\\\\n&Y=1\\quad  &0.25\\quad  &0.25\\quad  &0    \\\\\n&Y=2\\quad  &0\\quad     &0\\quad     &0.5\n\\end{array}\n\\begin{align}\n~  \\\\\n~  \\\\\n~  \\\\\nE(Y)&=\\sum_Y y f(y)  \\\\\n~  \\\\\n~  \\\\\n~  \\\\\nE(Y|X)&=\\sum_Y y f(y|x)\n\\end{align}\nTo me, both calculations are $1*0.25 + 1*0.25 + 2*0.5 = 1.5$. What am I doing wrong?\n", "A": "If $p_{X,Y}(x,y)$ denotes the joint probability mass function of discrete\nrandom variables $X$ and $Y$, then the marginal mass functions are\n$$\\begin{align}\np_X(x) &= \\sum_y p_{X,Y}(x,y)\\\\\np_Y(y) &= \\sum_x p_{X,Y}(x,y)\n\\end{align}$$\nand so we have that\n$$E[Y] = \\sum_y y\\cdot p_{Y}(y) = \\sum_y y\\cdot \\sum_xp_{X,Y}(x,y)\n= \\sum_x\\sum_y y\\cdot p_{X,Y}(x,y).\\tag{1}$$\nNow, the conditional probability mass function of $Y$ given that \n$X = x$ is\n$$p_{Y\\mid X}(y \\mid X=x) = \\frac{p_{X,Y}(x,y)}{p_X(x)}\n= \\frac{p_{X,Y}(x,y)}{\\sum_y p_{X,Y}(x,y)}\\tag{2}$$\nand\n$$E[Y\\mid X=x] = \\sum_y y\\cdot p_{Y\\mid X}(y \\mid X=x).\\tag{3}$$\nThe value of this expectation depends on our choice of the value $x$\ntaken on by $X$ and is thus a random variable; indeed, it is a function\nof the random variable $X$, and this random variable is denoted\n$E[Y\\mid X]$. It happens to take on values \n$E[Y\\mid X = x_1], E[Y\\mid X=x_2], \\cdots $ with probabilities\n$p_X(x_1), p_X(x_2), \\cdots$ and so its expected value is\n$$\\begin{align}E\\bigr[E[Y\\mid X]\\bigr] &= \\sum_x E[Y\\mid X = x]\\cdot p_X(x)\n&\\text{note the sum is w.r.t}~x\\\\\n&=  \\sum_x \\left[\\sum_y y\\cdot p_{Y\\mid X}(y \\mid X=x)\\right]\\cdot p_X(x)\n&\\text{using}~ (3)\\\\\n&= \\sum_x \\left[\\sum_y y\\cdot \\frac{p_{X,Y}(x,y)}{p_X(x)}\\right]\\cdot p_X(x)\n&\\text{using}~ (2)\\\\\n&= \\sum_x \\sum_y y\\cdot p_{X,Y}(x,y)\\\\\n&= E[Y] &\\text{using}~(1)\n\\end{align}$$\n\nIn general, the number $E[Y\\mid X = x]$ need not equal\nthe number $E[Y]$ for any $x$.  But, if $X$ and $Y$ are\nindependent random variables and so $p_{X,Y}(x,y) = p_X(x)p_Y(y)$\nfor all $x$ and $y$, then\n$$p_{Y\\mid X}(y \\mid X=x) = \\frac{p_{X,Y}(x,y)}{p_X(x)}\n= \\frac{p_X(x)p_Y(y)}{p_X(x)} = p_Y(y)\\tag{4}$$\nand so $(3)$ gives\n$$E[Y\\mid X=x] = \\sum_y y\\cdot p_{Y\\mid X}(y \\mid X=x)\n= \\sum_y y\\cdot p_Y(y) = E[Y]$$\nfor all $x$, that is, $E[Y\\mid X]$ is a degenerate random\nvariable that equals the number $E[Y]$ with probability $1$.\n\nIn your particular example, BabakP's answer after correction\nby Moderator whuber shows that $E[Y\\mid X = x]$ is a random\nvariable that takes on values $1, 1, 2$ with probabilities\n$0.25, 0.25, 0.5$ respectively and so its expectation is\n$0.25\\times 1 + 0.25\\times 1 + 0.5\\times 2 = 1.5$ while the\n$Y$ itself is a random variable taking on values $1$ and $1$\nwith equal probability $0.5$ and so $E[Y] = 1\\times 0.5 + 2\\times 0.5 = 1.5$\nas indeed one expects from the law of iterated expectation\n$$E\\left[[Y\\mid X]\\right] = E[Y].$$ \nIf the joint pmf was intended\nto illustrate the difference between conditional\nexpectation and expectation, then it was a spectacularly bad choice\nbecause the random variable $E[Y\\mid X]$ turns out to have the\nsame distribution as the random variable $Y$, and so the expected\nvalues are necessarily the same. More generally, $E[Y\\mid X]$ does\nnot have the same distribution as $Y$ but their expected values are\nthe same.\nConsider for exampple, the joint pmf\n$$\\begin{array}{c c c c}\n& \\quad    &X=1\\quad   &X=-1\\quad  &X=2  \\\\\n&Y=1\\quad  &0.2\\quad  &0.2\\quad  &0.1   \\\\\n&Y=2\\quad  &0.2\\quad     &0.1\\quad     &0.2\n\\end{array}$$\nfor which the conditional pmfs of $Y$ are\n$$X=1: \\qquad p_{Y\\mid X}(1\\mid X = 1) = \\frac{1}{2}, p_{Y\\mid X}(2\\mid X = 1) = \\frac{1}{2}\\\\\nX=-1: \\qquad p_{Y\\mid X}(1\\mid X = 1) = \\frac{2}{3}, p_{Y\\mid X}(2\\mid X = 1) = \\frac{1}{3}\\\\\nX=2: \\qquad p_{Y\\mid X}(1\\mid X = 1) = \\frac{1}{3}, p_{Y\\mid X}(2\\mid X = 1) = \\frac{2}{3}$$\nthe conditional means are\n$$\\begin{align}\nE[Y\\mid X = 1] &= 1\\times \\frac{1}{2} + 2 \\times \\frac{1}{2} = \\frac{3}{2}\\\\\nE[Y\\mid X = -1] &= 1\\times \\frac{2}{3} + 2 \\times \\frac{1}{3} = \\frac{4}{3}\\\\\nE[Y\\mid X = 2] &= 1\\times \\frac{1}{3} + 2 \\times \\frac{2}{3} = \\frac{5}{3}\n\\end{align}$$\nthat is, $E[Y\\mid X]$ is a random variable that takes on values\n$\\frac{3}{2}, \\frac{4}{3}, \\frac{5}{3}$ with probabiliities\n$\\frac{4}{10}, \\frac{3}{10}, \\frac{3}{10}$ respectively which is\nnot the same as the distribution of $Y$. Note also that $E[Y] = \\frac{3}{2}$\nhappens to equal $E[Y\\mid X=1]$ but not the\nother two conditional expectations.  While $E[Y\\mid X]$ and $Y$ have\ndifferent distributions, their expected values are the same:\n$$E\\left[E[Y\\mid X]\\right] = \\frac{3}{2}\\times\\frac{4}{10}\n+\\frac{4}{3}\\times\\frac{3}{10} + \\frac{5}{3}\\times \\frac{3}{10}\n= \\frac{3}{2} = E[Y].$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/68810", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 2, "answer_id": 0}}
{"Q": "Why is it that natural log changes are percentage changes? What is about logs that makes this so? Can somebody explain how the properties of logs make it so you can do log linear regressions where the coefficients are interpreted as percentage changes?\n", "A": "These posts all focus on the difference between two values as a proportion of the the first: $\\frac{y-x}{x}$ or $\\frac{y}{x} - 1$.  They explain why\n$$\\frac{y}{x} - 1 \\approx \\ln(\\frac{y}{x}) = \\ln(y) - \\ln(x).$$\nYou might be interested in the difference as a proportion of the average rather than as a proportion of the first value, $\\frac{y-x}{\\frac{y+x}{2}}$ rather than $\\frac{y-x}{x}$. The difference as a proportion of the average is relevant when comparing methods of measurement, e.g. when doing a Bland-Altman plot with differences proportional to the mean.  In this case, approximating the proportionate difference with the difference in the logarithms is even better:\n$$\\frac{y-x}{\\frac{y+x}{2}} \\approx \\ln(\\frac{y}{x})  .$$\nHere is why:\nLet $z = \\frac{y}{x}$.\n$$\\frac{y-x}{\\frac{y+x}{2}} = \\frac{2(z-1)}{z+1}$$\nCompare the Taylor series about $z =1$ for $\\frac{2(z-1)}{z+1}$ and $\\ln(z)$.\n$$\\frac{2(z-1)}{z+1} = (z-1) - \\frac{1}{2}(z-1)^2 + \\frac{1}{4}(z-1)^3 + ... + (-1)^{k+1}\\frac{1}{2^{k-1}}(z-1)^k + ....$$\n$$\\ln(z) = (z-1) - \\frac{1}{2}(z-1)^2 + \\frac{1}{3}(z-1)^3 + ... + (-1)^{k+1}\\frac{1}{k}(z-1)^k + ....$$\nThe series are the same out to the $(z-1)^2$ term.  The approximation works quite well from $z=0.5$ to $z=2$ , i.e., when one of the values is up to twice as large as the other, as shown by this figure.\n\nA value of $z=0.5$ or $z=2$ corresponds to a difference that is 2/3 of the average.\n$$\\frac{y-x}{\\frac{y+x}{2}} = \\frac{2x-x}{\\frac{2x+x}{2}} = \\frac{x}{\\frac{3x}{2}} = \\frac{2}{3}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/244199", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "78", "answer_count": 6, "answer_id": 4}}
{"Q": "Let $X_1, X_2,...,X_n \\sim \\textrm{Expo}(1)$ distribution with $n \\geq 3$. How to compute $\\mathrm{P}(X_1 + X_2 \\leq rX_3)$? So let $X_1, X_2,...,X_n$, where $n \\geq 3$ be a random sample from the Expo(1) distribution. \nHow do I set up the computation for \n$\\mathrm{P}(X_1 + X_2 \\leq rX_3 | \\sum_{i=1}^n X_i = t)$ where $r, t > 0$? \nEdit: I do not know how to use the notion that $X_1 + X_2$ is independent from $\\frac{X_1}{X_1 + X_2}$ to proceed with the problem. \n", "A": "So I think here is a way of solving the question analytically:\n$\\mathrm{P}(X_1 + X_2 \\leq rX_3 | \\sum_{i=1}^n X_i = t)\\\\\n\\rightarrow \\mathrm{P}\\left(X_1 + X_2 + X_3 \\leq (1 + r)X_3 | \\sum_{i=1}^n X_i = t\\right)\\\\\n\\rightarrow \\mathrm{P}\\left(\\frac{X_1 + X_2 + X_3}{X_3} \\leq (1 + r) | \\sum_{i=1}^n X_i = t\\right)\\\\\n\\rightarrow \\mathrm{P}\\left(\\frac{X_3}{X_1 + X_2 + X_3} \\geq \\frac{1}{1 + r} | \\sum_{i=1}^n X_i = t\\right)\\\\$\nThe sum of exponentially distributed r.v.'s follows a Gamma distribution. Hence, we have that\n$X_1 + X_2 \\sim \\textrm{Gamma}(2,1)\\\\\nX_3 \\sim \\textrm{Gamma}(1,1)$\nAnd so,\n$\\mathrm{P}\\left(\\frac{X_3}{X_1 + X_2 + X_3} \\geq \\frac{1}{1 + r} | \\sum_{i=1}^n X_i = t\\right)\\\\\n\\rightarrow \\mathrm{P}\\left(\\frac{\\textrm{Gamma}(1,1)}{\\textrm{Gamma}(2,1) + \\textrm{Gamma}(1,1)} \\geq \\frac{1}{1 + r} | \\sum_{i=1}^n X_i = t\\right)\\\\\n\\rightarrow \\mathrm{P}\\left(\\textrm{Beta}(1,2) \\geq \\frac{1}{1 + r} | \\sum_{i=1}^n X_i = t\\right)$\nWe can drop the conditional by noting that\n$\\frac{X_3}{X_1 + X_2 + X_3} \\perp \\sum_{i=1}^n X_i$. \n$U + V \\perp \\frac{U}{U+V}$ if $U$ and $V$ are Gamma distributed, and we let $U = X_1 + X_2$ and $V = X_3$.\nTo compute\n$\\mathrm{P}\\left(\\textrm{Beta}(1,2) \\geq \\frac{1}{1 + r}\\right)$\nwe let a random variable $Z \\sim \\textrm{Beta}(1,2)$\n$\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\int_{\\frac{1}{1+r}}^1 \\frac{1}{\\textrm{Beta}(1,2)}z^{1-1}(1-z)^{2-1}\\textrm{d}z\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\int_{\\frac{1}{1+r}}^1 \\frac{\\Gamma(3)}{\\Gamma(1)\\Gamma(2)}z^{1-1}(1-z)^{2-1}\\textrm{d}z\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\int_{\\frac{1}{1+r}}^1 \\frac{2!}{0!1!}z^{1-1}(1-z)^{2-1}\\textrm{d}z\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\int_{\\frac{1}{1+r}}^1 2z^{0}(1-z)^{1}\\textrm{d}z\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\int_{\\frac{1}{1+r}}^1 2-2z\\textrm{d}z\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = (2z - z^2)|_{\\frac{1}{1+r}}^1\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = 2 - 1 - \\frac{2}{1+r} + \\frac{1}{(1+r)^2}\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\frac{-1+r}{1+r} + \\frac{1}{(1+r)^2}\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\frac{r^2 - 1}{(1+r)^2} + \\frac{1}{(1+r)^2}\\\\\n\\rightarrow\\mathrm{P}\\left(Z \\geq \\frac{1}{1 + r}\\right) = \\frac{r^2}{(1+r)^2}$  \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/249309", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Decompose Covariance by Observations Suppose I observe $n$ iid realizations of two random variables $X$ and $Y$, denoted respectively $x_i$ and $y_i$. Observations can be groupped into two subsamples, with $n_1$ and $n_2$ observations. I want to decompose the sample covariance $\\widehat{\\sigma_{XY}}$ by the contribution of each group of observations plus possibly a residual term. Here is what I have:\n\\begin{eqnarray}\n\\widehat{\\sigma_{XY}} \n& = & \\frac{1}{n} \\sum^n x_i y_i -n \\overline{x} \\overline{y} \\\\\n& = & \\frac{n_1}{n} \\frac{1}{n_1} \\sum^{n_1} x_i y_i + \\frac{n_2}{n} \\frac{1}{n_2} \\sum^{n_2} x_i y_i \\\\ \n&&- n\\left(\\frac{n_1}{n} \\frac{1}{n_1} \\sum^{n_1} x_i + \\frac{n_2}{n} \\frac{1}{n_2} \\sum^{n_2} x_i\\right) \\left(\\frac{n_1}{n} \\frac{1}{n_1} \\sum^{n_1} y_i + \\frac{n_2}{n} \\frac{1}{n_2} \\sum^{n_2} y_i\\right) \\\\ \n& =&  \\frac{n_1}{n} \\frac{1}{n_1} \\left(\\sum^{n_1} x_i y_i -n_1 \\overline{x}_1 \\overline{y}_1\\right) + \\frac{n_2}{n} \\frac{1}{n_2} \\left(\\sum^{n_2} x_i y_i -n_2 \\overline{x}_2 \\overline{y}_2\\right) - \\frac{n_1n_2}{n}\\left(\\overline{x}_1\\overline{y}_2 + \\overline{x}_2 \\overline{y}_1\\right)\\\\\n& = & \\frac{n_1}{n} \\widehat{\\sigma_{XY, 1}} + \\frac{n_2}{n} \\widehat{\\sigma_{XY, 2}}  - \\frac{n_1n_2}{n}\\left(\\overline{x}_1\\overline{y}_2 + \\overline{x}_2 \\overline{y}_1\\right)\n\\end{eqnarray}\nThat is, the overall sample covariance is a weighted average of the sample covariances within each group minus a residual term.\nHowever, in a numerical example this decomposition does not hold as I do not obtain the overall sample covariance on the LHS. Especially the residual term seems too large.\nI would appreciate if somebody can double-check whether there is a mistake in the above.\nMany thanks\n", "A": "Covariance of $X$ and $Y$ is $\\mathrm{Cov}(X,Y) = \\mathrm{E}(XY) - \\mathrm{E}(X)\\mathrm{E}(Y)$, so the estimate $\\widehat{\\sigma_{XY}} = \\sigma$ \n$\\sigma = \\frac{1}{n} \\left(\\sum^n x_i y_i - n \\overline{x} \\overline{y}\\right) =  -\\overline{x} \\overline{y} + \\frac{1}{n} \\sum^n x_i y_i$\nNB: factor $\\frac{1}{n}$ does not multiply $\\overline{x} \\overline{y}$.\nIt follows that \n\\begin{eqnarray}\n\\sigma \n& = & \\frac{n_1}{n} \\sigma_1 + \\frac{n_2}{n} \\sigma_2  - \\frac{n_1 n_2}{n^2}\\left(\\overline{x}_1\\overline{y}_2 + \\overline{x}_2 \\overline{y}_1 - \\overline{x}_1 \\overline{y}_1 - \\overline{x}_2 \\overline{y}_2\n \\right)\n\\end{eqnarray}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/299205", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Conditional Probability Distribution of Multivariate Gaussian I've been working on the following question:\nLet the vector $(X,Y,Z)$ be a multivariate Gaussian random variables with mean vector and covariance matrix:\n$\\mu = \\begin{pmatrix} 0 \\\\ 3 \\\\ -1 \\end{pmatrix}$ and $\\Sigma = \\begin{pmatrix} 16 & 8 & 0 \\\\ 8 & 9 & -2 \\\\ 0 & -2 & 1 \\end{pmatrix}$\nDerive the distribution of $(X,Z)$ given $Y=0$\nI know (from my lecture notes) that the conditional distribution of $X_1$ given $X_2=x_2$ is Gaussian with mean: $\\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(x_2-\\mu_2)$\nand variance-covariance matrix: $\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$\nSo I rewrote $\\Sigma$ as $\\Sigma = \\begin{pmatrix} 16 & 0 & 8 \\\\ 0 & 1 & -2 \\\\ 8 & -2 & 9 \\end{pmatrix}$\nAnd then I let $X_1=\\begin{pmatrix} X \\\\ Z \\end{pmatrix}$ and $Y_2=(Y)$. So $E(X_1)=\\begin{pmatrix} 0 \\\\-1 \\end{pmatrix}$ and $E(X_2)=(3)$\nAnd $Cov(\\begin{pmatrix} Y_1 \\\\ Y_2 \\end{pmatrix}) = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$ where $\\Sigma_{11}=\\begin{pmatrix} 16 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\Sigma_{12}=\\begin{pmatrix} 8 \\\\ -2 \\end{pmatrix} = \\Sigma_{21}^T, \\Sigma_{22}=(9)$\nMeaning that $E(Y_1 \\mid Y_2 =0) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 8 \\\\ -2 \\end{pmatrix}(\\frac{1}{9})(0-3) = \\begin{pmatrix} \\frac{-8}{3} \\\\ \\frac{1}{3}\\end{pmatrix}$\nand $Cov(Y_1 \\mid Y_2=0) = \\begin{pmatrix} 16 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 8 \\\\ -2 \\end{pmatrix}(\\frac{1}{9})\\begin{pmatrix} 8 & -2 \\end{pmatrix} = \\begin{pmatrix} 16 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{64}{9} & -\\frac{16}{9} \\\\ -\\frac{16}{9} & \\frac{4}{9} \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 80 & 16 \\\\ 16 & 5 \\end{pmatrix}$\nAm I correct in this approach? Is there something I'm fundamentally missing here? The subsequent question is \"What is the distribution of $(2X-Z,3Y+Z)$?\" How might I approach this one?\nThanks for any help/suggestions/feedback.\n", "A": "You have the correct formulas, but I leave it to you to check whether you've applied them correctly.\nAs for the distribution of $(2X\u2212Z,3Y+Z)$, viewed as a 2 element column vector.\nConsider$(X.Y,Z)$ as a 3 element column vector. You need to determine the matrix $A$ such that $A*(X,Y,Z) = (2X\u2212Z,3Y+Z)$ . Hint: what dimensions must $A$ have to transform a 3 by 1 vector into a 2 by 1 vector?  Then use the result $\\text{Cov} (A*(X,Y,Z)) = A* \\text{Cov}(X,Y,Z)*A^T$ combined with the trivial calculation of the mean, and your knowledge of the type of distribution which a linear transformation of a Multivariate Gaussian has.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/345784", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Variance of an Unbiased Estimator for $\\sigma^2$ Let $X_1, X_2,...X_n\\sim N(0,\\sigma^2)$ independently. Define $$Q=\\frac{1}{2(n-1)}\\sum_{i=1}^{n-1}(X_{i+1}-X_i)^2$$\nI already proved that this Q is an unbiased estimator of $\\sigma^2$. Now I'm stuck with calculating its variance, I've tried using Chi-square but then I realized these $(X_{i+1}-X_i)$ are not independent. Can you guys help me with this? Many thanks in advance.\n", "A": "The easiest way to do this problem is by using vector algebra, re-expressing the estimator as a quadratic form in vector notation:\n$$Q = \\frac{1}{2(n-1)} \\mathbf{X}^\\text{T} \\mathbf{\\Delta} \\mathbf{X}\n\\quad \\quad \\mathbf{\\Delta} \\equiv\n\\begin{bmatrix}\n1  & -1 & 0  & 0  & \\cdots & 0 & 0 & 0 & 0 \\\\\n-1 & 2  & -1 & 0  & \\cdots & 0 & 0 & 0 & 0 \\\\\n0  & -1 & 2  & -1 & \\cdots & 0 & 0 & 0 & 0 \\\\\n0  & 0  & -1 & 2  & \\cdots & 0 & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots  & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0  & 0  & 0  & 0  & \\cdots & 2 & -1 & 0 & 0 \\\\\n0  & 0  & 0  & 0  & \\cdots & -1 & 2 & -1 & 0 \\\\\n0  & 0  & 0  & 0  & \\cdots & 0 & -1 & 2 & -1 \\\\\n0  & 0  & 0  & 0  & \\cdots & 0 & 0 & -1 & 1 \\\\\n\\end{bmatrix}.$$\nSince $\\mathbf{X} \\sim \\text{N}(\\boldsymbol{0},\\sigma^2 \\boldsymbol{I})$ we can compute the expected value of the quadratic form to confirm that the estimator is unbiased:\n$$\\begin{equation} \\begin{aligned}\n\\mathbb{E}(Q) \n&= \\frac{1}{2(n-1)} \\cdot \\mathbb{E}(\\mathbf{X}^\\text{T} \\mathbf{\\Delta} \\mathbf{X}) \\\\[6pt]\n&= \\frac{\\sigma^2}{2(n-1)} \\cdot \\text{tr}(\\mathbf{\\Delta} \\boldsymbol{I}) \\\\[6pt]\n&= \\frac{\\sigma^2}{2(n-1)} \\cdot (1 + 2 + 2 + \\cdots + 2 + 2 + 1) \\\\[6pt]\n&= \\frac{\\sigma^2}{2(n-1)} \\cdot (2n-2) \\\\[6pt]\n&= \\frac{2(n-1)}{2(n-1)} \\cdot \\sigma^2 = \\sigma^2. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nThis confirms that the estimator is unbiased.  Now, to get the variance of the estimator we can compute the variance of a quadratic form for the case of a joint normal random vector, which gives:\n$$\\begin{equation} \\begin{aligned}\n\\mathbb{V}(Q) \n&= \\frac{1}{4(n-1)^2} \\cdot \\mathbb{V}(\\mathbf{X}^\\text{T} \\mathbf{\\Delta} \\mathbf{X}) \\\\[6pt]\n&= \\frac{\\sigma^4}{2(n-1)^2} \\cdot \\text{tr}(\\mathbf{\\Delta} \\boldsymbol{I} \\mathbf{\\Delta} \\boldsymbol{I}) \\\\[6pt]\n&= \\frac{\\sigma^4}{2(n-1)^2} \\cdot \\text{tr}(\\mathbf{\\Delta}^2) \\\\[6pt]\n&= \\frac{\\sigma^4}{2(n-1)^2} \\cdot (2 + 6 + 6 + \\cdots + 6 + 6 + 2) \\\\[6pt]\n&= \\frac{\\sigma^4}{2(n-1)^2} \\cdot (6n - 8) \\\\[6pt]\n&= \\frac{3n-4}{(n-1)^2} \\cdot \\sigma^4 \\\\[6pt]\n&= \\frac{3n-4}{2n-2} \\cdot \\frac{2}{n-1} \\cdot \\sigma^4. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nThis gives us an expression for the variance of the estimator.  (I have framed it in this form to compare it to an alternative estimator below.)  As $n \\rightarrow \\infty$ we have $\\mathbb{V}(Q) \\rightarrow 0$ so it is a consistent estimator.  It is worth contrasting the variance of this estimator with the variance of the sample variance estimator (see e.g., O'Neill 2014, Result 3), which is:\n$$\\mathbb{V}(S^2) = \\frac{2}{n-1} \\cdot \\sigma^4.$$\nComparing these results we see that the estimators have the same variance when $n=2$, and when $n>2$ the sample variance estimator has lower variance than the present estimator.  In other words, the sample variance is a more efficient estimator than the present estimator.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/390822", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Determine if the following Markov chain is positive recurrent, null recurrent or transcient We consider the Markov Chain with transition probabilities \n$$\np(i,0)=\\frac{1}{i^2 +2},\\qquad p(i,i+1)= \\frac{i^2 +1}{i^2 +2}.\n$$\nDetermine if this Markov chain is positive recurrent, null recurrent  or transcient.\n\nMy attempt: Since all states are connected to $0$, then it is sufficient to determine if $0$ is a positive recurring state.\nConsider $T_{0}$ the hitting time, that is\n$$T_{0}=\\inf\\left\\{m\\geq 1\\: :\\: X_{m}=0\\right\\}.$$\nNote that \n$$\n\\mathbb{P}(T_{0}=n|X_{0}=0)=\\left(\\frac{1}{2}\\times\\frac{2}{3}\\times\\cdots\\times\\frac{(n-2)^2+1}{(n-2)^{2}+2}\\right)\\left(\\frac{1}{(n-1)^{2}+2}\\right)\n$$\nTherefore, we have\n$$\n\\mathbb{E}(T_{0}|X_{0}=0)=\\sum_{n=1}^{\\infty}n\\times \\left(\\frac{1}{2}\\times\\frac{2}{3}\\times\\cdots\\times\\frac{(n-2)^2+1}{(n-2)^{2}+2}\\right)\\left(\\frac{1}{(n-1)^{2}+2}\\right).\n$$\nI need to determine if this series converges or diverges. I have tried to limit it superiorly and inferiorly but I have not found good bounds.\n", "A": "Handle the product in the summation by taking logs:\n$$\\eqalign{\n\\log \\prod_{i=0}^{n-2} \\frac{i^2+1}{i^2+2} &= \\sum_{i=0}^n \\log\\left(1 - \\frac{1}{i^2+2}\\right)\\\\\n&\\ge -\\sum_{i=0}^n \\frac{1}{i^2+2} \\\\\n&\\gt -\\sum_{i=0}^\\infty \\frac{1}{i^2+2} \\\\\n&\\gt -\\frac{1}{2} - \\sum_{i=1}^\\infty \\frac{1}{i^2} \\\\\n&= -\\frac{3+\\pi^2}{6}.\n}$$\nConsequently you can underestimate the sum as \n$$\\eqalign {\n\\sum_{n=1}^\\infty n \\prod_{i=0}^{n-2} \\frac{i^2+1}{i^2+2} \\frac{1}{(n-1)^2+2} & \\gt \\frac{1}{e^{(3+\\pi^2)/6}} \\sum_{n=1}^\\infty \\frac{n}{(n-1)^2+2}.\n}$$\nThe right hand side diverges (compare it to $\\int_1^\\infty \\frac{x}{x^2+2}\\mathrm{d}x$).\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/439231", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Prove variance of mean estimator of an AR(1) model I need to solve the proof as given in the screenshot below. I had tried to do something (under My solution) but I don't know how to proceed with the proof. I'm not sure whether I did something wrong or mistake. Please help me to continue with the proof. Any help will be greatly appreciated! Thanks!\n\n", "A": "Compute directly (not the best homework question, if that's what this is):\n\\begin{align*}\nVar(\\bar{Y}) &= Var(\\frac{1}{n} \\sum_{t=1}^n Y_t) \\\\\n             &= \\frac{1}{n^2} Var( \\sum_{t=1}^n Y_t) \\\\\n             &= \\frac{1}{n} \\gamma(0) + 2 \\frac{1}{n} \\sum_{h = 1}^{n-h} \\frac{n-h}{n} \\gamma(h),\n\\end{align*}\nwhere $\\gamma(h) = \\frac{\\beta^h}{1-\\beta^2}$ is the autocovariance function at lag $h$.\nSubstituting $\\gamma(0) = \\frac{1}{1-\\beta^2}$ gives the first term in your sum\n$$ \n\\frac{1}{n} \\gamma(0) = \\frac{1}{n} \\frac{1}{1-\\beta^2}.\n$$\nSimilarly,\n\\begin{align*}\n2 \\frac{1}{n} \\sum_{h = 1}^{n-h} \\frac{n-h}{n} \\gamma(h) &= \\frac{2}{n^2(1-\\beta^2)}  \\sum_{h=1}^{n-h} (n-h) \\beta^h \\\\\n&=  \\frac{2}{n^2(1-\\beta^2)}  \\sum_{h=1}^{n-h} \\sum_{j=1}^{n-h} \\beta^j \\\\ \n&=  \\frac{2}{n^2(1-\\beta^2)}  \\sum_{h=1}^{n-h} ( \\frac{1-\\beta^{n+1-h}}{1-\\beta}) \\\\\n&=  \\frac{2}{n^2(1-\\beta^2)(1-\\beta)}  \\sum_{h=1}^{n-h} ( 1-\\beta^{n+1-h} )\\\\\n&= \\cdots.\n\\end{align*}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/453118", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Largest singular values Given the positive semi-definite, symmetric matrix $A = bb^T + \\sigma^2I$ where b is a column vector is it possible to find the singular values and singular vectors of the matrix analytically? I know that it has real eigenvalues since it's symmetric and positive semidefinite but not sure about solving directly for those values and their corresponding vectors.\n", "A": "The singular values are the eigenvalues of $A.$ By definition, when there exists a nonzero vector $\\mathbf x$ for which $A\\mathbf{x}=\\lambda \\mathbf{x},$ $\\lambda$ is an eigenvalue and $\\mathbf{x}$ is a corresponding eigenvector.\nNote, then, that\n$$A\\mathbf{b} = (\\mathbf{b}\\mathbf{b}^\\prime + \\sigma^2I)\\mathbf{b} = \\mathbf{b}(\\mathbf{b}^\\prime \\mathbf{b}) + \\sigma^2 \\mathbf{b} = (|\\mathbf{b}|^2+\\sigma^2)\\mathbf{b},$$\ndemonstrating that $\\mathbf{b}$ is an eigenvector with eigenvalue $\\lambda_1 = |\\mathbf{b}|^2 + \\sigma^2.$\nFurthermore, whenever $\\mathbf{x}$ is a vector orthogonal to $\\mathbf{b}$ -- that is, when $\\mathbf{b}^\\prime \\mathbf{x} = \\pmatrix{0},$ we may similarly compute\n$$A\\mathbf{x} = (\\mathbf{b}\\mathbf{b}^\\prime + \\sigma^2I)\\mathbf{x} = \\mathbf{b}(\\mathbf{b}^\\prime \\mathbf{x}) + \\sigma^2 \\mathbf{x} = (0+\\sigma^2)\\mathbf{x},$$\nshowing that all such vectors are eigenvectors with eigenvalue $\\sigma^2.$\nProvided these vectors are in a finite dimensional vector space of dimension $n$ (say), a straightforward induction establishes that the vectors $x$ for which $\\mathbf{b}^\\prime \\mathbf{x}=0$ form a subspace $\\mathbf{b}^\\perp$ of dimension $n-1.$  Let $\\mathbf{e}_2, \\ldots, \\mathbf{e}_n$ be an orthonormal basis for this subspace.  It extends to an orthonormal basis $\\mathscr{E} = (\\mathbf{\\hat  b}, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n)$ of the whole space where $\\mathbf{\\hat b} = \\mathbf{b}/|\\mathbf{b}|$.  In terms of this basis the matrix of $A$ therefore is\n$$\\operatorname{Mat}(A, \\mathscr{E}, \\mathscr{E}) = \\pmatrix{|\\mathbf{b}|^2+\\sigma^2 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & 0  & \\cdots & 0 \\\\\n0 & 0 & \\ddots & \\vdots & \\vdots \\\\\n\\vdots & \\vdots & \\cdots & \\ddots & 0 \\\\\n0 & 0 & \\cdots & 0 & \\sigma^2\n}$$\nWhether or not every step of this derivation was clear, you can verify the result by setting\n$$Q = \\left(\\mathbf{b}; \\mathbf{e}_2; \\ldots; \\mathbf{e}_n\\right)$$\nto be the matrix with the the given columns and computing\n$$Q\\,\\operatorname{Mat}(A, \\mathscr{E}, \\mathscr{E})\\,Q^\\prime = \\mathbf{b}^\\prime + \\sigma^2I = A.$$\nThis is explicitly a singular value decomposition of the form $U\\Sigma V^\\prime$ where $V=Q,$ $\\Sigma= \\operatorname{Mat}(A, \\mathscr{E}, \\mathscr{E}),$ and $U=Q^\\prime.$\nThe Gram Schmidt process provides a general algorithm to find $\\mathscr{E}$ (and therefore $Q$): its input is the series of vectors $\\mathbf{\\hat b}$, $(1,0,\\ldots,0)^\\prime,$ and so on through $(0,\\ldots,0,1)^\\prime.$  After $n-1$ steps it will produce an orthonormal basis including the starting vector $\\mathbf b.$\n\nAs an example, let $\\mathbf{b} = (3,4,0)^\\prime.$ With $\\sigma^2 = 1,$ compute\n$$\\mathbf{b}\\mathbf{b}^\\prime + \\sigma^2 I = \\pmatrix{10&12&0\\\\12&17&0\\\\0&0&1}$$\nHere, $|\\mathbf{b}|^2 = 3^2+4^2+0^2=5^2,$ so that $\\mathbf{\\hat b} = \\mathbf{b}/5 = (3/5,4/5,0)^\\prime.$  One way to extend this to an orthonormal basis is to pick $\\mathbf{e}_2 = (-4/5,3/5,0)^\\prime$ and $\\mathbf{e}_3 = (0,0,1)^\\prime.$  Thus\n$$Q = \\pmatrix{3/5&4/5&0\\\\-4/5&3/5&0\\\\0&0&1}$$\nand we may confirm that\n$$\\begin{align}\nQ\\,\\operatorname{Mat}(A, \\mathscr{E}, \\mathscr{E})\\,Q^\\prime &= \\pmatrix{3/5&4/5&0\\\\-4/5&3/5&0\\\\0&0&1}\\pmatrix{5^2+1^2&0&0\\\\0&1&0\\\\0&0&1}\\pmatrix{3/5&-4/5&0\\\\4/5&3/5&0\\\\0&0&1}\\\\\n&=\\pmatrix{10&12&0\\\\12&17&0\\\\0&0&1}\n\\end{align}$$\nas intended.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/494628", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Probability distribution of the distance of a point in a square to a fixed point The question is, given a fixed point with coordinates X,Y, in a square of size N. What is the probability distribution of the distance to a random point. More specifically, what is the probability distribution of the square root of the square sum of 2 randomly independent uniform variables (0,N).\n", "A": "To find the distribution of the distance between the origin, $(0,0)$ and a random point on the unit square $(0,1)^2$ we can integrate over the area that is within $d$ of the origin. That is, find the cdf $P(\\sqrt{X^2 + Y^2} \\leq d)$, then take the derivative to find the pdf. Extensions to the square $(0,N)$ are immediate.\nCase 1: $ 0 \\leq d \\leq 1$\n$$\nF_{D}(d) = P(\\sqrt{X^2 + Y^2} \\leq d) = P(X^2 + Y^2 \\leq d^2) \\\\\n= \\int_{0}^{d} \\int_{0}^{\\sqrt{d^2-x^2}}1dydx \\\\\n= \\int_{0}^{d} \\sqrt{d^2 - x^2}dx\\\\\n= \\frac{d^2\\pi}{4}\n$$\nCase 2: $1 \\leq d \\leq \\sqrt{2}$\n$$\nF_{D}(d) = \\int_{0}^{\\sqrt{d^2 - 1}} 1 dx + \\int_{\\sqrt{d^2 - 1}}^{1}\\int_{0}^{\\sqrt{d^2-x^2}}1dydx \\\\\n=\\sqrt{d^2-1} + \\int_{\\sqrt{d^2-1}}^{1}\\sqrt{d^2-x^2}dx\\\\\n= \\sqrt{d^2 -1} + \\frac{1}{2}\\left\\{t\\sqrt{d^2-t^2}+d^2\\tan^{-1}\\left(\\frac{t}{\\sqrt{d^2-t^2}}\\right) \\right\\}|_{\\sqrt{d^2-1}}^{1} \\\\\n= \\sqrt{d^2-1} + \\frac{1}{2}\\left\\{\\sqrt{d^2-1}+d^2\\tan^{-1}\\left(\\frac{1}{\\sqrt{d^2-1}}\\right) - \\sqrt{d^2-1}\\sqrt{1}-d^2\\tan^{-1}\\left(\\frac{\\sqrt{d^2-1}}{1}\\right)\\right\\} \\\\\n=\\sqrt{d^2-1} + \\frac{d^2}{2}\\left\\{ \\tan^{-1}\\left(\\frac{1}{\\sqrt{d^2-1}}\\right)-\\tan^{-1}\\left(\\sqrt{d^2-1}\\right)\\right\\}\n$$\nTaking the derivative gives the density\n$$\nf_{D}(d) = \\frac{d\\pi}{2}, 0 \\leq d \\leq 1\\\\\nf_{D}(d) = d\\left\\{\\tan^{-1}\\left(\\frac{1}{\\sqrt{d^2-1}}\\right)-\\tan^{-1}(\\sqrt{d^2-1})\\right\\}, 1 \\leq d \\leq \\sqrt{2} \\\\\n$$\nComparing the result with @BruceET's simulation answer on Expected average distance from a random point in a square to a corner, we find it matches exactly.\nden <- function(d) {\n  if(d < 0) {\n    return(0)\n  }\n  if(d < 1) {\n    return(d*pi/2)\n  }\n  if(d < sqrt(2)) {\n    return(d*(atan(1/sqrt(d^2-1)) - atan(sqrt(d^2-1))))\n  }\n  if(d > sqrt(2)) {\n    return(0)\n  }\n  stop()\n}\nys <- xs <- seq(from=0,t=1.42,by=0.01)\nfor(i in seq_along(xs)){\n  ys[i] <- den(xs[i])\n}\n\nset.seed(2021)\nx = runif(10^6);  y = runif(10^6)\nd = sqrt(x^2 + y^2)\nhist(d, prob=T, br=seq(0,1.42,0.02),xlim=c(0,1.5),ylim=c(0,1.5))\nlines(xs,ys,col=\"red\",lwd=2)\n\n\nCreated on 2021-06-15 by the reprex package (v2.0.0)\nBy symmetry this is equal to the distance between random points and $(0,1)$, $(1,0)$, and $(1,1)$. Finding the distance to a point on the boundary or the interior of the square will require considering more cases in the cumulative probability function.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/530697", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Compute Exact Null Distribution for Friedman Statistic Problem Statement: If there are no ties and $b=2,\\;k=3,$ derive the exact null distribution of $F_r,$ the Friedman statistic.\nNote: This is Exercise 15.35 in Mathematical Statistics with Applications, 5th Ed., by Wackerly, Mendenhall, and Scheaffer.\nMy Work So Far: There are two equivalent formulae for the Friedman statistic, and I will use this one:\n$$F_r=\\frac{12}{bk(k+1)}\\,\\sum_{i=1}^kR_i^2-3b(k+1).$$\nFor our situation, this simplifies down to\n$$F_r=\\frac{1}{2}\\sum_{i=1}^3R_i^2-24.$$\nI wrote the following (quick and dirty) function in R to compute this statistic:\ncompute_Friedman_Fr = function(y)\n{\n   0.5*(sum(y[1:2])^2+sum(y[3:4])^2+sum(y[5:6])^2)-24\n}\n\nwhere I am considering the first two elements of the list $y$ as the first treatment, the second two elements as the second treatment, and so on. The Friedman statistic is invariant to two kinds of permutations: permuting the treatments, and permuting the ranks within treatments. An example of a function call would be:\n> compute_Friedman_Fr(c(1,2,3,4,5,6))\n[1] 65.5\n\nHence, we can construct the following table:\n$$\n\\begin{array}{cc}\n\\textbf{Rank Array} &F_r \\\\ \\hline\n(1,2,3,4,5,6) &65.5\\\\\n(1,2,3,5,4,6) &62.5\\\\\n(1,2,3,6,4,5) &61.5\\\\\n(1,3,2,4,5,6) &62.5\\\\\n(1,3,2,5,4,6) &58.5\\\\\n(1,3,2,6,4,5) &56.5\\\\\n(1,4,2,3,5,6) &61.5\\\\\n(1,4,2,5,3,6) &53.5\\\\\n(1,4,2,6,3,5) &52.5\\\\\n(1,5,2,3,4,6) &56.5\\\\\n(1,5,2,4,3,6) &52.5\\\\\n(1,5,2,6,3,4) &50.5\\\\\n(1,6,2,3,4,5) &53.5\\\\\n(1,6,2,4,3,5) &50.5\\\\\n(1,6,2,5,3,4) &49.5\n\\end{array}\n$$\nMy Question: This bears absolutely no resemblance to the book's answer of\n\\begin{align*}\nP(F_r=4)&=P(F_r=0)=1/6\\\\\nP(F_r=3)&=P(F_r=1)=1/3\n\\end{align*}\nI feel like there is a category error somewhere. How could $F_r,$ with the formula above, possibly ever equal $0,1,3,$ or $4?$ Am I wrong? If so, why? Is the book wrong? If so, why?\n", "A": "So, if I understand @whuber's comment correctly, I am computing the statistic incorrectly. I should be doing this, instead (as one example):\n$$\n\\begin{array}{c|ccc}\nB/T &1 &2 &3 \\\\ \\hline\nA   &1 &2 &3 \\\\\nB   &1 &2 &3 \\\\ \\hline\n\\text{Total} \\;(R_i) &2 &4 &6\n\\end{array},\n$$\nso that\n$$F_r=\\frac12(4+16+36)-24=4.$$\nAnother example:\n$$\n\\begin{array}{c|ccc}\nB/T &1 &2 &3 \\\\ \\hline\nA   &1 &3 &2 \\\\\nB   &1 &2 &3 \\\\ \\hline\n\\text{Total} \\;(R_i) &2 &5 &5\n\\end{array},\n$$\nso that\n$$F_r=\\frac12(4+25+25)-24=3.$$\nMany thanks, whuber, as always!\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/559215", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
