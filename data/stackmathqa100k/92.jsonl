{"Q": "KL divergence between two univariate Gaussians I need to determine the KL-divergence between two Gaussians. I am comparing my results to these, but I can't reproduce their result. My result is obviously wrong, because the KL is not 0 for KL(p, p).\nI wonder where I am doing a mistake and ask if anyone can spot it.\nLet $p(x) = N(\\mu_1, \\sigma_1)$ and $q(x) = N(\\mu_2, \\sigma_2)$. From Bishop's\nPRML I know that\n$$KL(p, q) = - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx$$\nwhere integration is done over all real line, and that\n$$\\int p(x) \\log p(x) dx = -\\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2),$$\nso I restrict myself to $\\int p(x) \\log q(x) dx$, which I can write out as\n$$-\\int p(x) \\log \\frac{1}{(2 \\pi \\sigma_2^2)^{(1/2)}} e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}} dx,$$\nwhich can be separated into\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) - \\int p(x) \\log e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}} dx.$$\nTaking the log I get\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) - \\int p(x) \\bigg(-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2} \\bigg) dx,$$\nwhere I separate the sums and get $\\sigma_2^2$ out of the integral.\n$$\\frac{1}{2} \\log (2 \\pi \\sigma^2_2) + \\frac{\\int p(x) x^2 dx - \\int p(x) 2x\\mu_2 dx + \\int p(x) \\mu_2^2 dx}{2 \\sigma_2^2}$$\nLetting $\\langle \\rangle$ denote the expectation operator under $p$, I can rewrite this as\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\langle x^2 \\rangle - 2 \\langle x \\rangle \\mu_2 + \\mu_2^2}{2 \\sigma_2^2}.$$\nWe know that $var(x) = \\langle x^2 \\rangle - \\langle x \\rangle ^2$. Thus\n$$\\langle x^2 \\rangle = \\sigma_1^2 + \\mu_1^2$$\nand therefore \n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + \\mu_1^2 - 2 \\mu_1 \\mu_2 + \\mu_2^2}{2 \\sigma_2^2},$$\nwhich I can put as\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2}.$$\nPutting everything together, I get to\n\\begin{align*}\nKL(p, q) &= - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\\\\\\\\\n&= \\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2)\\\\\\\\\n&= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2}.\n\\end{align*}\nWhich is wrong since it equals $1$ for two identical Gaussians.\nCan anyone spot my error?\nUpdate\nThanks to mpiktas for clearing things up. The correct answer is:\n$KL(p, q) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}$\n", "A": "I did not have a look at your calculation but here is mine with a lot of details.\nSuppose $p$ is the density of a normal random variable with mean $\\mu_1$ and variance $\\sigma^2_1$, and that $q$ is the density of a normal random variable with mean $\\mu_2$ and variance $\\sigma^2_2$.\nThe Kullback-Leibler distance from $q$ to $p$ is:\n$$\\int \\left[\\log( p(x)) - \\log( q(x)) \\right] p(x) dx$$\n\\begin{align}&=\\int \\left[ -\\frac{1}{2} \\log(2\\pi) - \\log(\\sigma_1) - \\frac{1}{2} \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 + \\frac{1}{2}\\log(2\\pi) + \\log(\\sigma_2) + \\frac{1}{2} \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2  \\right]\\times \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2\\right] dx\\\\&=\\int \\left\\{\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2} \\left[ \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2 - \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\right] \\right\\}\\times \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2\\right] dx\\\\&\n=E_{1} \\left\\{\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2} \\left[ \\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2 - \\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\right]\\right\\}\\\\&=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2\\sigma_1^2} E_1 \\left\\{(X-\\mu_1)^2\\right\\}\\\\ &=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2} E_1 \\left\\{(X-\\mu_2)^2\\right\\} - \\frac{1}{2};\\end{align}\n(Now note that $(X - \\mu_2)^2 = (X-\\mu_1+\\mu_1-\\mu_2)^2 = (X-\\mu_1)^2 + 2(X-\\mu_1)(\\mu_1-\\mu_2) + (\\mu_1-\\mu_2)^2$)\n\\begin{align}&=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{1}{2\\sigma_2^2}\n\\left[E_1\\left\\{(X-\\mu_1)^2\\right\\} + 2(\\mu_1-\\mu_2)E_1\\left\\{X-\\mu_1\\right\\} + (\\mu_1-\\mu_2)^2\\right] - \\frac{1}{2}\\\\&=\\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}.\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/7440", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "138", "answer_count": 2, "answer_id": 0}}
{"Q": "How to compute the standard error of the mean of an AR(1) process? I try to compute the standard error of the mean for a demeaned AR(1) process $x_{t+1} = \\rho x_t + \\varepsilon_{t+1} =\\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t+1-i}$\nHere is what I did:\n$$\n\\begin{align*}\nVar(\\overline{x}) &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} x_t\\right) \\\\\n                  &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} \\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t-i}\\right) \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_0 +  & \\rho^1 \\varepsilon_{-1} +  & \\rho^2 \\varepsilon_{-2} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{-\\infty} + \\\\ \n                                                      \\rho^0  \\varepsilon_1 +  & \\rho^1 \\varepsilon_{0}  +   & \\rho^2 \\varepsilon_{-1} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{1-\\infty} + \\\\\n                                                      \\vdots               & \\vdots                 & \\vdots                & \\ddots  & \\vdots \\\\\n                                                      \\rho^0\\varepsilon_{N-1} + & \\rho^1 \\varepsilon_{N-2} +  & \\rho^2 \\varepsilon_{N-3} + & \\cdots  & \\rho^{\\infty} \\varepsilon_{N-1-\\infty} + \\\\\n                                      \\end{pmatrix} \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_{N-1} + \\\\ \n                                                      (\\rho^0 + \\rho^1)  \\varepsilon_{N-2} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  \\varepsilon_{N-3} + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  \\varepsilon_{1} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  \\varepsilon_{0} + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})  \\varepsilon_{-1} + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  \\varepsilon_{-2} + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                 &= \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^0   + \\\\ \n                                                      (\\rho^0 + \\rho^1)  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                &= \\frac{N \\sigma_{\\varepsilon}^2}{N^2} (\\rho^0 + \\rho^1 + \\dots + \\rho^{\\infty}) \\\\\n                &= \\frac{\\sigma_{\\varepsilon}^2}{N} \\frac{1}{1 - \\rho} \\\\\n\\end{align*}\n$$\nProbably, not every step is done in the most obvious way, so let me add some thoughts. In the third row, I just write out to two sum-signs. Here, the matrix has N rows. In the fourth row, I realign the matrix so that there is one row for every epsilon, so the number of rows is infinite here. Note that the last three parts in the matrix have the same number of elements, just differencing by a factor $\\rho$ in each row. In the fifth row, I apply the rule that the variance of the sum of independent shocks is the sum of the variances of those shocks and notice that each $\\rho^j$ element is summed up $N$ times.\nThe end result looks neat, but is probably wrong. Why do I think so? Because I run a MCS in R and things don't add up:\nnrMCS <- 10000\nN <- 100\npers <- 0.9\nmeans <- numeric(nrMCS)\nfor (i in 1:nrMCS) {\n   means[i] <- mean(arima.sim(list(order=c(1,0,0), ar=pers), n = N))\n}\n#quantile(means, probs=c(0.025, 0.05, 0.5, 0.95, 0.975))\n#That is the empirical standard error\nsd(means)\n0.9459876\n#This should be the standard error according to my formula\n1/(N*(1-pers))\n0.1\n\nAny hints on what I am doing wrong would be great! Or maybe a hint where I can find the correct derivation (I couldn't find anything). Is the problem maybe that I assume independence between the same errors?\n$$Var(X + X) = Var(2X) = 4Var(X) \\neq 2Var(X)$$\nI thought about that, but don't see where I make that erroneous assumption in my derivation.\nUPDATE\nI forgot to square the rhos, as Nuzhi correctly pointed out. Hence it should look like:\n$$                Var(\\overline{x}) = \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^{2\\times0}   + \\\\ \n                                                      (\\rho^0 + \\rho^1)^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)^2  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})^2  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})^2    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})^2  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} $$\n", "A": "Well there are three things as i see it with this question :\n1) In your derivation when your taking the variance of the terms inside Rho should get squared and you should end up with the expression .. i didnt consider the auto covariance earlier ..sorry about that\n$$\nVar(\\overline{x}) = \\frac{\\sigma_{\\varepsilon}^2}{N} \\frac{1}{1 - \\rho^2} + \\sum\\limits_{t=0}^{N-1}\\sum\\limits_{t\\neq j}^{N-1}\\frac{\\sigma_{\\varepsilon}^2}{N^2} \\frac{1}{1 - \\rho^2}\\rho^{|j-t|}$$\n2) In your code you have calculated the variance of xbar ... for the standard error the code should include taking the sqrt of the answer given\n3) You have assumed that the white noise has been generated from a (0,1) distribution when in fact the white noise only has to have constant variance .. i dont know what values of the constant variance R uses to generate the time series ... perhaps you could check on that ..\nHope this helps you :) \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/40585", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 4, "answer_id": 1}}
{"Q": "Correlation coefficient for a uniform distribution on an ellipse I am currently reading a paper that claims that the correlation coefficient for a uniform distribution on the interior of an ellipse\n$$f_{X,Y} (x,y) = \\begin{cases}\\text{constant} & \\text{if} \\ (x,y) \\ \\text{inside the ellipse} \\\\  0 & \\text{otherwise}  \\end{cases}$$\nis given by\n$$\\rho = \\sqrt{1- \\left(\\frac{h}{H}\\right)^2 }$$\nwhere $h$ and $H$ are the vertical heights at the center and at the extremes respectively.\n\nThe author does not reveal how he reaches that and instead only says that we need to change scales, rotate, translate and of course integrate. I would very much like to retrace his steps but I am a bit lost with all that. I would therefore be grateful for some hints.\nThank you in advance.\nOh and for the record\n\nCh\u00e2tillon, Guy. \"The balloon rules for a rough estimate of the correlation coefficient.\" The American Statistician 38.1 (1984): 58-60\n\nIt's quite amusing.\n", "A": "Let $(X,Y)$ be uniformly distributed on the interior of the ellipse\n$$\\frac{x^2}{a^2} + \\frac{y^2}{b^2} = 1$$ where $a$ and $b$ are the\nsemi-axes of the ellipse.  Then, $X$ and $Y$ have marginal densities\n\\begin{align}\nf_X(x) &= \\frac{2}{\\pi a^2}\\sqrt{a^2-x^2}\\,\\,\\mathbf 1_{-a,a}(x),\\\\\nf_X(x) &= \\frac{2}{\\pi b^2}\\sqrt{b^2-y^2}\\,\\,\\mathbf 1_{-b,b}(y),\n\\end{align}\nand it is easy to see that $E[X] = E[Y] = 0$.  Also,\n\\begin{align}\n\\sigma_X^2 = E[X^2] &= \\frac{2}{\\pi a^2}\\int_a^a x^2\\sqrt{a^2-x^2}\\,\\mathrm dx\\\\\n&= \\frac{4}{\\pi a^2}\\int_0^a x^2\\sqrt{a^2-x^2}\\,\\mathrm dx\\\\\n&= \\frac{4}{\\pi a^2}\\times a^4 \\frac 12\\frac{\\Gamma(3/2)\\Gamma(3/2)}{\\Gamma(3)}\\\\\n&= \\frac{a^2}{4},\n\\end{align}\nand similarly, $\\sigma_Y^2 = \\frac{b^2}{4}$. Finally, \n$X$ and $Y$ are uncorrelated random variables.\nLet \\begin{align}\nU &= X\\cos \\theta - Y \\sin \\theta\\\\\nV &= X\\sin \\theta + Y \\cos \\theta\n\\end{align}\nwhich is a rotation transformation applied to $(X,Y)$. Then,\n$(U,V)$ are uniformly distributed on the interior of an\nellipse whose axes do not coincide with the $u$ and $v$ axes.\nBut, it is easy to verify that $U$ and $V$ are zero-mean\nrandom variables and that their variances are \n\\begin{align}\n\\sigma_U^2 &= \\frac{a^2\\cos^2\\theta + b^2\\sin^2\\theta}{4}\\\\\n\\sigma_V^2 &= \\frac{a^2\\sin^2\\theta + b^2\\cos^2\\theta}{4}\n\\end{align}\nFurthermore, \n$$\\operatorname{cov}(U,V) = (\\sigma_X^2-\\sigma_Y^2)\\sin\\theta\\cos\\theta\n= \\frac{a^2-b^2}{8}\\sin 2\\theta$$\nfrom which we can get the value of $\\rho_{U,V}$.\nNow, the ellipse on whose interior $(U,V)$ is uniformly distributed\nhas equation\n$$\\frac{(u \\cos\\theta + v\\sin \\theta)^2}{a^2}\n+ \\frac{(-u \\sin\\theta + v\\cos \\theta)^2}{b^2} = 1,$$\nthat is,\n$$\\left(\\frac{\\cos^2\\theta}{a^2} + \\frac{\\sin^2\\theta}{b^2}\\right) u^2  + \\left(\\frac{\\sin^2\\theta}{a^2} + \\frac{\\cos^2\\theta}{b^2}\\right) v^2 \n+ \\left(\\left(\\frac{1}{a^2} - \\frac{1}{b^2}\\right)\\sin 2\\theta \\right)uv = 1,$$\nwhich can also be expressed as\n$$\\sigma_V^2\\cdot u^2 + \\sigma_U^2\\cdot v^2 \n-2\\rho_{U,V}\\sigma_U\\sigma_V\\cdot uv = \\frac{a^2b^2}{4}\\tag{1}$$\nSetting $u = 0$ in $(1)$ gives \n$\\displaystyle h  = \\frac{ab}{\\sigma_U}$.\nwhile implicit differentiation of $(1)$ with respect to $u$ gives\n$$\\sigma_V^2\\cdot 2u + \\sigma_U^2\\cdot 2v\\frac{\\mathrm dv}{\\mathrm du} \n-2\\rho_{U,V}\\sigma_U\\sigma_V\\cdot \n\\left(v + u\\frac{\\mathrm dv}{\\mathrm du}\\right) = 0,$$\nthat is, the tangent to the ellipse $(1)$ is horizontal at\nthe two points $(u,v)$ on the ellipse for which\n$$\\rho_{U,V}\\sigma_U\\cdot v = \\sigma_v\\cdot u.$$\nThe value of $H$ can be figured out from this, and will (in the\nunlikely event that I have made no mistakes in doing the\nabove calculations) lead to the desired result.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/182293", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 1, "answer_id": 0}}
{"Q": "Marginal distribution of normal random variable with a normal mean I have a question about calculation of conditional density of two normal distributions.  I have random variables $X\\mid M \\sim \\text{N}(M,\\sigma^2)$ and $M \\sim \\text{N}(\\theta, s^2)$, with conditional and marginal densities given by:\n$$\\begin{equation} \\begin{aligned}\nf(x|m) &= \\frac{1}{\\sigma \\sqrt{2\\pi}} \\cdot \\exp \\Big( -\\frac{1}{2} \\Big( \\frac{x-m}{\\sigma} \\Big)^2 \\Big), \\\\[10pt]\nf(m) &= \\frac{1}{s \\sqrt{2\\pi}} \\cdot \\exp \\Big( - \\frac{1}{2} \\Big( \\frac{m-\\theta}{s} \\Big)^2  \\Big).\n\\end{aligned} \\end{equation}$$\nI would like to know the marginal distribution of $X$.  I have multiplied the above densities to form the joint density, but I cannot successfully integrate the result to get the marginal density of interest.  My intuition tells me that this is a normal distribution with different parameters, but I can't prove it.\n", "A": "Your intuition is correct - the marginal distribution of a normal random variable with a normal mean is indeed normal.  To see this, we first re-frame the joint distribution as a product of normal densities  by completing the square:\n$$\\begin{equation} \\begin{aligned}\nf(x,m) \n&= f(x|m) f(m) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( -\\frac{1}{2} \\Big[ \\Big( \\frac{x-m}{\\sigma} \\Big)^2 + \\Big( \\frac{m-\\theta}{s} \\Big)^2 \\Big] \\Big) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( -\\frac{1}{2} \\Big[ \\Big( \\frac{1}{\\sigma^2}+\\frac{1}{s^2} \\Big) m^2 -2 \\Big( \\frac{x}{\\sigma^2} + \\frac{\\theta}{s^2} \\Big) m + \\Big( \\frac{x^2}{\\sigma^2} + \\frac{\\theta^2}{s^2} \\Big) \\Big] \\Big) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( -\\frac{1}{2 \\sigma^2 s^2} \\Big[ (s^2+\\sigma^2) m^2 -2 (x s^2+ \\theta \\sigma^2) m + (x^2 s^2+ \\theta^2 \\sigma^2) \\Big] \\Big) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( - \\frac{s^2+\\sigma^2}{2 \\sigma^2 s^2} \\Big[ m^2 -2 \\cdot \\frac{x s^2 + \\theta \\sigma^2}{s^2+\\sigma^2} \\cdot m +  \\frac{x^2 s^2 + \\theta^2 \\sigma^2}{s^2+\\sigma^2} \\Big] \\Big) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( - \\frac{s^2+\\sigma^2}{2 \\sigma^2 s^2} \\Big( m - \\frac{x s^2 + \\theta \\sigma^2}{s^2+\\sigma^2} \\Big)^2 \\Big) \\\\[6pt]\n&\\quad \\quad \\quad \\text{ } \\times \\exp \\Big( \\frac{(x s^2 + \\theta \\sigma^2)^2}{2 \\sigma^2 s^2 (s^2+\\sigma^2)} - \\frac{x^2 s^2 + \\theta^2 \\sigma^2}{2 \\sigma^2 s^2} \\Big) \\\\[10pt]\n&= \\frac{1}{2\\pi \\sigma s} \\cdot \\exp \\Big( - \\frac{s^2+\\sigma^2}{2 \\sigma^2 s^2} \\Big( m - \\frac{x s^2 + \\theta \\sigma^2}{s^2+\\sigma^2} \\Big)^2 \\Big) \\cdot \\exp \\Big( -\\frac{1}{2} \\frac{(x-\\theta)^2}{s^2+\\sigma^2} \\Big) \\\\[10pt]\n&= \\sqrt{\\frac{s^2+\\sigma^2}{2\\pi \\sigma^2 s^2}} \\cdot \\exp \\Big( - \\frac{s^2+\\sigma^2}{2 \\sigma^2 s^2} \\Big( m - \\frac{x s^2 + \\theta \\sigma^2}{s^2+\\sigma^2} \\Big)^2 \\Big) \\\\[6pt]\n&\\quad \\times \\sqrt{\\frac{1}{2\\pi (s^2+\\sigma^2)}} \\cdot \\exp \\Big( -\\frac{1}{2} \\frac{(x-\\theta)^2}{s^2+\\sigma^2} \\Big) \\\\[10pt]\n&= \\text{N} \\Big( m \\Big| \\frac{xs^2+\\theta\\sigma^2}{s^2+\\sigma^2}, \\frac{s^2 \\sigma^2}{s^2+\\sigma^2} \\Big) \\cdot \\text{N}(x|\\theta, s^2+\\sigma^2).\n\\end{aligned} \\end{equation}$$\nWe then integrate out $m$ to obtain the marginal density $f(x) = \\text{N}(x|\\theta, s^2+\\sigma^2)$.  From this exercise we see that $X \\sim \\text{N}(\\theta, s^2+\\sigma^2)$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/372062", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "13", "answer_count": 5, "answer_id": 1}}
