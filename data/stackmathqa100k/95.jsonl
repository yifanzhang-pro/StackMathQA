{"Q": "$N(\\theta,\\theta)$: MLE for a Normal where mean=variance $\\newcommand{\\nd}{\\frac{n}{2}}$For an $n$-sample following a Normal$(\\mu=\\theta,\\sigma^2=\\theta)$, how do we find the mle?\nI can find the root of the score function\n$$\n\\theta=\\frac{1\\pm\\sqrt{1-4\\frac{s}{n}}}{2},s=\\sum x_i^2,\n$$\nbut I don't see which one is the maximum.\nI tried to substitute in the second derivative of the log-likelihood, without success.\nFor the likelihood, with $x=(x_1,x_2,\\ldots,x_n)$,\n$$\nf(x) = (2\\pi)^{-n/2} \\theta^{-n/2} \\exp\\left( -\\frac{1}{2\\theta}\\sum(x_i-\\theta)^2\\right), \n$$\nthen, with $s=\\sum x_i^2$ and $t=\\sum x_i$,\n$$\n\\ln f(x) = -\\nd \\ln(2\\pi) -\\nd\\ln\\theta-\\frac{s}{2\\theta}-t+\\nd\\theta,\n$$\nso that\n$$\n\\partial_\\theta \\ln f(x) = -\\nd\\frac{1}{\\theta}+\\frac{s}{2\\theta^2}+\\nd, \n$$\nand the roots are given by\n$$\n\\theta^2-\\theta+\\frac{s}{n}=0.\n$$\nAlso,\n$$\n\\partial_{\\theta,\\theta} \\ln f(x) = \\nd \\frac{1}{\\theta^2} - \\frac{s}{\\theta^3}.\n$$\n", "A": "Recall that the normal distribution  $N(\\mu, \\sigma^2)$ has pdf $f(x\\mid \\mu ,\\sigma ^{2})={\\frac {1}{{\\sqrt {2\\pi \\sigma ^{2}}}\\ }}\\exp {\\left(-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}\\right)},$ Note here that $\\mu = \\theta$ and $\\sigma^2 = \\theta$ and therefore $\\sigma = \\sqrt{\\theta}$\n\\begin{aligned}\n     L(x_1,x_2,...,x_n | \\theta) &= \\prod_{i=1}^n f(x_i | \\theta)\n     \\\\\n     &=  \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\theta}}  \\ \\exp \\Big \\{ - \\frac{1}{2 \\theta} (x_i - \\theta)^2 \\Big\\}\n     \\\\\n     & = (2 \\pi)^{-n/2}  (\\theta)^{-n /2}   \\prod_{i=1}^n  \\ \\exp \\Big \\{ - \\frac{1}{2 \\theta} (x_i - \\theta)^2 \\Big\\}\n     \\\\\n     & =  (2 \\pi)^{-n/2}  (\\theta)^{-n /2}  \\ \\exp \\Big \\{ - \\frac{1}{2 \\theta} \\sum_{i=1}^n (x_i - \\theta)^2 \\Big\\}\n     \\\\\n     \\log L& = - \\frac{n}{2} \\log(2\\pi) -  \\frac{n}{2} \\log(\\theta) - \\frac{1}{2 \\theta}  \\sum_{i=1}^n (x_i - \\theta)^2\n\\end{aligned}\nConsider the term $\\frac{1}{2 \\theta}  \\sum_{i=1}^n (x_i - \\theta)^2$ which can be expanded and simplified\n\\begin{aligned}\n     \\frac{1}{2 \\theta}  \\sum_{i=1}^n (x_i - \\theta)^2 & =  \\frac{1}{2 \\theta}  \\sum_{i=1}^n (x_i - \\theta)(x_i - \\theta)\n     \\\\\n     & = \\frac{1}{2 \\theta} \\sum_{i=1}^n \\left( x_i^2 - 2 \\theta x_i + \\theta^2   \\right)\n     \\\\\n     & =  \\frac{1}{2 \\theta}  \\left( \\sum_{i=1}^n (x_i^2) - 2 \\theta \\sum_{i=1}^n (x_i) + n\\theta^2   \\right)\n     \\\\\n     & = \\frac{1}{2 \\theta} \\sum_{i=1}^n (x_i^2) -  \\sum_{i=1} (x_i) + \\frac{n\\theta}{2}\n\\end{aligned}\nWe can now compute the derivative with respect to $\\theta$, equate to zero and solve for $\\theta$\n\\begin{aligned}\n \\log L& = - \\frac{n}{2} \\log(2\\pi) -  \\frac{n}{2} \\log(\\theta) - \\left(  \\frac{1}{2 \\theta} \\sum_{i=1}^n (x_i^2) -  \\sum_{i=1} (x_i) + \\frac{n\\theta}{2} \\right)\n  \\\\\n \\frac{d}{d\\theta} \\log L & = \\frac{-n}{2\\theta} - \\left( \\frac{-1}{2\\theta^2}  \\sum_{i=1}^n (x_i^2) + \\frac{n}{2} \\right) = 0\n \\\\\n & = \\frac{-n}{2\\theta} + \\frac{1}{2\\theta^2}  \\sum_{i=1}^n (x_i^2) - \\frac{n}{2}\n \\\\\n & = - \\theta^2 - \\theta + \\frac{1}{n} \\sum_{i=1}^n (x_i^2)\n \\\\\n &\\text{let $s = \\frac{1}{n} \\sum_{i=1}^n (x_i^2)$}\n \\\\\n 0 & = - \\theta^2 - \\theta + s\n \\\\\n \\hat \\theta &= \\frac{\\sqrt{1 + 4s} -1 }{2}\n\\end{aligned}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/56295", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "10", "answer_count": 3, "answer_id": 1}}
{"Q": "Prove that sample covariance matrix is positive definite Consider the $p \\times p$ sample covariance matrix:\n$$\\mathbf{S} = \\frac{1}{n-1} \\cdot \\mathbf{Y}_\\mathbf{c}^\\text{T} \\mathbf{Y}_\\mathbf{c}\n\\quad \\quad \\quad\n\\mathbf{Y}_\\mathbf{c} = \\mathbf{C} \\mathbf{Y},$$\nwhere $\\mathbf{C} = \\mathbf{I}-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\text{T}$ is the $n \\times n$ centering matrix and\n$\\mathbf{Y}$ is an $n \\times p$ matrix.  How can it be proved that if the variables are continuos, not linearly related and $n-1> p$ then the sample covariance matrix is \u200b\u200bpositive definite?\nThe following clue is found in Ranchera's book:\n\n", "A": "First, let's simplify the equation for your sample covariance matrix.  Using the fact that the centering matrix is symmetric and idempotent you get the $p \\times p$ form:\n$$\\begin{align}\n\\mathbf{S} \n&= \\frac{1}{n-1} \\cdot \\mathbf{Y}_\\mathbf{c}^\\text{T} \\mathbf{Y}_\\mathbf{c} \\\\[6pt]\n&= \\frac{1}{n-1} \\cdot (\\mathbf{C} \\mathbf{Y})^\\text{T} (\\mathbf{C} \\mathbf{Y}) \\\\[6pt]\n&= \\frac{1}{n-1} \\cdot \\mathbf{Y}^\\text{T} \\mathbf{C}^\\text{T} \\mathbf{C} \\mathbf{Y} \\\\[6pt]\n&= \\frac{1}{n-1} \\cdot \\mathbf{Y}^\\text{T} \\mathbf{C} \\mathbf{Y}. \\\\[6pt]\n\\end{align}$$\nThis is a simple quadratic form in $\\mathbf{Y}$.  I will show that this matrix is non-negative definite (or \"positive semi-definite\" if you prefer) but it is not always positive definite.  To do this, consider an arbitrary non-zero column vector $\\mathbf{z} \\in \\mathbb{R}^p - \\{ \\mathbf{0} \\}$ and let $\\mathbf{a} = \\mathbf{Y} \\mathbf{z} \\in \\mathbb{R}^n$ be the resulting column vector.  Since the centering matrix is non-negative definite (it has one eigenvalue equal to zero and the rest are equal to one) you have:\n$$\\begin{align}\n\\mathbf{z}^\\text{T} \\mathbf{S} \\mathbf{z}\n&= \\frac{1}{n-1} \\cdot \\mathbf{z}^\\text{T} \\mathbf{Y}^\\text{T} \\mathbf{C} \\mathbf{Y} \\mathbf{z} \\\\[6pt]\n&= \\frac{1}{n-1} \\cdot (\\mathbf{Y} \\mathbf{z})^\\text{T} \\mathbf{C} \\mathbf{Y} \\mathbf{z} \\\\[6pt]\n&= \\frac{1}{n-1} \\cdot \\mathbf{a}^\\text{T} \\mathbf{C} \\mathbf{a} \n\\geqslant 0. \\\\[6pt]\n\\end{align}$$\nThis shows that $\\mathbf{S}$ is non-negative definite.  However, it is not always positive definite.  To see this, take any $\\mathbf{z} \\neq \\mathbf{0}$ giving $\\mathbf{a} = \\mathbf{Y} \\mathbf{z} \\propto \\mathbf{1}$ and substitute into the quadratic form to get $\\mathbf{z}^\\text{T} \\mathbf{S} \\mathbf{z} = 0$.\n\nUpdate: This update is based on the additional information you have added in your edit to the question and your comments.  In order to get a positive definite sample variance matrix you need $\\mathbf{a}^\\text{T} \\mathbf{C} \\mathbf{a} \n> 0$.  If $n-1>p$ and all $n$ rows of $\\mathbf{Y}$ are linearly independent then $\\mathbf{Y} \\mathbf{z} \\propto \\mathbf{1}$ implies $\\mathbf{z} = \\mathbf{0}$.  The contrapositive implication is that $\\mathbf{a}^\\text{T} \\mathbf{C} \\mathbf{a} \n> 0$ for all $\\mathbf{z} \\neq 0$, which establishes that the sample covariance matrix is positive definite.  Presumably this is what you are looking for.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/487510", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Expectation of bootstrap variance estimators I'm quite unclear how they derived (5) from (4). Since $\\bar{X}$ and $s^2$ are unbiased estimators, I believe my lack of follow through comes from not knowing how to compute $\\mathbb{E}[\\bar{X}^2 s^2]$. Any help is appreciated.\n\n", "A": "$\\bullet$ Since the data is normally distributed, $\\bar X$ and $X_i-\\bar X$ and thus $s^2$ are independently distributed.\n$\\bullet$ The variance of $s^2$ is\n$$\\operatorname{Var}[s^2] = \\frac{2\\sigma^4}{(n-1) }.\\tag 1\\label 1$$\nNow\n\\begin{align}\\frac{4}n \\mathbb E\\left[\\bar X^2s^2\\right]+ \\frac{2}{n^2}\\mathbb E\\left[s^4\\right]&=\\frac{4}n \\mathbb E\\left[\\bar X^2\\right]\\mathbb E\\left[s^2\\right]+ \\frac{2}{n^2}\\mathbb E\\left[s^4\\right]\\\\ &=\\frac{4}n \\mathbb E\\left[\\operatorname{Var}\\left[\\bar X\\right]+ \\left(\\mathbb E[\\bar X]\\right)^2\\right]\\mathbb E\\left[s^2\\right]+  \\frac{2}{n^2}\\mathbb E\\left[\\operatorname{Var}\\left[s^2\\right]+ \\left(\\mathbb E\\left[s^2\\right]\\right)^2\\right] \\\\ &= \\frac{4}n \\left[\\frac{\\sigma^2}n +\\mu^2\\right]\\sigma^2+ \\frac{2}{n^2}\\left[\\frac{2\\sigma^4}{n-1}+\\sigma^4\\right]~~\\textrm{using}~~\\eqref{1}\\\\ &= \\frac{4\\sigma^4}{n^2} +\\frac{4\\mu^2\\sigma^2}{n}+ \\frac{2}{n^2}\\left[\\frac{2\\sigma^4+n\\sigma^4-\\sigma^4}{n-1}\\right]\\\\ &=  \\frac{4\\sigma^4}{n^2} +\\frac{4\\mu^2\\sigma^2}{n}+ \\frac{2}{n^2}\\frac{(n+1)\\sigma^4}{n-1}.\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/593166", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
