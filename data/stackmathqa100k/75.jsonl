{"Q": "How to get the $INR(x_i)$ in PCA, the relative contribution of $x_i$ to the total inertia? Given the following data array :\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\nJ/I&1 & 2 & 3 & 4 & 5 & 6\\\\\n\\hline\nx & 1 & 0 & 0 & 2 & 1 & 2\\\\\ny & 0 & 0 & 1 & 2 & 0 & 3\\\\\nz & 0 & 1 & 2 & 1 & 0 & 2\\\\\n\\hline\n\\end{array}$$\nI can get the following values for the centered data $Y$ along with the variance\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n&1 & 2 & 3 & 4 & 5 & 6 & v_2 & v_3\\\\\n\\hline\n\\mbox{x }&1 & 0 & 0 & 2 & 1 & 2 & 1/\\sqrt{6} & 1/\\sqrt{2}\\\\\n\\mbox{y }&0 & 0 & 1 & 2 & 0 & 3 & 2/\\sqrt{6} & 0\\\\\n\\mbox{z }&0 & 1 & 2 & 1 & 0 & 2 & 1/\\sqrt{6} & -1/\\sqrt{2}\\\\\n\\hline\n\\end{array}$$\nFrom there I can get the principal components value :\n\\begin{align}\nVv_2&=\n\\begin{pmatrix}4 & 4 & 0\\\\\n4 & 8 & 4\\\\\n0 & 4 & 4\\end{pmatrix}\n\\frac{1}{\\sqrt 6}\\begin{pmatrix}\n1\\\\2\\\\1\n\\end{pmatrix}\\\\\n&=\\frac{2}{\\sqrt 6}\n\\begin{pmatrix}\n1\\\\2\\\\1\n\\end{pmatrix}\n\\end{align}\n$PC_1$ value is therefore $2$. \n\\begin{align}\nVv_3&=\n\\begin{pmatrix}4 & 4 & 0\\\\\n4 & 8 & 4\\\\\n0 & 4 & 4\\end{pmatrix}\n\\frac{1}{\\sqrt 2}\\begin{pmatrix}\n1\\\\0\\\\-1\n\\end{pmatrix}\\\\\n&=\\frac{2}{3\\sqrt 2}\n\\begin{pmatrix}\n1\\\\0\\\\-1\n\\end{pmatrix}\n\\end{align}\n$PC_1$ value is therefore $\\frac{2}{3}$\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n&1 & 2 & 3 & 4 & 5 & 6 & v_2 & v_3\\\\\n\\hline\n\\mbox{value $PC_1$ }&&&&&&&2&\\\\\n\\mbox{Value $PC_2$ }&&&&&&&&2/3\\\\\n\\mbox{value coef INR }&&&&&&&&\\\\\n\\mbox{value coef CTR }&&&&&&&&\\\\\n\\mbox{value coef COR }&&&&&&&&\\\\\n\\hline\n\\end{array}$$\nHow to get the INR (I think it's a French acronym) the contribution of an individual $x_i$ to the total inertia $I_T$:\n$$INR(i)=\\frac{p_id(0,y_i)^2}{I_T}$$\nWith $d$ being usually the euclidean distance. We can deduce from the definition that $\\sum INR(i)=1$.\n\\begin{align}\nINR(1) &= \\frac{1}{6}\\times\\frac{(-1)\u00b2+1\u00b2}{2+\\frac{2}{3}} = \\frac{1}{4}\\\\\nINR(2) &= \\frac{1}{6}\\times\\frac{(-1)\u00b2+(-1)\u00b2}{2+\\frac{2}{3}} = \\frac{1}{4}\\\\\nINR(3) &= \\frac{1}{6}\\times\\frac{1\u00b2+1\u00b2}{2+\\frac{2}{3}} = \\frac{1}{4}\\\\\nINR(4) &= \\frac{1}{6}\\times\\frac{1\u00b2+2\u00b2+1\u00b2}{2+\\frac{2}{3}} = \\frac{2}{3}???\\\\\n\\end{align}\nIt would be much more than $1$ now.\n", "A": "If I'm reading your question correctly, you're looking for inertia around an arbitrary point in your data cloud. The can be formulated as: $I_g-||\\bar x-a||^2$.  where.  $I_g$ here is total inertia, $a$ is particular point in question.  \nHere's a source on that: with derivation included. Page 8-9.\nhttps://cedric.cnam.fr/fichiers/art_1827.pdf\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/277183", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Find the UMVUE of $\\frac{\\mu^2}{\\sigma}$ where $X_i\\sim\\mathsf N(\\mu,\\sigma^2)$ \nSuppose $X_1, ..., X_4$ are i.i.d $\\mathsf N(\\mu, \\sigma^2)$\n  random variables. Give the UMVUE of $\\frac{\\mu^2}{\\sigma}$ expressed in terms of $\\bar{X}$, $S$, integers, and $\\pi$. \n\nHere is a relevant question.\nI first note that if $X_1,...,X_n$ are i.i.d $\\mathsf N(\\mu,\\sigma^2)$ random variables having pdf\n$$\\begin{align*}\nf(x\\mid\\mu,\\sigma^2)\n&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\\\\\\\\n&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\mu^2}{2\\sigma^2}}\\text{exp}\\left(-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x\\right)\n\\end{align*}$$\nwhere $\\mu\\in\\mathbb{R}$ and $\\sigma^2\\gt0$, then\n$$T(\\vec{X})=\\left(\\sum_{i=1}^n X_i^2, \\sum_{i=1}^n X_i\\right)$$\nare sufficient statistics and are also complete since $$\\{\\left(-\\frac{1}{2\\sigma^2},\\frac{\\mu}{\\sigma^2}\\right):\\mu\\in\\mathbb{R}, \\sigma^2\\gt0\\}=(-\\infty,0)\\times(-\\infty,\\infty)$$\ncontains an open set in $\\mathbb{R}^2$\nI also note that the sample mean and sample variance are stochastically independent and so letting\n$$\\overline{X^2}=\\frac{1}{n}\\sum_{i=1}^n X_i^2$$\n$$\\overline{X}^2=\\frac{1}{n}\\sum_{i=1}^n X_i$$\nwe have\n$$\\mathsf E\\left(\\frac{\\overline{X^2}}{S}\\right)=\\mathsf E\\left(\\overline{X^2}\\right)\\cdot\\mathsf E\\left(\\frac{1}{S}\\right)=\\overline{X^2}\\cdot\\mathsf E\\left(\\frac{1}{S}\\right)$$\nIt remains only to find $\\mathsf E\\left(\\frac{1}{S}\\right)$\nWe know that $$(n-1)\\frac{S^2}{\\sigma^2}\\sim\\chi_{n-1}^2$$\nHence\n$$\\begin{align*}\n\\mathsf E\\left(\\frac{\\sigma}{S\\sqrt{3}}\\right)\n&=\\int_0^{\\infty} \\frac{1}{\\sqrt{x}} \\cdot\\frac{1}{\\Gamma(1.5)2^{1.5}}\\cdot\\sqrt{x}\\cdot e^{-x/2}dx\\\\\\\\\n&=\\frac{4}{\\sqrt{\\pi}\\cdot2^{1.5}}\n\\end{align*}$$\nSo $$\\mathsf E\\left(\\frac{1}{S}\\right)=\\frac{4\\sqrt{3}}{\\sqrt{\\pi}\\cdot 2^{1.5}\\cdot \\sigma}$$\nBut since $\\mathsf E(S)\\neq\\sigma$ I don't think I can just plug in $S$ for $\\sigma$ here.\nI have that since $\\mathsf E\\left(\\overline{X^2}\\right)=\\mathsf{Var}\\left(\\overline{X}\\right)+\\mathsf E\\left(\\bar{X}\\right)^2=\\frac{\\sigma^2}{4}+\\mu^2$\nHence\n$$\\sigma=\\sqrt{4\\left(E\\left(\\overline{X^2}\\right)-E\\left(\\overline{X}\\right)^2\\right)}=\\sqrt{4\\left(\\overline{X^2}-\\overline{X}^2\\right)}$$\nHence the UMVUE of $\\frac{\\mu^2}{\\sigma}$ is\n$$\\frac{4\\sqrt{3}\\cdot\\overline{X^2}}{\\sqrt{\\pi}\\cdot 2^{1.5}\\cdot \\sqrt{4\\left(\\overline{X^2}-\\overline{X}^2\\right)}}=\\frac{\\sqrt{\\frac{3}{2\\pi}}\\left(\\frac{S^2}{4}+\\bar{X}^2\\right)}{\\sqrt{\\frac{S^2}{4}}}$$\nIs this a valid solution?\n", "A": "I have skipped some details in the following calculations and would ask you to verify them.\nAs usual, we have the statistics $$\\overline X=\\frac{1}{4}\\sum_{i=1}^4 X_i\\qquad,\\qquad S^2=\\frac{1}{3}\\sum_{i=1}^4(X_i-\\overline X)^2$$\nAssuming both $\\mu$ and $\\sigma$ are unknown, we know that $(\\overline X,S^2)$ is a complete sufficient statistic for $(\\mu,\\sigma^2)$. We also know that $\\overline X$ and $S$ are independently distributed. \nAs you say,\n\\begin{align}\n E\\left(\\overline X^2\\right)&=\\operatorname{Var}(\\overline X)+\\left(E(\\overline X)\\right)^2\n\\\\&=\\frac{\\sigma^2}{4}+\\mu^2\n\\end{align}\nSince we are estimating $\\mu^2/\\sigma$, it is reasonable to assume that a part of our UMVUE is of the form $\\overline X^2/S$. And for evaluating $E\\left(\\frac{\\overline X^2}{S}\\right)=E(\\overline X^2)E\\left(\\frac{1}{S}\\right)$, we have\n\\begin{align}\nE\\left(\\frac{1}{S}\\right)&=\\frac{\\sqrt{3}}{\\sigma}\\, E\\left(\\sqrt\\frac{\\sigma^2}{3\\,S^2}\\right)\n\\\\\\\\&=\\frac{\\sqrt{3}}{\\sigma}\\, E\\left(\\frac{1}{\\sqrt Z}\\right)\\qquad\\qquad,\\,\\text{ where }Z\\sim\\chi^2_{3}\n\\\\\\\\&=\\frac{\\sqrt{3}}{\\sigma}\\int_0^\\infty \\frac{1}{\\sqrt z}\\,\\frac{e^{-z/2}z^{3/2-1}}{2^{3/2}\\,\\Gamma(3/2)}\\,dz\n\\\\\\\\&=\\frac{1}{\\sigma}\\sqrt\\frac{3}{2\\pi}\\int_0^\\infty e^{-z/2}\\,dz\n\\\\\\\\&=\\frac{1}{\\sigma}\\sqrt\\frac{6}{\\pi}\n\\end{align}\nAgain, for an unbiased estimator of $\\sigma$, $$E\\left(\\frac{1}{2}\\sqrt\\frac{3\\pi}{2}S\\right)=\\sigma$$\nSo,\n\\begin{align}\nE\\left(\\frac{\\overline X^2}{S}\\right)&=E\\left(\\overline X^2\\right)E\\left(\\frac{1}{S}\\right)\n\\\\&=\\left(\\mu^2+\\frac{\\sigma^2}{4}\\right)\\frac{1}{\\sigma}\\sqrt\\frac{6}{\\pi}\n\\\\&=\\sqrt\\frac{6}{\\pi}\\left(\\frac{\\mu^2}{\\sigma}+\\frac{\\sigma}{4}\\right)\n\\end{align}\nOr, $$E\\left(\\sqrt{\\frac{\\pi}{6}}\\,\\frac{\\overline X^2}{S}-\\frac{\\frac{1}{2}\\sqrt\\frac{3\\pi}{2}S}{4}\\right)=\\frac{\\mu^2}{\\sigma}$$\nHence our unbiased estimator based on the complete sufficient statistic $(\\overline X,S^2)$ is \n\\begin{align}\nT(X_1,X_2,X_3,X_4)&=\\sqrt{\\frac{\\pi}{6}}\\,\\frac{\\overline X^2}{S}-\\frac{1}{8}\\sqrt\\frac{3\\pi}{2}S\n\\end{align}\nBy Lehmann-Scheffe, $T$ is the UMVUE of $\\mu^2/\\sigma$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/373936", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Density of sum of truncated normal and normal distribution Suppose that $\\varepsilon\\sim N(0, \\sigma_\\varepsilon)$ and $\\delta\\sim N^+(0, \\sigma_\\delta)$. What is the density function for $X = \\varepsilon - \\delta$?\nThis proof apparently appeared in a Query by M.A. Weinstein in Technometrics 6 in 1964, which stated that the density of $X$ is given by\n$$f_X(x) = \\frac{2}{\\sigma} \\phi\\left(\\frac{x}{\\sigma}\\right) \\left(1 - \\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right)\\right),$$\nwhere $\\sigma^2 = \\sigma_\\varepsilon^2 + \\sigma_\\delta^2$ and $\\lambda = \\sigma_\\delta / \\sigma_\\varepsilon$ and $\\phi$ and $\\Phi$ are the standard normal density and distribution functions, respectively. However, that paper is very difficult to find online. What is the proof that the density of $X$ takes the above form?\n", "A": "Ultimately I needed to work through the algebra a bit more to arrive at the specified form. For posterity, the full proof is given below.\nProof\nFirst consider the distribution function of $X$, which is given by\n    $$F(x) = \\Pr(X \\leq x) = \\Pr(\\varepsilon - \\delta \\leq x)$$\n$$= \\int_{\\varepsilon - \\delta \\leq x} f_\\varepsilon(\\varepsilon) f_\\delta(\\delta) d\\delta d\\varepsilon$$\n$$= \\int_{\\delta\\in\\mathbb{R}^+} f_\\delta(\\delta) \\int_{\\varepsilon\\in (-\\infty, x + \\delta]} f_\\varepsilon(\\varepsilon) d\\varepsilon d\\delta.$$\n    Substituting in known density functions yields\n    $$\\int_0^\\infty 2\\phi(\\delta | 0, \\sigma_\\delta) \\int_{-\\infty}^{x + \\delta} \\phi(\\varepsilon | 0, \\sigma_\\varepsilon) d\\varepsilon d\\delta$$\n$$= 2\\int_0^\\infty \\phi(\\delta | 0, \\sigma_\\delta) \\Phi(x + \\delta | 0, \\sigma_\\varepsilon) d\\delta.$$\n    The density of $X$ is then given by\n    $$f(x) = \\frac{dF}{dx} = 2\\int_0^\\infty \\phi(\\delta | 0, \\sigma_\\delta) \\phi(x + \\delta | 0, \\sigma_\\varepsilon) d\\delta.$$\n    Using Sage to perform this integration, the result is given by\n    $$f(x) = -\\frac{{\\left(\\operatorname{erf}\\left(\\frac{\\sigma_{\\delta} x}{2 \\, \\sqrt{\\frac{1}{2} \\, \\sigma_{\\delta}^{2} + \\frac{1}{2} \\, \\sigma_{\\varepsilon}^{2}} \\sigma_{\\varepsilon}}\\right) e^{\\left(\\frac{\\sigma_{\\delta}^{2} x^{2}}{2 \\, {\\left(\\sigma_{\\delta}^{2} \\sigma_{\\varepsilon}^{2} + \\sigma_{\\varepsilon}^{4}\\right)}}\\right)} - e^{\\left(\\frac{\\sigma_{\\delta}^{2} x^{2}}{2 \\, {\\left(\\sigma_{\\delta}^{2} \\sigma_{\\varepsilon}^{2} + \\sigma_{\\varepsilon}^{4}\\right)}}\\right)}\\right)} e^{\\left(-\\frac{x^{2}}{2 \\, \\sigma_{\\varepsilon}^{2}}\\right)}}{2 \\, \\sqrt{\\pi} \\sqrt{\\frac{1}{2} \\, \\sigma_{\\delta}^{2} + \\frac{1}{2} \\, \\sigma_{\\varepsilon}^{2}}}.$$\n    Defining $\\lambda = \\sigma_\\delta / \\sigma_\\varepsilon$ and $\\sigma^2 = \\sigma_\\varepsilon^2 + \\sigma_\\delta^2$, the following can be simplified:\n    $$\\frac{\\sigma_{\\delta} x}{2 \\, \\sqrt{\\frac{1}{2} \\, \\sigma_{\\delta}^{2} + \\frac{1}{2} \\, \\sigma_{\\varepsilon}^{2}} \\sigma_{\\varepsilon}} = \\frac{\\lambda x}{\\sigma\\sqrt{2}} = \\frac{x}{(\\sigma / \\lambda) \\sqrt{2}},$$\n$$\\frac{\\sigma_{\\delta}^{2} x^{2}}{2 \\, {\\left(\\sigma_{\\delta}^{2} \\sigma_{\\varepsilon}^{2} + \\sigma_{\\varepsilon}^{4}\\right)}} = \\frac{\\lambda^2 x^2}{2\\sigma^2} = \\frac{x^2}{2(\\sigma / \\lambda)^2}.$$\n    Thus,\n    $$f(x) = -\\frac{\\exp\\left(\\frac{x^2}{2(\\sigma / \\lambda)^2}\\right)\\left(\\operatorname{erf}\\left(\\frac{x}{(\\sigma / \\lambda) \\sqrt{2}}\\right) - 1\\right)\\exp\\left(-\\frac{x^2}{2\\sigma_\\varepsilon^2}\\right)}{\\sigma\\sqrt{2\\pi}}$$\n$$= -\\frac{\\left(\\operatorname{erf}\\left(\\frac{x}{(\\sigma / \\lambda) \\sqrt{2}}\\right) - 1\\right) \\exp\\left(-x^2\\left(\\frac{1}{2\\sigma_\\varepsilon^2} - \\frac{1}{2(\\sigma / \\lambda)^2}\\right)\\right)}{\\sigma\\sqrt{2\\pi}}.$$\n    Now,\n    $$\\operatorname{erf}\\left(\\frac{x}{(\\sigma / \\lambda) \\sqrt{2}}\\right) - 1 = \\left(1 + \\operatorname{erf}\\left(\\frac{x}{(\\sigma / \\lambda) \\sqrt{2}}\\right)\\right) - 2 = 2\\left(\\frac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\frac{x}{(\\sigma / \\lambda) \\sqrt{2}}\\right)\\right) - 1\\right)$$\n$$= 2\\left(\\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right) - 1\\right) = -2\\left(1 - \\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right)\\right).$$\n    Also,\n    $$\\frac{1}{2\\sigma_\\varepsilon^2} - \\frac{1}{2(\\sigma / \\lambda)^2} = \\frac{1}{2\\sigma_\\varepsilon^2} - \\frac{\\sigma_\\delta^2}{2\\sigma_\\varepsilon^2(\\sigma_\\delta^2 + \\sigma_\\varepsilon^2)} = \\frac{\\sigma_\\delta^2 + \\sigma_\\varepsilon^2 - \\sigma_\\delta^2}{2\\sigma_\\varepsilon^2(\\sigma_\\delta^2 + \\sigma_\\varepsilon^2)} = \\frac{\\sigma_\\varepsilon^2}{2\\sigma_\\varepsilon^2(\\sigma_\\delta^2 + \\sigma_\\varepsilon^2)} = \\frac{1}{2\\sigma^2}.$$\n    So,\n    $$f(x) = 2\\left(1 - \\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right)\\right)\\frac{\\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)}{\\sigma\\sqrt{2\\pi}} = 2\\left(1 - \\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right)\\right) \\phi(x | 0, \\sigma) = \\frac{2}{\\sigma}\\phi\\left(\\frac{x}{\\sigma}\\right) \\left(1 - \\Phi\\left(\\frac{x\\lambda}{\\sigma}\\right)\\right).$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/419722", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 1}}
{"Q": "An inequality for a bi-modal hypergeometric distribution Say $X$ has a hypergeometric distribution with parameters $m$, $n$ and $k$, with $k\\leq n<\\frac12m$.\nI know that $X$ has a dual mode if and only if $d=\\frac{(k+1)(n+1)}{m+2}$ is integer. In that case $P(X=d)=P(X=d-1)$ equals the maximum probability.\nI am wondering if I can say anything about $P(X=d+1)$ versus $P(X=d-2)$ then. When is the former higher than the latter? I.e. when is:\n$P(X=d+1)>P(X=d-2)$\nAlways? I tried many combinations programmatically and did not find any counterexample.\nSo far I have found:\n$\\frac{P(X=d+1)}{P(X=d-2)}=\\frac{(k-d+2)(k-d+1)(k-d)(n-d+2)(n-d+1)(n-d)}{(d+1)d(d-1)(m-k-n+d+1)(m-k-n+d)(m-k-n+d-1)}$\nBecause $d=\\frac{(k+1)(n+1)}{m+2}$, this can be simplified to:\n$\\frac{P(X=d+1)}{P(X=d-2)}=\\frac{(k-d+2)(k-d)(n-d+2)(n-d)}{(d+1)(d-1)(m-k-n+d+1)(m-k-n+d-1)}$\nI have tried further combining this with $d=\\frac{(k+1)(n+1)}{m+2}$ being integer, but that gets quite complex and gives me no further clue.\nI feel there is something relatively easy to prove here...?\nFor $n=\\frac12m$, $P(X=d+1)=P(X=d-2)$ due to symmetry.\n", "A": "In the case you are considering you have $P(X=d)=P(X=d-1)$ \nso  let's consider the sign of $$\\frac{P(X=d+1)}{P(X=d)}-\\frac{P(X=d-2)}{P(X=d-1)} = \\tfrac{(k-d)(n-d)}{(d+1) (m-k-n+d+1)} -\\tfrac{ (d-1) (m-k-n+d-1)}{(k-d+2)(n-d+2)} \\\\= \\tfrac{(k-d)(n-d)(k-d+2)(n-d+2)-(d-1) (m-k-n+d-1)(d+1) (m-k-n+d+1)}{(d+1) (m-k-n+d+1)(k-d+2)(n-d+2)}$$\nThe denominator is positive so does not affect the sign.  In the numerator, we can make the substitution $d=\\frac{(k+1)(n+1)}{m+2}$ and then multiply through by the positive $(m+2)^4$. Expanding the result and factorising gives  \n$$\\tfrac{m^6 +(8-2n-2k)m^5 +(24-16n-16k+kn)m^4 +(32-48n-48k+32kn)m^3 +(16-64n-64k+96kn)m^2 +(-32n-32k+128kn)m +64kn}{\\text{something positive}}  \\\\ = \\frac{(m+2)^4(m-2n)(m-2k)}{\\text{something positive}} $$ \nand this is positive, i.e. $P(X=d+1) > P(X=d-2)$, when $k\\leq n<\\frac12m$. $\\blacksquare$\nAs a check, the difference is actually $\\frac{(m-2n)(m-2k)}{(d+1) (m-k-n+d+1)(k-d+2)(n-d+2)}$. \nIt is also positive when $n\\lt k<\\frac12m$, and when both $k > \\frac12m$ and $n>\\frac12m$.\nIt is negative , i.e. $P(X=d+1) < P(X=d-2)$ when $k\\lt \\frac12m <n$ or $n\\lt \\frac12m <k$.\nFinally, it is zero, i.e. $P(X=d+1) = P(X=d-2)$ when $k= \\frac12m$ or $n= \\frac12m$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/458289", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
