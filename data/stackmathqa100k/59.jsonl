{"Q": "Equality of the sum of powers Hi everyone, I got a problem when proving lemmas for some combinatorial problems,\nand it is a question about integers.\nLet \n$\\sum_{k=1}^m a_k^t = \\sum_{k=1}^n b_k^t$ \nbe an equation,\nwhere $m, n, t, a_i, b_i$ are positive integers, and\n$a_i \\neq a_j$ for all $i, j$,\n$b_i \\neq b_j$ for all $i, j$,\n$a_i \\neq b_j$ for all $i, j$.\n\nDoes the equality have no solutions?\n\nFor $n \\neq m$, it is easy to find solutions for $t=2$ by Pythagorean theorem,\nand even for $n = m$, we have solutions like \n$1^2 + 4^2 + 6^2 + 7^2 = 2^2 + 3^2 + 5^2 + 8^2$.\nFor $t > 2$, similar equalities hold:\n$1^2 + 4^2 + 6^2 + 7^2 + 10^2 + 11^2 + 13^2 + 16^2 = 2^2 + 3^2 + 5^2 + 8^2 + 9^2 + 12^2 + 14^2 + 15^2$\nand \n$1^3 + 4^3 + 6^3 + 7^3 + 10^3 + 11^3 + 13^3 + 16^3 = 2^3 + 3^3 + 5^3 + 8^3 + 9^3 + 12^3 + 14^3 + 15^3$,\nand we can extend this trick to all $t > 2$.\n\nThe question is, if we introduce one more restriction, that is,\n  $|a_i - a_j| \\geq 2$ and $|b_i - b_j| \\geq 2$ for all $i, j$,\n  is it still possible to find solutions for the equation?\n\nFor $t = 2$ we can combine two Pythagorean triples, say,\n$5^2 + 12^2 + 25^2 = 7^2 + 13^2 + 24^2$,\nbut how about the cases for $t > 2$ and $n = m$?\n", "A": "One set of solutions for t = 3 is the class of numbers known as Taxicab Numbers, named after the number of a taxicab G. H. Hardy took, 1729, that Ramanujan mentioned was equal to 13 + 123 = 93 + 103.  This particular example fails, as |10 - 9| = 1 < 2, but there are other Taxicab numbers, such as:\n1673 + 4363 = 2283 + 4233 = 2553 + 4143.\nThis might be a helpful site for your question.\n-Gabriel Benamy\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/16764", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "9", "answer_count": 3, "answer_id": 1}}
{"Q": "'Sign matrices'-(-1,+1) square matrices My question arises from a discussion on an answer given by Maurizio Monge here.I do not know if there is a known terminology for such matrices. By \"sign matrices,\" I mean square matrices whose entries are in  ${-1,+1}$. \nFor instance, \n$\\begin{bmatrix}\n1 &-1 \\\\ \n -1& -1\n\\end{bmatrix}$ ,\n $\\begin{bmatrix}\n -1&1&1 \\\\ \n 1&1&-1 \\\\ \n -1&-1&-1 \n\\end{bmatrix}$\nClearly, there are  $2^{n^2}$ sign matrices of size $n\\times n$. So, we start their theory by enumerating them as follows. For a matrix of size $n\\times n$ we consider a truth table of $n^2$ arguments and therefore $2^{n^2}$ rows. Each row corresponds to the entries in one matrix$(a_{11},a_{12},\\dots,a_{1n},a_{21},a_{22},\\dots,a_{nn})$.\nLet $M_{(n,k)}$ be the $n \\times n$ sign matrix corresponding  to the $k^th$ row of the truth table. \nQuestion: Does the following matrix product give the zero matrix for sign matrices of even size?\n$\\prod_{k=1}^{2^{n^2}}M_{(n,k)}$\nThank you. As usual, I will be delighted if you point me to good references on this.\n", "A": "It is not part of the question but I consider it useful to give the result for the $M_3$ case( which I found using a $C$++ program). There are 512 rows and the matrix below is $M_3$. However, I am not completely certain since the compiler I used might have rounded off the numbers while multiplying the matrices though the data type used was long double,the most precise by far.\n\\begin{matrix}\n-1 & 1 & -1 & 1 & -1 & -1 & -1 & -1 & 1\\\\  \n-1 & 1 & -1 & 1 & -1 & -1 & -1 & -1 & -1\\\\  \n-1 & 1 & -1 & -1 & 1 & 1 & 1 & 1 & 1\\\\  \n-1 & 1 & -1 & -1 & 1 & 1 & 1 & 1 & -1\\\\  \n-1 & 1 & -1 & -1 & 1 & 1 & 1 & -1 & 1\\\\  \n-1 & 1 & -1 & -1 & 1 & 1 & 1 & -1 & -1\\\\  \n\\end{matrix}\n$$\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots\\cdots$$\n\\begin{matrix}\n-1 & -1 & -1 & -1 & -1 & -1 & -1 & 1 & -1\\\\  \n-1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & 1\\\\  \n-1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1  \n\\end{matrix}\n\\begin{bmatrix}\n1.50197\\times10^{100} & 1.50197\\times10^{100} & 1.50197\\times10^{100}\\\\ \n1.50197\\times10^{100} & 1.50197\\times10^{100} &1.50197\\times10^{100} \\\\ \n1.50197\\times10^{100}& 1.50197\\times10^{100} &1.50197\\times10^{100} \n\\end{bmatrix}\nIt is sad that the \"array size\" blows out of the compiler's capacity when running my program for the $M_4$ case, the case that would either kill or save my question.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/40451", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 1}}
{"Q": "A Diophantine equation Let $a,b$ be integers, $a>b\\ge 1$, \n$a^2(a+1)$ be divisible by $b$, and \n$3a^2$ be divisible by $b$.\nLet us consider the following expression:\n$\\frac {1+3a+3a^2+a^2(a-b+1)/b}\n{1+\\frac{3}{a}+\\frac{3}{a^2}+\\frac{b}{a^2(a-b+1)}}$.\nThis fraction is always integer if $b=1$. For $b>1$, I know only one pair $a,b$ such that \nthe fraction is integer, namely, $a=15,b=9$ (I checked all pairs with $a<10^4$).\nCan anyone prove that there are no other pairs $a,b$ such that the fraction is integer?\nThis question is related to the algebraic graph theory.\nThanks for any comments or hints!\n", "A": "If we substitute $f= \\frac{a-b+1}{ab}$ and subtract $a^3$ the question becomes; when is this an integer:\n$$\\frac{a^3 \\times (f-1) \\times (a^3 + \\frac{1}{f})}{(a+1)^3 + \\frac{1}{f} -1}$$\nNote that $b=1$ implies that $f=1$ and this quotient is zero.\nUsing Maple I found that $a=14$, $b=12$ is a second example which satisfies your conditions.\n(sorry, not a full answer, I'm new here and not sure how to post)\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/66654", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "$ - \\sum_{n=1}^\\infty \\frac{(-1)^n}{2^n-1} =? \\sum_{n=1}^\\infty \\frac{1}{2^n+1}$ Numerical evidence suggests:\n$$ - \\sum_{n=1}^\\infty \\frac{(-1)^n}{2^n-1} =? \\sum_{n=1}^\\infty \\frac{1}{2^n+1} \\approx 0.764499780348444 $$\nCouldn't find cancellation via rearrangement. \nFor the second series WA found closed form.\n\nIs the equality true?\n\n", "A": "This is just a formal proof using idea of Lo\u00efc Teyssier that appeared in the comments. We just rewrite both sides of the equation as double sums (using geometric series) and notice that they are equal after changing the order of summation.\nSince $$\\frac{q}{1-q} = \\sum_{k=1}^\\infty q^k,$$ we have for the left hand side\n\\begin{align}\n- \\sum_{n=1}^\\infty \\frac{(-1)^n}{2^n-1} & = - \\sum_{n=1}^\\infty \\frac{(-1)^n2^{-n}}{1-2^{-n}}  \\\\\n& = - \\sum_{n=1}^\\infty (-1)^n\\sum_{k=1}^\\infty (2^{-n})^k \\\\\n& = \\sum_{k,n = 1}^\\infty (-1)^{n+1} 2^{-nk}.\n\\end{align}\nSimilarly, for the right hand side we use $$\\frac{q}{1+q} = \\sum_{k=1}^\\infty (-1)^{k+1} q^k $$ to get\n\\begin{align}\n\\sum_{n=1}^\\infty \\frac{1}{1+2^n} &= \\sum_{n=1}^\\infty \\frac{2^{-n}}{1+2^{-n}}\\\\\n& = \\sum_{n=1}^\\infty \\sum_{k=1}^\\infty (-1)^{k+1}(2^{-n})^k \\\\\n& = \\sum_{k,n=1}^\\infty (-1)^{k+1} 2^{-nk}.\n\\end{align}\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/143592", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 2, "answer_id": 0}}
{"Q": "On class numbers $h(-d)$ and the diophantine equation $x^2+dy^2 = 2^{2+h(-d)}$ Given fundamental discriminant $d \\equiv -1 \\bmod 8$ such that the quadratic imaginary number field $\\mathbb{Q}(\\sqrt{-d})$ has odd class number $h(-d)$. Is it true that one can always solve the diophantine equation,\n$$x^2+dy^2 = 2^{2+h(-d)}\\tag{1}$$\nwith odd integers $x,y$? \nFor example, one can indeed solve,\n$$x^2+7y^2 = 2^3$$\n$$x^2+23y^2 = 2^5$$\n$$x^2+47y^2 = 2^7$$\n$$x^2+71y^2 = 2^9$$\nwhich have $d$ with $h(-d) = 1,3,5,7$, respectively, and it is solvable for all such $d$ with $h(-d) \\leq 25$ in the link given above (I tested them all), but it would be nice to know if it is true in general.\n", "A": "Or, put the cheesy way: with $d = p \\equiv 7 \\pmod 8,$ there is a primitive positive quadratic form with coefficients $$ \\langle 2, 1 , \\frac{p+1}{8} \\rangle.  $$ As a result, with class number $h,$ by repeated Gauss composition we know the principal form \n$$ \\langle 1, 1 , \\frac{p+1}{4} \\rangle $$ integrally represents $2^h.$ Note that, from Theorem 4.12 on page 64 of Buell, the composition algorithm of Shanks,  as we continue with repeated Gauss composition, the middle coefficient remains  $\\equiv 1 \\pmod 4,$ which means that we are showing that the principal form primitively represents $2^h.$\nNext, we have the same class number in the genus of $$\\langle 1, 0 , p \\rangle,  $$ although there is no longer a primitive form that represents $2$ itself.  Page 118, Theorem 7.5 (a) in Buell, Binary Quadratic Forms.\nFinally,\n$$ 4 (x^2 + xy +\\frac{p+1}{4} y^2 )  = 4 x^2 + 4 x y + (p+1) y^2 = (2x+y)^2 + p y^2, $$ so we can represent $4 \\cdot 2^h$ as $u^2 + p v^2.$\nEDIT, Tuesday: Note that $x^2 + xy +\\frac{p+1}{4} y^2$ and $u^2 + p v^2$ represent exactly the same odd numbers, as $\\frac{p+1}{4}$ is even, so to get an odd number we have $x^2 + xy = x(x+y)$ odd, so $x$ is odd and $y$ is even; take $u=x, v = y/2.$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/148624", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 1}}
{"Q": "Sets of squares representing all squares up to $n^2$ Let $S_n=\\{1,2,\\ldots,n\\}$ be natural numbers up to $n$.\nSay that a subset $S \\subseteq S_n$\nsquare-represents $S_n^2$ if every\nsquare $1^2,2^2,\\ldots,n^2$ can be represented by adding or subtracting\nat most one copy of squares of elements of $S$.\nExample. For $n=7$, the set $S$ of $|S|=5$ numbers\n$\\{1, 2, 3, 5, 7\\}$\nsquare-represents $S_7^2$\nbecause\n$4^2 = 5^2 - 3^2$\nand $6^2 = 7^2 - 2^2 - 3^2$.\nThe \"at most one copy\" condition means that one could represent each\nsquare missing from $S$ by a two-pan balance.\nMy question is:\n\nQ. What is the minimum size $|S|$ to square-represent $S_n^2$ as $n$ gets large?\n  In particular, is this size sublinear, $o(n)$?\n\nIn some sense this asks for the frequency of occurrences of Pythagorean triples,\nquadruples, and other analogous solutions of equations of the\nform $a^2 = b^2 \\pm c^2 \\pm d^2 \\pm \\cdots$ with $a,b,c,d,\\ldots$ distinct.\nThis may be well-known to the experts, in which case I apologize for asking a naive question.\n", "A": "Here is a little example following Jeremy Rouse's construction. Let $A$ be the $a_i$ terms\nand $B$ the $b_i$ terms.\n$$A=\\{1,2,3,4,5,6,8,11,16\\}$$\n$$B=\\{0,0,0,0,42,78,142,263,519\\}$$\nFor example \n$$a_9 = \\lfloor \\sqrt{b_8} \\rfloor = \\lfloor \\sqrt{263} \\rfloor = 16 \\;,$$\nand\n$$b_9=b_8+a^2_9 = 263+16^2 = 519 \\;.$$\nThe missing squares can be achieved as follows:\n$$7^2 = 2^2 + 3^2 + 6^2$$\n$$9^2 = 1^2 + 4^2 + 8^2$$\n$$10^2 = 6^2 + 8^2$$\n$$12^2 = 3^2 - 11^2 + 16^2$$\n$$13^2 = -4^2+ 8^2+ 11^2$$\n$$14^2 = 2^2 -8^2 + 16^2$$\n$$15^2 = 1^2 + 2^2 - 6^2 + 16^2$$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/191185", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 1}}
{"Q": "Can a block matrix with at least 3 zero blocks of different size on the diagonal and 1's everywhere else have only integer eigenvalues? Let $M=\\begin{pmatrix}\n\\begin{array}{cccccccc}\n 0 & 0 & 1 & 1 & 1 & 1 & 1 &1\\\\\n 0 & 0 & 1 & 1 & 1 & 1 & 1 &1\\\\\n 1 & 1 & 0 & 0 & 0 & 1 & 1 &1\\\\\n 1 & 1 & 0 & 0 & 0 & 1 & 1 &1\\\\\n 1 & 1 & 0 & 0 & 0 & 1 & 1 &1\\\\\n 1 & 1 & 1 & 1 & 1 & 0 & 0 &1\\\\\n 1 & 1 & 1 & 1 & 1 & 0 & 0 &1\\\\\n 1 & 1 & 1 & 1 & 1 & 1 & 1 &0\\\\\n\\end{array}\n\\end{pmatrix}\\implies\\begin{pmatrix}\n\\begin{array}{ccc}\n 0_{2\\times2} & 1_{2\\times3} & 1_{2\\times2}&1_{2\\times1}\\\\\n 1_{3\\times2} & 0_{3\\times3} & 1_{3\\times2}&1_{3\\times1}  \\\\\n 1_{2\\times2} & 1_{2\\times3} & 0_{2\\times2}&1_{2\\times1}  \\\\\n 1_{1\\times2} & 1_{1\\times3} & 1_{1\\times2}&0_{1\\times1}  \\\\\n  \\end{array}\n\\end{pmatrix}$ be a symmetric matrix whose entries are blocks(diagonal blocks entry must be zero and non-diagonal blocks entry must be one).  How to prove/disprove that all the eigenvalues of $M$ are not integer if and only if the diagonal blocks are not of the same order.Note that the number of diagonal blocks $\\geq 3$ and order of diagonal blocks$\\geq 1$ . \n", "A": "You can have a look at Eigenvalues of complete multipartite graphs. One example with integer spectrum is $K_{2,2,2,8,8\\ }.$\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/225251", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 3, "answer_id": 1}}
{"Q": "Integral quaternary forms and theta functions The following question arises when I attempt to understand the modular parameterization of the elliptic curve $$E:y^2-y=x^3-x$$\nIn Mazur-Swinnerton-Dyer and Zagier's construction, a theta function associated with a positive definite quadratic form is induced:\n$$\\theta(q)=\\sum_{x\\in\\mathbb{Z}^4}q^{\\frac{1}{2}x^{T}Ax}$$\nwhere $$A=\\left(\\begin{matrix}2 & 0 & 1 & 1\\\\\n0 & 4 & 1 &2\\\\\n1 & 1 & 10 & 1\\\\\n1 & 2 & 1 & 20\n\\end{matrix}\\right)$$\n$A$ is a positive definite matrix of determinant $37^2$, and we have $37A^{-1}=K^TAK$ where $K$ is an integral matrix of determinant $\\pm 1$.\nQuestion: Suppose $A$ is a positive definite $4\\times 4$ matrix with integral entries. All diagonal entries are even numbers. The determinant of $A$ is a square number $N^2$. Is it true that for every $N=p$ ($p>2$ is a prime number), there is at least one $A$ that $NA^{-1}=K^TAK$, where $K$ is an integral matrix of determinant $\\pm 1$?\n", "A": "The answer is true, using the following construction.\nLet $B$ be the quaternion algebra of discriminant $p$ and let $O$ be a maximal order with an element $x$ satisfying $x^2 = -p$. The reduced norm is a quadratic form on $O$, with positive definite Gram matrix $A$ of determinant $p^2$. The matrix $A^{-1}$ then represents the reduced norm on the dual lattice $O^{\\sharp}$. Recall that $\\text{nrd}(\\text{diff}(O)) = \\text{discrd}(O) = p$ and since $O$ is maximal, $\\text{diff}(O)$ is invertible and $O = \\text{diff}(O) O^{\\sharp}$ (see e.g.\nVoight, John, Quaternion algebras,  ZBL07261776. Section 16.8), hence\n$$\np O^{\\sharp} \\subseteq \\text{diff}(O) O^{\\sharp} = O\n$$\nwhich implies that $p \\cdot (O^{\\sharp} / O) = 0$ (therefore $O^{\\sharp} / O$ is an abelian group of exponent $p$ and size $p^2$, so isomorphic to $(\\mathbb{Z} / p \\mathbb{Z})^2$). Next, we note that the matrix $A^{-1}$ is also the change of basis matrix between the chosen basis of $O$ and its dual, therefore $pA^{-1}$ is integral.\nThis is not enough, but so far we have not used the element $x$. The matrix $pA^{-1}$ is the matrix representing the norm form on the ideal $xO^{\\sharp}$, since $x^2 = -p$. But $nrd(xO^{\\sharp})=p$ is a maximal order with an element $x$ such that $x^2 = -p$. By a theorem of Ibukiyama (see reference below), it is isomorphic (hence isometric as lattices) to $O$.\nThis could also be seen directly (referring to some of the answers above - it is possible to do it using a finite number of cases) by explicitly writing down the maximal orders. I will list below explicit constructions, which are based on\nIbukiyama, Tomoyoshi, On maximal orders of division quaternion algebras over the rational number field with certain optimal embeddings, Nagoya Math. J. 88, 181-195 (1982). ZBL0473.12012. -\nIf $p \\equiv 3 \\bmod 4$, then $B = (-p,-1)$, and $O = \\mathbb{Z}<(1+i)/2,j>$. In this case, we have\n$$\nA = \\left( \\begin{array}{cccc} \n2 & 1 & 0 & 0 \\\\ \n1 & \\frac{p+1}{2} & 0 & 0 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 1 & \\frac{p+1}{2}\n\\end{array} \\right)\n,\npA^{-1} = \\left( \\begin{array}{cccc} \n\\frac{p+1}{2} & -1 & 0 & 0 \\\\ \n-1 & 2 & 0 & 0 \\\\\n0 & 0 & \\frac{p+1}{2} & -1 \\\\\n0 & 0 & -1 & 2\n\\end{array} \\right),\nK = \\left( \\begin{array}{cccc} \n-1 & 1 & 0 & 0 \\\\ \n1 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{array} \\right)\n$$\nIf $p \\equiv 1 \\bmod 4$, then choosing $q \\equiv 3 \\bmod 4$ such that\n$\\left( \\frac{q}{p} \\right) = -1$, we can find $c$ such that\n$c^2 \\equiv -p \\bmod q$, and then $B = (-p,-q)$ and\n$$\nO = \\mathbb{Z} \\oplus \\mathbb{Z} \\frac{1+j}{2} \\oplus\n\\mathbb{Z} \\frac{i(1+j)}{2} \\oplus \\mathbb{Z} \\frac{(c+i)j}{q}\n$$\nIn this case, we compute that\n$$\nA = \\left( \\begin{array}{cccc} \n2 & 1 & 0 & 0 \\\\ \n1 & \\frac{q+1}{2} & 0 & c \\\\\n0 & 0 & \\frac{p(q+1)}{2} & p \\\\\n0 & c & p & \\frac{2(p+c^2)}{q}\n\\end{array} \\right)\n,\npA^{-1} = \\left( \\begin{array}{cccc} \n\\frac{(q+1)(c^2+p)}{2q} & -c-\\frac{c^2+p}{q} & -c & \\frac{c(q+1)}{2} \\\\ \n-c-\\frac{c^2+p}{q} & 2(c^2 + \\frac{c^2+p}{q}) & 2c & -c(q+1) \\\\\n-c & 2c & 2 & -q \\\\\n\\frac{c(q+1)}{2} & -c(q+1) & -q & \\frac{q(q+1)}{2}\n\\end{array} \\right)\n$$\nand if $\\{e_1, e_2, e_3, e_4 \\}$ is the above basis for $O$, then we see that $ \\{e_2 e_4, ce_1 - e_4, e_1, j e_2 \\} $ is a basis for the resulting module, which written in terms of the original basis yields the matrix\n$$\nK = \\left( \\begin{array}{cccc}\n0 & c & 1 & -\\frac{q+1}{2} \\\\\n-c & 0 & 0 & 1 \\\\\n-1 & 0 & 0 & 0 \\\\\n\\frac{q+1}{2} & -1 & 0 & 0\n\\end{array}\n \\right).\n$$\nReferring to Will Jagy's wondering in the first answer, the reason that this does not work for most lattices is that they do not correspond to a maximal order (as in the answer by few_reps, the quotient of the lattices is actually cyclic) and there are only one or two (depending on $p \\bmod 4$ isomorphism classes of maximal orders which contain a root of $-p$. For example, for $p = 37$, there are only two isomorphism classes of maximal orders in the quaternion algebra, and only one of them contains a root of $-p$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/231770", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Solution to a Diophantine equation Find all the non-trivial integer solutions to the equation\n$$\\frac{a}{b+c}+\\frac{b}{a+c}+\\frac{c}{a+b}=4.$$\n", "A": "There is one idea. To search for the solution of the equation.\n$$\\frac{a}{b+c}+\\frac{b}{a+c}+\\frac{c}{a+b}=q$$\nIf we know any solution $(a,b,c)$ of this equation. Then it is possible to find another $(a_2, b_2, c_2)$.  Make such a change.\n$$y=(a+b+2c)(q(a+b)-c)-(a+b)^2-(a+c)(b+c)$$\n$$z=(a+2b+c)(2b-qa-(q-1)c)+(b+2a+c)(2a-qb-(q-1)c)$$\nThen the following solution can be found by the formula.\n$$a_2=((5-4q)c-(q-2)(3b+a))y^3+((5-4q)b+4(1-q)c)zy^2+$$\n$$+(3c+(q-1)(a-b))yz^2-az^3$$\n$$b_2=((5-4q)c-(q-2)(3a+b))y^3+((5-4q)a+4(1-q)c)zy^2+$$\n$$+(3c+(q-1)(b-a))yz^2-bz^3$$\n$$c_2=2(q-2)cy^3+3(2-q)(a+b)zy^2+$$\n$$+((5-4q)(a+b)+2(1-q)c)yz^2+(2c-(q-1)(a+b))z^3$$\nI tried this formula to simplify, but nothing happens. Maybe someone will check in Maple?\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/264754", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 3, "answer_id": 1}}
{"Q": "How to deduce an equation from this 3 Diophantine equations with 5 variables? I have three equations: \n${m \\choose 2} + nk = {x \\choose 2}$\n${n \\choose 2} + mk = {y \\choose 2}$\n$x + y = m + n + k$\n$m, n, k, x, y$ are natural numbers. I want to deduce from this 3 equations either $x = y$ or $m = n$. From where I got these equations makes me sure that this is only possible if $x = y$ and $m = n$. Just deducing either $x=y$ or $m=n$ is enough. \nI can show that if I show that $x + y$ is not divisible by 3. So it will be enough if we can show that $x + y$ is not divisible by 3.\n", "A": "The first two equalities imply $x>m$ and $y>n$ so one can substitute $x=m+X$, $y=n+Y$ and $k=X+Y$, with still $X,Y \\in \\mathbb N$:\n${X \\choose 2}=nX+nY-mX\\tag{1}$\n${Y \\choose 2}=mX+mY-nY\\tag{2}$\nFrom (1) follows: $\\quad m=n+n\\frac{Y}{X}-\\frac{1}{X}{X \\choose 2}$,\nthen eliminate $m$ from (2): $\\quad {Y \\choose 2}+{X \\choose 2}+\\frac{Y}{X}{X \\choose 2}=nX+nY+n\\frac{Y^2}{X}$,\nand finally: $\\quad 2n=X-\\frac{X^2+2XY}{X^2+XY+Y^2}$.\nClearly $X-2n=\\frac{X^2+2XY+0Y^2}{X^2+XY+Y^2}\\in(0,2)$, but since it is an integer it can only be $1$.\nThis proves that $X=2n+1$ and by symmetry $Y=2m+1$.\nSubstituting for $X$ and $Y$ in (1) or (2) finally yields $m=n$, so $X=Y$ and $x=y$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/277766", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 0}}
{"Q": "Cycle index of $S_n \\times S_m$ Given $n$, $m$ and $k$, I would like to evaluate cycle index of $S_n \\times S_m$ for $c_1 = c_2 = ... = c_{nm} = k$. What is the fastest known algorithm to calculate it? For this particular case, is there any polynomial time algorithm? I know that $Z(S_n) = \\frac{1}{n}\\sum_{i=1}^na_iZ(S_{n-i})$. Maybe there is a similiar recursive formula for $Z(S_n \\times S_m)$?\nEdit: For better explanation of the question here I give an example where $n = 2$, $m = 3$, $k = 2$.\n$Z(S_2) = \\frac{1}{2}(a_1^2 + a_2)$\n$Z(S_3) = \\frac{1}{6}(b_1^3 + 3b_1b_2 + 2b_3)$\n$Z(S_2 \\times S_3) = \\frac{1}{12} c_1^6 + \\frac{1}{3} c_2^3 + \\frac{1}{6} c_3^2 + \\frac{1}{4} c_1^2 c_2^2 + \\frac{1}{6} c_6$\nAnd now substituting $c_1 = c_2 = c_3 = c_4 = c_5 = c_6 = k$ gives $\\frac{1}{12} k^6 \n+ \\frac{1}{4} k^4\n+ \\frac{1}{3} k^3\n+ \\frac{1}{6} k^2\n+ \\frac{1}{6} k$, so the answer I'm looking for is $13$. Note that I'm interested only in the value, not the polynomial itself.\n", "A": "$Z(S_n \\times S_m)$ evaluated at $c_1 = c_2 = ... = c_{nm} = k$ equals\n$$\\sum_{i_1+2i_2+\\dots+ni_n=n}\\frac{1}{1^{i_1} i_1!\\cdots n^{i_n} i_n!} \\sum_{j_1+2j_2+\\dots+mj_m=m} \\frac{1}{1^{j_1} j_1!\\cdots m^{j_m} j_m!}\\cdot\nk^{\\sum_{p=1}^n\\sum_{q=1}^m \\gcd(p,q)i_p j_q}.$$\nThere are $p(n)\\cdot p(m)$ terms in this double sum.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/291012", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "Minimal polynomial in $\\mathbb Z[x]$ of seventh degree with given roots I am looking for a seventh degree polynomial with integer coefficients, which has the following roots.\n$$x_1=2\\left(\\cos\\frac{2\\pi}{43}+\\cos\\frac{12\\pi}{43}+\\cos\\frac{14\\pi}{43}\\right),$$\n$$x_2=2\\left(\\cos\\frac{6\\pi}{43}+\\cos\\frac{36\\pi}{43}+\\cos\\frac{42\\pi}{43}\\right),$$\n$$x_3=2\\left(\\cos\\frac{18\\pi}{43}+\\cos\\frac{22\\pi}{43}+\\cos\\frac{40\\pi}{43}\\right)$$\n$$x_4=2\\left(\\cos\\frac{20\\pi}{43}+\\cos\\frac{32\\pi}{43}+\\cos\\frac{34\\pi}{43}\\right),$$\n$$x_5=2\\left(\\cos\\frac{10\\pi}{43}+\\cos\\frac{16\\pi}{43}+\\cos\\frac{26\\pi}{43}\\right),$$\n$$x_6=2\\left(\\cos\\frac{8\\pi}{43}+\\cos\\frac{30\\pi}{43}+\\cos\\frac{38\\pi}{43}\\right)$$ and\n$$x_7=2\\left(\\cos\\frac{4\\pi}{43}+\\cos\\frac{24\\pi}{43}+\\cos\\frac{28\\pi}{43}\\right).$$\nI see only that $\\sum\\limits_{k=1}^7x_k=-1$, but the computations for $\\sum\\limits_{1\\leq i<j\\leq7}x_ix_j$ and the similar are very complicated by hand and I have no any software besides WA, which does not help.\nThank you for your help!\nUpdate.\nI got $$\\sum\\limits_{1\\leq i<j\\leq7}x_ix_j=-18.$$\n", "A": "By PARI / GP I get\n$x^7 + x^6 - 18*x^5 - 35*x^4 + 38*x^3 + 104*x^2 + 7*x - 49$ :\nK = nfinit (subst(polcyclo(43),x,y))\nw = Mod(y,K.pol)\nf0(k) = (w^k + 1/w^k)\nf(k1,k2,k3) = f0(k1) + f0(k2) + f0(k3)\nv = [f(1,6,7),f(3,18,21),f(9,11,20),f(10,16,17),f(5,8,13),f(4,15,19),f(2,12,14)]\n/*\n=\n[x^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49,\nx^7 + x^6 - 18x^5 - 35x^4 + 38x^3 + 104x^2 + 7*x - 49]\n*/\nmps = [minpoly(w) | w<-v]\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/375278", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 3, "answer_id": 1}}
{"Q": "Can a product of Cohn matrices over the Eisenstein integers with non-zero, non-unit coefficients be a Cohn matrix? For $k > 1$, is it possible that $\\begin{pmatrix} a_1 & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} a_2 & 1 \\\\ -1 & 0 \\end{pmatrix}\\ldots \\begin{pmatrix} a_k & 1 \\\\ -1 & 0 \\end{pmatrix} = \\pm \\begin{pmatrix} b & 1 \\\\ -1 & 0 \\end{pmatrix}$ if $a_1,a_2,\\ldots a_k,b$ are Eisenstein integers and $|a_i| > 1$ for $i=1,2,\\ldots k$?\nIf the Eisenstein integers are replaced with the Gaussian integers, this is possible.\n$\\begin{pmatrix} 3 & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 - i & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 + i & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 - i & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} -2 & 1 \\\\ -1 & 0 \\end{pmatrix} = -\\begin{pmatrix} i & 1 \\\\ -1 & 0 \\end{pmatrix}$\nThis problem came up in my research; I am primarily interested in the case where $b = (\\pm 1 \\pm \\sqrt{-3})/2$, but I suspect that there might not be a solution for any choice of $b$.\nI originally asked this question on math.stackexchange (https://math.stackexchange.com/q/3903567/202799), but it seems to be more difficult than I first thought.\n", "A": "To my surprise, not only is there a solution for some $b$, there is actually a very simple infinite family of solutions for every $b$. Let $\\omega = \\frac{1 + \\sqrt{-3}}{2}$. Then\n$\\begin{pmatrix} a_0 + a_1\\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 2 -\\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 + \\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 2 - \\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 + \\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} 2 - \\omega & 1 \\\\ -1 & 0 \\end{pmatrix}\\begin{pmatrix} b_0 + b_1\\omega & 1 \\\\ -1 & 0 \\end{pmatrix} = -\\begin{pmatrix} a_0 + b_0 - 1 + (a_1 + b_1 - 1)\\omega & 1 \\\\ -1 & 0 \\end{pmatrix}$\nI wish I could say that there was some clever trick to finding this solution, but I cannot: it was found by a brute force search through $\\approx 5$ million possibilities using Python after I realized that the bottom right coordinate of the product of $n$ Cohn matrices only depends on the inner $n - 2$ matrices.\nI do not know if there are shorter solutions. One can prove that there are no solutions with $k < 5$, but there could be solutions with $k = 5$ or $k = 6$. If there are, however, they have to involve some elements with squared norm at least $4$, as I checked everything smaller.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/376328", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "How to show a $3\\times3$ matrix has three distinct eigenvalues? Here is a question I heared from others:\nGiven four distinct positive real numbers $a_1,a_2,a_3,a_4$ and set $$a:=\\sqrt{\\sum_{1\\leq i\\leq 4}a_i^2}$$\n$A=(x_{i,j})_{1\\leq i\\leq3,1\\leq j\\leq4}$ is a $3\\times4$-matrix specified by $$ x_{i,j}=a_i\\delta_{i,j}+a_j\\delta_{4,j}-\\frac{1}{a^2}(a_i^2+a_4^2)a_j $$\nwhere $\\delta_{i,j}$ is the Kronecker symbol or visually\n$$ A=\\begin{pmatrix}a_1 &0&0&a_4\\\\ 0&a_2&0&a_4\\\\0&0&a_3&a_4\\end{pmatrix}-\\frac{1}{a^2}\n\\begin{pmatrix} \na_1(a_1^2+a_4^2) & a_2(a_1^2+a_4^2) & a_3(a_1^2+a_4^2) & a_4(a_1^2+a_4^2)\\\\\na_1(a_2^2+a_4^2) & a_2(a_2^2+a_4^2) & a_3(a_2^2+a_4^2) & a_4(a_2^2+a_4^2)\\\\\na_1(a_3^2+a_4^2) & a_2(a_3^2+a_4^2) & a_3(a_3^2+a_4^2) & a_4(a_3^2+a_4^2)\\\\\n\\end{pmatrix} $$\nThe question is to show that the $3\\times3$-matrix $B=AA^T$ admits three distinct eigenvalues.($A^T$ is the transpose of $A$)\nWhat I am curious about is how many methods can be utilized to show a matrix has different eigenvalues?\nAs for this question my idea is to calculate the characteristic polynomial $f$ of $B$ along with $f'$ which is a quadratic polynomial via Sagemath and show that neither of roots of $f'$ belongs to $f$. Or equivalently to calculate the resultant $R(f,f')$ of $f$ and $f'$ and show that $R(f,f')$ doesn't vanish for any distinct positive $a_i$'s.\nBut the difficulties are both ways involve hideous calculation which I don't think I can write down by hand. So I'm wondering if there is a tricky way to get to that point? (e.g. an algebraic-geometry method?)\n", "A": "This question is from this year's Alibaba mathematics competition (qualifying round, which is finished 2 days ago), and here's my solution that could be wrong (I also participated in the competition and this is the solution I submitted). I tried to solve the problem geometrically to avoid tons of computations.\n\nFirst, we can deal with $A^{T}A$ instead of $AA^{T}$ since the first matrix's eigenvalues is same as the second eigenvalue's matrix with zero (consider SVD of $A$). The key point is that $A$ can be written as $A = BP$ where\n$$\nB = \\begin{pmatrix} a_1 & 0 & 0 & a_4 \\\\ 0 & a_2 & 0 & a_4 \\\\ 0 & 0 & a_3 & a_4 \\end{pmatrix}\n$$\nand\n$$\nP = I_3 - \\mathbf{v}\\mathbf{v}^{T}, \\mathbf{v} = \\frac{1}{a}(a_1, a_2, a_3, a_4)^{T}. \n$$\nEspecially, the matrix $P$ is an orthogonal projection matrix that project a vector in $\\mathbb{R}^{4}$ to the subspace of vectors that are perpendicular to $\\mathbf{v}$. It satisfies $P^{T} = P^{2} = P$.\nTo show that the eigenvalues are distinct, we will show that each eigenspace (for nonzero eigenvalues) has dimension 1. In other words, for a given eigenvalue, there exists a unique eigenvector (up to constant factor) corresponding to the eigenvalue.\nFirst, the above $\\mathbf{v}$ is an eigenvector of $A^{T}A$ correspond to the eigenvalue 0 since $A\\mathbf{v} = BP\\mathbf{v} = \\mathbf{0}$. Since the eigenvectors are orthogonal to each other, the other three eigenvectors are in the image of $P$ (the hyperplane perpendicular to $\\mathbf{v}$). If we fix an (nonzero) eigenvalue $\\lambda$ and a corresponding eigenvector $\\mathbf{w}$, we have $P\\mathbf{w} = \\mathbf{w}$ and so\n$$\nA^{T}A\\mathbf{w} = PB^{T}BP\\mathbf{w} = PB^{T}B\\mathbf{w} = \\lambda \\mathbf{w}.\n$$\nFrom this, the vector $B^{T}B\\mathbf{w}$ should be written as\n$$\nB^{T}B\\mathbf{w} = \\lambda \\mathbf{w} + \\beta \\mathbf{v}\n$$\nfor some $\\beta$. If we set $\\mathbf{w} = (x_1, x_2, x_3, x_4)^{T}$, then expanding the above equation gives\n$$\n\\begin{pmatrix}a_1^2 & 0 & 0 & a_{1}a_{4} \\\\ 0 & a_{2}^{2} &  0 & a_{2}a_{4} \\\\ 0 & 0& a_{3}^{2} & a_{3}a_{4} \\\\ a_{1}a_{4} & a_{2}a_{4} & a_{3}a_{4} & 3a_{4}^{2} \\end{pmatrix}\\mathbf{w} = \\begin{pmatrix} a_{1}^{2}x_{1} + a_{1}a_{4}x_{4} \\\\ a_{2}^{2}x_{2} + a_{2}a_{4}x_{4} \\\\ a_{3}^{2}x_{3} + a_{3}a_{4}x_{4} \\\\ a_{4}(a_{1}x_{1} + a_{2}x_{2} + a_{3}x_{3} + 3a_{4}x_{4})\\end{pmatrix} = \\begin{pmatrix} a_{1}^{2}x_{1} + a_{1}a_{4}x_{4} \\\\ a_{2}^{2}x_{2} + a_{2}a_{4}x_{4} \\\\ a_{3}^{2}x_{3} + a_{3}a_{4}x_{4} \\\\ 2a_{4}^{2}x_{4}\\end{pmatrix}  = \\begin{pmatrix} \\lambda x_1 + \\beta' a_1 \\\\ \\lambda x_2 + \\beta' a_2 \\\\ \\lambda x_3 + \\beta' a_3 \\\\ \\lambda x_4 + \\beta' a_4\\end{pmatrix}, \\quad \\beta' = \\frac{\\beta}{a}\n$$\nHere we used $\\langle \\mathbf{v}, \\mathbf{w} \\rangle = a_{1}x_{1} + \\cdots + a_{4}x_{4} = 0$ for the second equality. From this, we can show that $x_{4}$ should be nonzero (here's the point that distinctiveness of $a_i$'s are used), so we can assume that $x_4 = 1$ and the other components $x_1, x_2, x_3$ are uniquely determined. This proves our claim that each eigenspace has dimension 1, i.e. the eigenvalues are distinct.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/393053", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 3, "answer_id": 2}}
{"Q": "Subtraction-free identities that hold for rings but not for semirings? Here is a concrete, if seemingly unmotivated, aspect of the question I am interested in:\n\nQuestion 1. Let $a$ and $b$ be two elements of a (noncommutative) semiring $R$ such that $1+a^3$ and $1+b^3$ and $\\left(1+b\\right)\\left(1+a\\right)$ are invertible. Does it follow that $1+a$ and $1+b$ are invertible as well?\n\nThe answer to this question is\n\n*\n\n*\"yes\" if $ab=ba$ (because in this case, $1+a$ is a left and right divisor of the invertible element $\\left(1+b\\right)\\left(1+a\\right)$, and thus must itself be invertible; likewise for $1+b$).\n\n\n*\"yes\" if $R$ is a ring (because in this case, $1+a$ is a left and right divisor of the invertible element $1+a^3 = \\left(1+a\\right)\\left(1-a+a^2\\right) = \\left(1-a+a^2\\right)\\left(1+a\\right)$, and thus must itself be invertible; likewise for $1+b$).\n\n\n*\"yes\" if $1+a$ is right-cancellable (because in this case, we can cancel $1+a$ from $\\left(1+a\\right) \\left(\\left(1+b\\right)\\left(1+a\\right)\\right)^{-1} \\left(\\left(1+b\\right)\\left(1+a\\right)\\right) = 1+a$ to obtain $\\left(1+a\\right) \\left(\\left(1+b\\right)\\left(1+a\\right)\\right)^{-1} \\left(1+b\\right) = 1$, which shows that $1+a$ is invertible), and likewise if $1+b$ is left-cancellable.\nI am struggling to find semirings that are sufficiently perverse to satisfy none of these cases and yet have $\\left(1+b\\right)\\left(1+a\\right)$ invertible. (It is easy to find cases where $1+a^3$ is invertible but $1+a$ is not; e.g., take $a = \\begin{pmatrix} 0&1\\\\ 0&0 \\end{pmatrix}$ in the matrix semiring $\\mathbb{N}^{2\\times 2}$.)\nThe real question I'm trying to answer is the following (some hopefully reasonably clear handwaving included):\n\nQuestion 2. Assume we are given an identity that involves only positive integers, addition, multiplication and taking reciprocals. For example, the identity can be $\\left(a^{-1} + b^{-1}\\right)^{-1} = a \\left(a+b\\right)^{-1} b$ or the positive Woodbury identity $\\left(a+ucv\\right)^{-1} + a^{-1}u \\left(c^{-1} + va^{-1}u\\right)^{-1} va^{-1} = a^{-1}$. Assume that this identity always holds when the variables are specialized to arbitrary elements of an arbitrary ring, assuming that all reciprocals appearing in it are well-defined. Is it then true that this identity also holds when the variables are specialized to arbitrary elements of an arbitrary semiring, assuming that all reciprocals appearing in it are well-defined?\n\nThere is a natural case for \"yes\": After all, the same claim holds for commutative semirings, because in this case, it is possible to get rid of all reciprocals in the identity by bringing all fractions to a common denominator and then cross-multiplying with these denominators. However, this strategy doesn't work for noncommutative semirings (and even simple-looking equalities of the form $ab^{-1} = cd^{-1}$ cannot be brought to a reciprocal-free form, if I am not mistaken). Question 1 is the instance of Question 2 for the identity\n\\begin{align}\n\\left(1+a^3\\right)^{-1} \\left(1+b^3\\right)^{-1} \\left(1+a\\right) \\left(\\left(1+b\\right)\\left(1+a\\right)\\right)^{-1} \\left(1+b\\right) = \\left(1+a^3\\right)^{-1} \\left(1+b^3\\right)^{-1}\n\\end{align}\n(where, of course, the only purpose of the $\\left(1+a^3\\right)^{-1} \\left(1+b^3\\right)^{-1}$ factors is to require the invertibility of $1+a^3$ and $1+b^3$). Indeed, if $\\alpha$ and $\\beta$ are two elements of a monoid such that $\\beta\\alpha$ is invertible, then we have the chain of equivalences\n\\begin{align}\n\\left(\\alpha\\text{ is invertible} \\right)\n\\iff\n\\left(\\beta\\text{ is invertible} \\right)\n\\iff\n\\left( \\alpha \\left(\\beta\\alpha\\right)^{-1} \\beta = 1 \\right)\n\\end{align}\n(easy exercise).\n", "A": "The answer to your first question is yes (which was very surprising to me, to be honest).  I have no idea whether the second question also has a positive answer.  (By the way, don't let the work below fool you.  This took me an entire week of serious computations to discover the main idea.)\nWe will assume $(1+a^3)u=1$ and $d(1+a)=1$.  We find that\n$$\nd+au = d(1+(1+a)au) = d((1+a^3)u+(a+a^2)u)=d(1+a)(1+a^2)u=(1+a^2)u.\n$$\nThus, we compute\n$$\n(1+a)d = d[1+(1+a)ad] = d[1+ad+a^2d] = d[u+a^3u+a^2d+ad]\n$$\n$$\n=d[a^2(d+au) + ad + u] = d[a^2(1+a^2)u+ad+u] = d[a(d+au)+a^4u+u]\n$$\n$$\n=d[a(1+a^2)u+(1+a^4)u] = d(1+a)(1+a^3)u=1.\n$$\n\nEdited to add: A similar idea works for higher odd powers.  The fifth power case is sufficient to give the main idea.\nAssume $(1+a^5)u=1=d(1+a)$.  We find\n$$\nd+(a+a^3)u = d[(1+a^5)u + (1+a)(a+a^3)u] = d(1+a)(1+a^2+a^4)u=(1+a^2+a^4)u.\n$$\nThen we compute\n$$\n(1+a)d=d^3[(1+a)^2+(1+a)^3ad] = d^3[(1+a)^2(1+a^5)u + (a+3a^2+3a^3+a^4)d] = d^3[u+2au+a^2u+2a^6u + (a+3a^2+3a^3)d + a^4[(a+a^3)u+d]] = \\cdots\n$$\nand you keep reducing monomials with $d$ to monomials involving only $a$ and $u$.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/398544", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "26", "answer_count": 3, "answer_id": 1}}
{"Q": "An infinite series that converges to $\\frac{\\sqrt{3}\\pi}{24}$ Can you prove or disprove the following claim:\n\nClaim:\n$$\\frac{\\sqrt{3} \\pi}{24}=\\displaystyle\\sum_{n=0}^{\\infty}\\frac{1}{(6n+1)(6n+5)}$$\n\nThe SageMath cell that demonstrates this claim can be found here.\n", "A": "Here is an elementary proof. We rewrite the series as\n$$\\frac{1}{4}\\int_0^1\\frac{1-x^4}{1-x^6}\\,dx=\\frac{1}{8}\\int_0^1\\frac{dx}{1-x+x^2}+\\frac{1}{8}\\int_0^1\\frac{dx}{1+x+x^2}.$$\nIt is straightforward to show that\n\\begin{align*}\n\\int_0^1\\frac{dx}{1-x+x^2}&=\\frac{2\\pi}{3\\sqrt{3}},\\\\\n\\int_0^1\\frac{dx}{1+x+x^2}&=\\frac{\\pi}{3\\sqrt{3}},\n\\end{align*}\nso we are done.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/400819", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 3, "answer_id": 1}}
{"Q": "$2n \\times 2n$ matrices with entries in $\\{1, 0, -1\\}$ with exactly $n$ zeroes in each row and each column with orthogonal rows and orthogonal columns I am interested in answering the following question:\nQuestion\nFor a given $n$, does there exist a $2n \\times 2n$ matrix with entries in $\\{1, 0, -1\\}$ having orthogonal rows and columns with exactly $n$ zeroes in each row and column?\nConjectures\n\n*\n\n*For $n=2^k$, $k\\ge0$ such a matrix always exists.\n\n\n*For $n=3$ such a matrix does not exist.\n\n\n*For $n=5$ such a matrix exists, for example:\n$$\n\\left(\n\\begin{array}{cccccccccc}\n 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n 1 & 0 & 0 & 0 & -1 & 0 & -1 & -1 & -1 & 0 \\\\\n 1 & 0 & 0 & -1 & 0 & 0 & -1 & 1 & 1 & 0 \\\\\n 0 & -1 & -1 & 1 & 1 & 0 & -1 & 0 & 0 & 0 \\\\\n 1 & 0 & -1 & 0 & 0 & -1 & 1 & 0 & 0 & -1 \\\\\n 1 & -1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\\\\n 0 & 0 & 0 & -1 & 1 & -1 & 0 & -1 & 0 & 1 \\\\\n 0 & -1 & 1 & 0 & 0 & 0 & 0 & -1 & 1 & -1 \\\\\n 0 & -1 & 1 & 0 & 0 & -1 & 0 & 1 & -1 & 0 \\\\\n 0 & 0 & 0 & -1 & 1 & 1 & 0 & 0 & -1 & -1 \\\\\n\\end{array}\n\\right)\n$$\n\n\n*For $n=7$ such a matrix does not exist.\n", "A": "Your first conjecture was proven by Nate in the comments.\nYour second conjecture is also true - there is no such matrix for $n=3$. If we just look at which entries are nonzero in each row, because any two rows are orthogonal, they must share an even number of nonzero entries, i.e. either share $0$ entries or $2$ entries. Only $3$ other rows can share $2$ nonzero entries with a given row, since otherwise there'd be more than $3$ nonzero entries in a column by the pidgeonhole principle, so $2$ rows must share $0$ nonzero entries, but then they share $3$ entries with each other, contradiction.\n", "meta": {"language": "en", "url": "https://mathoverflow.net/questions/421691", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "10", "answer_count": 3, "answer_id": 0}}
