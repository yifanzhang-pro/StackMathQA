{"Q": "A trivial question about Covariance I'm just learning about Covariance and encountered something I don't quite understand.\nAssume we have two random variables X and Y, where the respective joint-probability function assigns equal weights to each event.\nAccording to wikipedia the Cov(X, Y) can then be caluculated as follows:\n$${\\displaystyle \\operatorname {cov} (X,Y)={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-E(X))(y_{i}-E(Y)).}$$\nWhat confuses me is the fact, that they sum only over $x_i$ and $y_i$; $i=1,...,n$ , but not $x_i$ and $y_j; \\;i=1,...,n$ $j=1,...,m$ , thus many possible combinations are not calculated. In other words if we look at each calculated combination individually, we only get a $n*1$ matrix, instead of a $n*m$ matrix. \nCan anyone explain this (I suppose it's rather obvious but I just don't see the reason at the moment). \n", "A": "The idea is that the possible outcomes in your sample are $i=1, \\ldots, n$, and each outcome $i$ has equal probability $\\frac{1}{n}$ (under the probability measure that assigns equal probability to all outcomes that you appear to be using). You have $n$ outcomes, not $n^2$.\nTo somewhat indulge your idea, you could compute:\n$$ \\operatorname{Cov}(X,Y) = \\sum_i \\sum_j (x_i - \\mu_x) (y_j - \\mu_y) P(X = x_i, Y = y_j )$$\nWhere:\n\n\n*\n\n*$P(X = x_i, Y = y_j) = \\frac{1}{n}$ if $i=j$ since that outcome occurs $1/n$ times.\n\n*$P(X = x_i, Y = y_j) = 0 $ if $i \\neq j$ since that outcome doesn't or didn't\noccur.\n\n\nBut then you'd just have:\n$$ \\sum_i \\sum_j (x_i - \\mu_x) (y_j - \\mu_y) P(X = x_i, Y = y_j ) = \\sum_i (x_i - \\mu_x) (y_i - \\mu_y) P(X = x_i, Y = y_i )  $$\nWhich is what the original formula is when $P(X = x_i, Y = y_i ) = \\frac{1}{n}$.\nYou intuitively seem to want something like $P(X = x_i, Y = y_j )  = \\frac{1}{n^2}$ but that is seriously wrong.\nSimple dice example (to build intuition):\nLet $X$ be the result of a roll of a single 6 sided die. Let $Y = X^2$.\nRecall that a probability space has three components: a sample space $\\Omega$, a set of events $\\mathcal{F}$, and a probability measure $P$ that assigns probabilities to events. (I'm going to hand wave away the event stuff to keep it simpler.)\n$X$and $Y$ are functions from $\\Omega$ to $\\mathcal{R}$. We can write out the possible values for $X$ and $Y$ as a function of $\\omega \\in \\Omega$\n$$ \\begin{array}{rrr} & X(\\omega) & Y(\\omega) \\\\\n\\omega_1 & 1 & 1\\\\\n\\omega_2 & 2 & 4 \\\\\n\\omega_3 & 3 & 9 \\\\\n\\omega_4 & 4 & 16 \\\\\n\\omega_5 & 5 & 25 \\\\\n\\omega_6 & 6 & 36\n\\end{array}\n$$\nWe don't have 36 possible outcomes here. We have 6.\nSince each outcome of a die is equally likely, we have $P( \\{ \\omega_1) \\}) = P( \\{ \\omega_2) \\}) = P( \\{ \\omega_3) \\}) = P( \\{ \\omega_4) \\}) = P( \\{ \\omega_5)\\}) = P( \\{ \\omega_6) \\}) = \\frac{1}{6}$. (If your die wasn't fair, these numbers could be different.)\nWhat's the mean of $X$?\n\\begin{align*}\n\\operatorname{E}[X] = \\sum_{\\omega \\in \\Omega} X(\\omega) P( \\{ \\omega \\} ) &= 1 \\frac{1}{6} + 2\\frac{1}{6} + 3 \\frac{1}{6} + 4 \\frac{1}{6} + 5 \\frac{1}{6} + 6 \\frac{1}{6}\\\\\n&= \\frac{7}{2}\n\\end{align*}\nWhat's the mean of $Y$?\n\\begin{align*}\n\\operatorname{E}[Y] = \\sum_{\\omega \\in \\Omega} X(\\omega) P( \\{ \\omega \\} ) &= 1 \\frac{1}{6} + 4\\frac{1}{6} + 9 \\frac{1}{6} + 16 \\frac{1}{6} + 25 \\frac{1}{6} + 36 \\frac{1}{6}\\\\\n&= \\frac{91}{6}\n\\end{align*}\nWhat's the covariance of $X$ and $Y$?\n\\begin{align*}\n\\sum_{\\omega \\in \\Omega} \\left(X(\\omega) - \\frac{7}{2}\\right)\\left( Y(\\omega) - \\frac{91}{6}\\right) P( \\{ \\omega \\} ) &= \\left( 1 - \\frac{7}{2} \\right)\\left( 1 - \\frac{91}{6} \\right) P(\\{\\omega_1\\}) + \\left( 2 - \\frac{7}{2} \\right)\\left( 4 - \\frac{91}{6} \\right) P(\\{\\omega_2\\}) + \\left( 3 - \\frac{7}{2} \\right)\\left( 9 - \\frac{91}{6} \\right) P(\\{\\omega_3\\}) + \\left( 4 - \\frac{7}{2} \\right)\\left( 16 - \\frac{91}{6} \\right) P(\\{\\omega_4\\}) + \\left( 5 - \\frac{7}{2} \\right)\\left( 25 - \\frac{91}{6} \\right) P(\\{\\omega_5\\}) + \\left( 6 - \\frac{7}{2} \\right)\\left( 36 - \\frac{91}{6} \\right) P(\\{\\omega_6\\}) \\\\\n&\\approx 20.4167\n\\end{align*}\nDon't worry about the arithmetic. The point is that to calculate $\\operatorname{Cov}\\left(X , Y\\right) = \\operatorname{E}\\left[(X -\\operatorname{E}[X])(Y - \\operatorname{E}[Y])  \\right] = \\sum_{\\omega \\in \\Omega} \\left(X(\\omega) - \\operatorname{E}[X]\\right)\\left( Y(\\omega) - \\operatorname{E}[Y]\\right) P( \\{ \\omega \\} ) $ you sum over the 6 possible outcomes $\\omega_1, \\ldots, \\omega_6$.\nBack to your situation...\nThe possible outcomes in your sample are $i=1\\, \\ldots, n$. Those are the outcomes you should sum over.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/266856", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "How can we apply the rule of stationary distribution to the continuous case of Markov chain? If the Markov chain converged then $$\\pi = Q* \\pi$$where $ \\pi$ is the posterior distribution and $Q$ is the transition distribution(it's a matrix in the discrete case).\nI tried to apply that on the continuous case of Markov chain in this example where\n the transition distribution is:\n$$p(X_{t+1} | X_t=x_t) = \\text{N}(\\phi x_t, 1)$$\nand the posterior(stationary) distribution  is\n$$X_t \\sim \\text{N} \\Bigg( 0, \\frac{1}{1-\\phi^2} \\Bigg)$$\nThe product of them both doesn't equal the posterior.   \n", "A": "That stationary distribution is correct.  Using the law of total probability, you have:\n$$\\begin{equation} \\begin{aligned}\np(T_{t+1} = x) \n&= \\int \\limits_\\mathbb{R} p(X_{t+1} = x | X_t = r) \\cdot p(X_t = r) \\ dr \\\\[6pt]\n&= \\int \\limits_{-\\infty}^\\infty \\text{N}(x | \\phi r, 1) \\cdot \\text{N} \\bigg( r \\bigg| 0, \\frac{1}{1-\\phi^2} \\bigg) \\ dr \\\\[6pt]\n&= \\int \\limits_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\exp \\bigg( -\\frac{1}{2} (x - \\phi r)^2 \\bigg) \\cdot \\sqrt{\\frac{1-\\phi^2}{2 \\pi}} \\exp \\bigg( -\\frac{1}{2} (1-\\phi^2) r^2 \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{2 \\pi} \\int \\limits_{-\\infty}^\\infty \\exp \\bigg( - \\frac{1}{2} (x - \\phi r)^2 - \\frac{1}{2} (1-\\phi^2) r^2 \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{2 \\pi} \\int \\limits_{-\\infty}^\\infty \\exp \\bigg( - \\frac{1}{2} \\bigg[ (x - \\phi r)^2 + (1-\\phi^2) r^2 \\bigg] \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{2 \\pi} \\int \\limits_{-\\infty}^\\infty \\exp \\bigg( - \\frac{1}{2} \\bigg[ x^2 - 2 \\phi x r + \\phi^2 r^2 + r^2 - \\phi^2 r^2 \\bigg] \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{2 \\pi} \\int \\limits_{-\\infty}^\\infty \\exp \\bigg( - \\frac{1}{2} \\bigg[ x^2 - 2 \\phi x r + r^2 \\bigg] \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{2 \\pi} \\int \\limits_{-\\infty}^\\infty \\exp \\bigg( - \\frac{1}{2} \\bigg[ x^2 (1 - \\phi^2) + (r-\\phi x)^2 \\bigg] \\bigg) \\ dr \\\\[6pt]\n&= \\frac{\\sqrt{1-\\phi^2}}{\\sqrt{2 \\pi}} \\exp \\bigg( -\\frac{1}{2} (1-\\phi^2) x^2 \\bigg) \\int \\limits_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\exp \\bigg( - \\frac{1}{2} (r-\\phi x)^2 \\bigg) \\ dr \\\\[6pt]\n&= \\text{N} \\bigg( x \\bigg| 0, \\frac{1}{1-\\phi^2} \\bigg) \\times \\int \\limits_{-\\infty}^\\infty \\text{N} (r|\\phi x,1) \\ dr \\\\[6pt]\n&= \\text{N} \\bigg( x \\bigg| 0, \\frac{1}{1-\\phi^2} \\bigg). \\\\[6pt]\n\\end{aligned} \\end{equation}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/423484", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
