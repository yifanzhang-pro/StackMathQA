{"Q": "Calculation of an \"unconstrained\" normal distribution (starting from a censored one) Assume that two r.v. $W$ and $Y|W=w$ with\n(1) $W \\sim \\text{N}(\\mu_w,\\sigma_w^2)$ (iid)\n(2) $Y|W=w \\sim \\text{N}(w,\\sigma_y^2)$ (iid)\nFurther we only observe $Y$ if $Y$ is less then $W$, i.e., \n(3) $Y|Y\\le W$\nGoal: Find the pdf of the censored observations, i.e., of $Y|Y\\le W$ and from that deduce the uncensored pdf and the first two moments (so i.m.h.o. we have to find$f_Y(y)$). The first two moments of this uncensored pdf are supposed to depend upon $E(Y|Y\\le W)$ and $Var(Y|Y\\le W)$.\n\nBy definition of conditional pdf we have that:\n(4) $f_{Y|W}(y|W = w)= \\frac{f_{Y,W}(y,w)}{f_W(w)}$ \nNext, the definition of a truncated density gives for a abitrary value of $W$:\n(5) $ f_{Y|Y\\le W}(y|y\\le w) = \\frac{f_Y(y)}{P(Y\\le W)}$\n\nI would simply rewrite (4) to\n$f_{Y|W}(y|W = w)f_W(w) = f_{Y,W}(y,w)$\nthen integration over $f_{Y,W}(y,w)$ w.r.t $w$ should yield $f_Y(y)$, i.e.,\n(a) $\\int_{-\\infty}^{\\infty} f_{Y,W}(y,w) dw = \\int_{-\\infty}^{\\infty} f_Y(y|W = w)f_W(w) dw = f_Y(y)$\nPlugin in $f_Y(y)$ into (5), ($P(Y\\le W)$ will also be given by $f_Y(y)$) I will se how the moments of $f_{Y|Y\\le W}(y|y\\le w)$ will look and how the moments of $f_Y(y)$ depend upon them.\nSo (a) will look like\n$f_Y(y) = \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}\\text{exp}\\big(-\\frac{(y-w)^2}{2\\sigma_y^2}\\big)\\frac{1}{\\sqrt{2\\pi\\sigma^2_w}}\\text{exp}\\big(-\\frac{(w-\\mu_w)^2}{2\\sigma_w^2}\\big)dw$\nExcept for the $w$ in the first $\\text{exp}$, this looks very easy but since there is a $w$ im a little bit stuck how to solve this... \n", "A": "Ok. Let's do this, for CV's shake.\nFirst compact by setting $C=\\frac{1}{\\sqrt{2\\pi\\sigma^2_y}}\\frac{1}{\\sqrt{2\\pi\\sigma^2_w}} = \\frac{1}{2\\pi\\sigma_y\\sigma_w}$, so \n$$f_Y(y) =C \\int_{-\\infty}^{\\infty}\\exp\\left\\{-\\frac{(y-w)^2}{2\\sigma_y^2}\\right\\}\\exp\\left\\{-\\frac{(w-\\mu_w)^2}{2\\sigma_w^2}\\right\\}dw$$\nWe have \n$$exp\\left\\{-\\frac{(y-w)^2}{2\\sigma_y^2}\\right\\}\\exp\\left\\{-\\frac{(w-\\mu_w)^2}{2\\sigma_w^2}\\right\\} = \n\\exp\\left\\{-\\frac{y^2-2yw+w^2}{2\\sigma_y^2}\\right\\}\\exp\\left\\{-\\frac{w^2-2w\\mu_w+\\mu_w^2}{2\\sigma_w^2}\\right\\} \n=\\exp\\left\\{-\\frac{y^2}{2\\sigma_y^2}-\\frac{\\mu_w^2}{2\\sigma_w^2}\\right\\} \\exp\\left\\{-\\frac{w^2}{2\\sigma_y^2}-\\frac{w^2}{2\\sigma_w^2}\\right\\}\\exp\\left\\{\\frac{2yw}{2\\sigma_y^2}+\\frac{2w\\mu_w}{2\\sigma_w^2}\\right\\}$$\nSetting $s^2\\equiv \\sigma_y^2+\\sigma_w^2$ we arrive at\n$$=\\exp\\left\\{-\\frac{y^2}{2\\sigma_y^2}-\\frac{\\mu_w^2}{2\\sigma_w^2}\\right\\} \\exp\\left\\{-\\frac{s^2}{2\\sigma_y^2\\sigma_w^2}w^2\\right\\}\\exp\\left\\{\\frac{\\sigma_w^2y+\\sigma_y^2\\mu_w}{\\sigma_y^2\\sigma_w^2}w\\right\\}$$\nInclude the first $\\exp$ in the constant, $C^*=C \\exp\\left\\{-\\frac{y^2}{2\\sigma_y^2}-\\frac{\\mu_w^2}{2\\sigma_w^2}\\right\\}$.\nSet\n$$\\beta\\equiv \\frac{s^2}{2\\sigma_y^2\\sigma_w^2},\\qquad \\alpha\\equiv \\frac{\\sigma_w^2y+\\sigma_y^2\\mu_w}{\\sigma_y^2\\sigma_w^2}$$ to obtain\n$$f_Y(y) =C^* \\int_{-\\infty}^{\\infty}e^{-\\beta w^2+\\alpha w}dw=C^*\\left[ \\int_{-\\infty}^{0}e^{-\\beta w^2+\\alpha w}dw + \\int_{0}^{\\infty}e^{-\\beta w^2+\\alpha w}dw\\right]$$\n$$=C^* \\int_{0}^{\\infty}e^{-\\beta w^2}\\left[e^{-\\alpha w}+e^{\\alpha w}\\right]dw =2C^* \\int_{0}^{\\infty}e^{-\\beta w^2}\\operatorname{cosh}(\\alpha w)dw$$\nwhere $\\operatorname{cosh}$ is the hyperbolic cosine.\nUsing a formula provided in Gradshteyn & Ryzhik (2007), \"Table of Integrals, Series and Products\", 7th ed., p. 384, eq. 3.546(2) we have\n$$f_Y(y)=2C^*\\frac 12 \\sqrt {\\frac {\\pi}{\\beta}} \\exp\\left\\{\\frac {\\alpha^2}{4\\beta}\\right\\}$$\nNow $$\\frac {\\alpha^2}{4\\beta} = \\frac {\\left(\\frac{\\sigma_w^2y+\\sigma_y^2\\mu_w}{\\sigma_y^2\\sigma_w^2}\\right)^2}{4\\frac{s^2}{2\\sigma_y^2\\sigma_w^2}} = \\frac {(\\sigma_w^2y+\\sigma_y^2\\mu_w)^2}{2\\sigma_y^2\\sigma_w^2s^2}$$ \nand bringing back in $C^*$ (and $\\beta$) in all its glory we have\n$$f_Y(y)=\\frac{1}{2\\pi\\sigma_y\\sigma_w}\\exp\\left\\{-\\frac{y^2}{2\\sigma_y^2}-\\frac{2\\mu_w^2}{\\sigma_w^2}\\right\\}\\sqrt{\\pi} \\left(\\sqrt {\\frac{s^2}{2\\sigma_y^2\\sigma_w^2}}\\right)^{-1} \\exp\\left\\{\\frac {(\\sigma_w^2y+\\sigma_y^2\\mu_w)^2}{2\\sigma_y^2\\sigma_w^2s^2}\\right\\} $$\nThe constant terms simplify to \n$$\\frac{1}{2\\pi\\sigma_y\\sigma_w}\\sqrt{\\pi} \\left(\\sqrt {\\frac{s^2}{2\\sigma_y^2\\sigma_w^2}}\\right)^{-1} = \\frac{1}{s\\sqrt{2\\pi}} $$\nand, the exponentials end up in the normal exponential. So in the end\n$$f_Y(y) = \\frac{1}{s\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{(y-\\mu_w)^2}{2s^2}\\right\\}= N(\\mu_w, s^2),\\qquad s^2\\equiv \\sigma_y^2+\\sigma_w^2$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/73157", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 0}}
{"Q": "Show that the value is, indeed, the MLE Let $ X_1, ... X_n$ i.i.d with pdf\n$$f(x;\\theta)=\\frac{x+1}{\\theta(\\theta+1)}\\exp(-x/\\theta), x>0, \\theta >0$$\nIt is asked to find the MLE estimator for $\\theta.$\nThe likelihood function is given by\n$$L(\\theta;x)=[\\theta(1-\\theta)]^{-n}\\exp\\left(\\frac{\\sum_i x_i}{\\theta}\\right)\\prod_i (x_i+1)I_{(0,\\infty)}(x_i)$$\nThen, the derivative of log-likelihood will be\n$$\\frac{dlogL(\\theta;x)}{d\\theta}=\\frac{-(2\\theta+1)n}{\\theta(1+\\theta)} + \\frac{\\sum X_i}{\\theta^2}$$\nI've obtained my candidate to MLE. But, doing the second derivative of the log-likelihood, I could not conclude that it is negative so that the candidate is, indeed, the point of maximum. What should I do, in this case?\n", "A": "Removing the multiplicative constants that do not depend on $\\theta$, the likelihood function in this case is:\n$$\\begin{equation} \\begin{aligned}\nL_\\mathbf{x}(\\theta) \n&= \\prod_{i=1}^n \\frac{1}{\\theta (\\theta+1)} \\cdot \\exp \\Big( - \\frac{x_i}{\\theta} \\Big) \\\\[6pt]\n&= \\frac{1}{\\theta^n (\\theta+1)^n} \\cdot \\exp \\Big( - \\frac{n \\bar{x}}{\\theta} \\Big). \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nThus, the log-likelihood function is:\n$$\\ell_\\mathbf{x}(\\theta) \\equiv \\ln L_\\mathbf{x}(\\theta) = -n \\Bigg[ \\ln(\\theta) + \\ln(\\theta+1) + \\frac{\\bar{x}}{\\theta} \\Bigg] \\quad \\quad \\quad \\text{for all } \\theta >0.$$\nI will write the derivatives of this function out in a form that is useful for finding the critical points and then finding the second derivative at those critical points:\n$$\\begin{equation} \\begin{aligned}\n\\frac{d \\ell_\\mathbf{x}}{d \\theta}(\\theta)\n&= -n \\Bigg[ \\frac{1}{\\theta} + \\frac{1}{\\theta+1} - \\frac{\\bar{x}}{\\theta^2} \\Bigg] \\\\[6pt]\n&= -n \\Bigg[  \\frac{2\\theta + 1}{\\theta(\\theta+1)} - \\frac{\\bar{x}}{\\theta^2} \\Bigg] \\\\[6pt]\n&= - \\frac{n}{\\theta^2} \\Bigg[ \\frac{(2\\theta + 1)\\theta}{\\theta+1} - \\bar{x} \\Bigg], \\\\[6pt]\n\\frac{d^2 \\ell_\\mathbf{x}}{d \\theta^2}(\\theta)\n&= -n \\Bigg[ - \\frac{1}{\\theta^2} - \\frac{1}{(\\theta+1)^2} + \\frac{2\\bar{x}}{\\theta^3} \\Bigg] \\\\[6pt]\n&= -n \\Bigg[ - \\frac{(\\theta+1)^2 + \\theta^2}{\\theta^2 (\\theta+1)^2} + \\frac{2\\bar{x}}{\\theta^3} \\Bigg] \\\\[6pt]\n&= -n \\Bigg[ - \\frac{2 \\theta^2 + 2 \\theta + 1}{\\theta^2 (\\theta+1)^2} + \\frac{2\\bar{x}}{\\theta^3} \\Bigg] \\\\[6pt]\n&= -n \\Bigg[ \\frac{(2\\theta+1)(\\theta+1) - \\theta}{\\theta^2 (\\theta+1)^2} - \\frac{2\\bar{x}}{\\theta^3} \\Bigg] \\\\[6pt]\n&= \\frac{n}{\\theta^3} \\Bigg[ \\frac{[(2\\theta+1) - \\theta]\\theta}{\\theta+1} - 2\\bar{x} \\Bigg] \\\\[6pt]\n&= \\frac{n}{\\theta^3} \\Bigg[ \\frac{(2\\theta+1) \\theta}{\\theta+1} - \\bar{x} - \\frac{\\theta^2}{\\theta+1} - \\bar{x} \\Bigg] \\\\[6pt]\n&= - \\frac{1}{\\theta} \\frac{d \\ell_\\mathbf{x}}{d \\theta}(\\theta) - \\frac{n}{\\theta^3} \\Bigg( \\frac{\\theta^2}{\\theta+1} + \\bar{x} \\Bigg). \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nFrom this form, we see that at any critical point we must have:\n$$\\begin{equation} \\begin{aligned}\n\\frac{d^2 \\ell_\\mathbf{x}}{d \\theta^2}(\\theta)\n&= - \\frac{n}{\\theta^3} \\Bigg( \\frac{\\theta^2}{\\theta+1} + \\bar{x} \\Bigg) < 0. \\\\[6pt]\n\\end{aligned} \\end{equation}$$\nSince every critical point is a local maximum, this means that there is a unique critical point that is the MLE of the function.  Thus, the MLE is obtained by solving the equation:\n$$\\bar{x} = \\frac{(2 \\hat{\\theta} + 1) \\hat{\\theta}}{\\hat{\\theta} + 1}\n\\quad \\quad \\quad \\implies \\quad \\quad \\quad \n2 \\hat{\\theta}^2 + \\hat{\\theta} (1-\\bar{x}) - \\bar{x} = 0.$$\nThis is a quadratic equation with explicit solution:\n$$\\hat{\\theta} = \\frac{1}{4} \\Bigg[ \\bar{x} - 1 + \\sqrt{\\bar{x}^2 + 6 \\bar{x} + 1} \\Bigg].$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/115962", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 3, "answer_id": 2}}
{"Q": "Probability of two random variables being equal The question is as follows:\n\nLet $X_1$ ~ Binomial(3,1/3) and $X_2$ ~ Binomial(4,1/2) be independent random variables. Compute P($X_1$ = $X_2$)\n\nI'm not sure what it means to compute the probability of two random variables being equal.\n", "A": "Let $Z=X_1-X_2$\n$P(Z=z)=\\sum_{x_1=z+x_2}^\\infty P(X_1=x,X_2=x-z)$ (since $X_1$ and $X_2$ are independent)\n$P(Z=z)=\\sum_{x_1=z+x_2}^\\infty P(X_1=x)P(X_2=x-z)\\\\=\\sum_{x_1=z+x_2}^\\infty \\binom{3}{x}(\\frac{1}{3})^x(\\frac{2}{3})^{3-x}\\binom{4}{4-x+z}(\\frac{1}{2})^{x-z}(\\frac{1}{2})^{4-x+z}$\nWhen $X_1=X_2\\Rightarrow z=0 $\nTherefore\n$P(Z=0)=\\sum_{x_1=x_2}^\\infty \\binom{3}{x}(\\frac{1}{3})^x(\\frac{2}{3})^{3-x}\\binom{4}{4-x+0}(\\frac{1}{2})^{x-0}(\\frac{1}{2})^{4-x+0}\\\\=\\binom{3}{0}(\\frac{1}{3})^0(\\frac{2}{3})^{3-0}\\binom{4}{4-0+0}(\\frac{1}{2})^{0-0}(\\frac{1}{2})^{4-0+0}  \\quad (x_1=x_2=0)\\\\+\\binom{3}{1}(\\frac{1}{3})^1(\\frac{2}{3})^{3-1}\\binom{4}{4-1+0}(\\frac{1}{2})^{1-0}(\\frac{1}{2})^{4-1+0}\\quad(x_1=x_2=1)\\\\+\\binom{3}{2}(\\frac{1}{3})^2(\\frac{2}{3})^{3-2}\\binom{4}{4-2+0}(\\frac{1}{2})^{2-0}(\\frac{1}{2})^{4-2+0}\\quad(x_1=x_2=2)\\\\+\\binom{3}{3}(\\frac{1}{3})^3(\\frac{2}{3})^{3-3}\\binom{4}{4-3+0}(\\frac{1}{2})^{3-0}(\\frac{1}{2})^{4-3+0}\\quad(x_1=x_2=3)$\nOk, you can add up them to get the probability.\nAnd remember for binomial distribution the random variable is total number of success. In your case $X_1$ and $X_2$ can be both 0, 1,2,3 for the total number of success of the two distribution.This answer in fact the same as the answer by Daniel and comment by ACE.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/182691", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 0}}
{"Q": "A GENERAL inequality for a bi-modal hypergeometric distribution Say $X$ has a hypergeometric distribution with parameters $m$, $n$ and $k$, with $k\\leq n<\\frac12m$.\nI know that $X$ has a dual mode if and only if $d=\\frac{(k+1)(n+1)}{m+2}$ is integer. In that case $P(X=d)=P(X=d-1)$ equals the maximum probability.\nSee my previous question. I got a great answer proving $P(X=d+1) > P(X=d-2)$. That got me wondering: can we make a more general statement? More specifically (for natural $c \\leq d-2$):\n$P(X=d+c) > P(X=d-1-c)$\nThis is true for $c = 1$, but also in many cases when $c \\geq 2$. I have not found any counterexamples yet. Can this be proven? Or where to start?\n", "A": "You can turn the answer from the other question into an inductive proof for this question*.\n$$\\tfrac{P(X=d+c+1)}{P(X=d+c)}-\\tfrac{P(X=d-c-2)}{P(X=d-c-1)} = \\tfrac{(k-d-c)(n-d-c)}{(d+1+c) (m-k-n+d+1+c)} -\\tfrac{ (d-c-1) (m-k-n+d-c-1)}{(k-d+c+2)(n-d+c+2)} \\\\= \\tfrac{(k-d-c)(n-d-c)(k-d+c+2)(n-d+c+2)-(d-c-1) (m-k-n+d-c-1)(d+1+c) (m-k-n+d+1+c)}{(d+1+c) (m-k-n+d+1+c)(k-d+c+2)(n-d+c+2)}$$\nagain the denominator is positive, and we only need to show that the numerator is positive.\nWe can do the same steps, substituting $d=(k+1)(n+1)/(m+2)$ gives for the numerator:\n$$(c+1)^2(m-2k)(m-2n)$$\nwhich is positive when both $k< \\frac{1}{2}m$ and $n < \\frac{1}{2}m$.\n\nSome other interesting points\n\n\n*\n\n*For $c = 0$ you get the previous answer. \n\n*For $c=-1$ you get $\\frac{P(X=d)}{P(X=d-1)}-\\frac{P(X=d-1)}{P(X=d)} = 0$, which is true by the assumption $P(X=d) = P(X=d-1)$.\n\n*Also for $n=\\frac{1}{2}m$ you get that the term $(m-2n)$ equals zero and you get the symmetry $P(X=d+c) = P(X=d-c-1)$ \n\n*If $\\tfrac{P(X=d+c+1)}{P(X=d+c)}-\\tfrac{P(X=d-c-2)}{P(X=d-c-1)}> 0$ and $$P(X=d+c) \\geq P(X=d-1-c)$$ then $P(X=d+(c+1)) > P(X=d-1-(c+1))$ \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/459012", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Distribution of the pooled variance in paired samples Suppose a bivariate normal populations with means $\\mu_1$ and $\\mu_2$ and equal variance $\\sigma^2$ but having a correlation of $\\rho$.\nTaking a paired sample, it is possible to compute the pooled variance. If $S^2_1$ and $S^2_2$ are the sample variance of the first elements of the pairs and the second elements of the pairs respectively, then, let's note $S_p^2 = \\frac{S^2_1+S^2_2}{2}$ the pooled variance (equivalent to the mean of the variances as the samples sizes are the same for the first elements and the second elements).\nMy question is: how can we demonstrate that the distribution of $S_p^2 / \\sigma^2 \\approx \\chi^2_\\nu / \\nu$ with $\\nu$ the degree of freedom equal to $2(n-1)/(1+\\rho^2)$?\nIf this result is well known, what reference provided the original demonstration?\n", "A": "I'm not sure about a reference for this result, but it is possible to derive it relatively easily, so I hope that suffices.  One way to approach this problem is to look at it as a problem involving a quadratic form taken on a normal random vector.  The pooled sample variance can be expressed as a quadratic form of this kind, and these quadratic forms are generally approximated using the chi-squared distribution (with exact correspondence in some cases).\n\nDerivation of the result: In order to show where your assumptions come into the derivation, I will do the first part of the derivation without assuming equal variances for the two groups.  If we denote your vectors by $\\mathbf{X} = (X_1,...,X_n)$ and $\\mathbf{Y} = (Y_1,...,Y_n)$ then your stipulated problem gives the joint normal distribution:\n$$\\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix} \n\\sim \\text{N} (\\boldsymbol{\\mu}, \\mathbf{\\Sigma} )\n\\quad \\quad \\quad\n\\boldsymbol{\\mu} =\n\\begin{bmatrix} \\mu_X \\mathbf{1} \\\\ \\mu_Y \\mathbf{1} \\end{bmatrix}\n\\quad \\quad \\quad\n\\mathbf{\\Sigma} =\n\\begin{bmatrix} \\sigma_X^2 \\mathbf{I} & \\rho \\sigma_X \\sigma_Y \\mathbf{I} \\\\\n\\rho \\sigma_X \\sigma_Y \\mathbf{I} & \\sigma_Y^2 \\mathbf{I} \\end{bmatrix}.$$\nLetting $\\mathbf{C}$ denote the $n \\times n$ centering matrix, you can write the pooled sample variance in this problem as the quadratic form:\n$$\\begin{align}\nS_\\text{pooled}^2\n&= \\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix}^\\text{T}\n\\mathbf{A} \\begin{bmatrix} \\mathbf{X} \\\\ \\mathbf{Y} \\end{bmatrix}\n\\quad \\quad \\quad\n\\mathbf{A} \\equiv \\frac{1}{2(n-1)} \\begin{bmatrix} \\mathbf{C} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{C} \\end{bmatrix}. \\\\[6pt]\n\\end{align}$$\nNow, using standard formulae for the mean and variance of quadradic forms of normal random vectors, and noting that $\\mathbf{C}$ is an idempotent matrix (i.e., $\\mathbf{C} = \\mathbf{C}^2$), you have:\n$$\\begin{align}\n\\mathbb{E}(S_\\text{pooled}^2) \n&= \\text{tr}(\\mathbf{A} \\mathbf{\\Sigma}) + \\boldsymbol{\\mu}^\\text{T} \\mathbf{A} \\boldsymbol{\\mu} \\\\[6pt]\n&= \\text{tr} \\Bigg( \\frac{1}{2(n-1)} \\begin{bmatrix} \\sigma_X^2 \\mathbf{C} & \\rho \\sigma_X \\sigma_Y \\mathbf{C} \\\\ \\rho \\sigma_X \\sigma_Y \\mathbf{C} & \\sigma_Y^2 \\mathbf{C} \\end{bmatrix} \\Bigg) + \\mathbf{0} \\\\[6pt]\n&= \\frac{1}{2(n-1)} \\text{tr} \\Bigg( \\begin{bmatrix} \\sigma_X^2 \\mathbf{C} & \\rho \\sigma_X \\sigma_Y \\mathbf{C} \\\\ \\rho \\sigma_X \\sigma_Y \\mathbf{C} & \\sigma_Y^2 \\mathbf{C} \\end{bmatrix} \\Bigg) \\\\[6pt]\n&= \\frac{1}{2(n-1)} \\Bigg[ n \\times \\frac{n-1}{n} \\cdot \\sigma_X^2 + n \\times \\frac{n-1}{n} \\cdot \\sigma_Y^2 \\Bigg] \\\\[6pt]\n&= \\frac{\\sigma_X^2 + \\sigma_Y^2}{2}, \\\\[12pt]\n\\mathbb{V}(S_\\text{pooled}^2) \n&= 2 \\text{tr}(\\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A} \\mathbf{\\Sigma}) + 4 \\boldsymbol{\\mu}^\\text{T} \\mathbf{A} \\mathbf{\\Sigma} \\mathbf{A} \\boldsymbol{\\mu} \\\\[6pt]\n&= 2 \\text{tr} \\Bigg( \\frac{1}{4(n-1)^2} \\begin{bmatrix} \\sigma_X^2 \\mathbf{C} & \\rho \\sigma_X \\sigma_Y \\mathbf{C} \\\\ \\rho \\sigma_X \\sigma_Y \\mathbf{C} & \\sigma_Y^2 \\mathbf{C} \\end{bmatrix}^2 \\Bigg) + \\mathbf{0} \\\\[6pt]\n&= \\frac{1}{2(n-1)^2} \\text{tr} \\Bigg( \\begin{bmatrix} (\\sigma_X^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2) \\mathbf{C} & (\\sigma_X^2 + \\sigma_Y^2) \\rho \\sigma_X \\sigma_Y \\mathbf{C} \\\\ (\\sigma_X^2 + \\sigma_Y^2) \\rho \\sigma_X \\sigma_Y \\mathbf{C} & (\\sigma_Y^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2)  \\mathbf{C} \\end{bmatrix} \\Bigg) \\\\[6pt]\n&= \\frac{1}{2(n-1)^2} \\Bigg[ n \\times \\frac{n-1}{n} \\cdot (\\sigma_X^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2) + n \\times \\frac{n-1}{n} \\cdot (\\sigma_Y^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2) \\Bigg] \\\\[6pt]\n&= \\frac{1}{2(n-1)} \\Bigg[ (\\sigma_X^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2) + (\\sigma_Y^4 + \\rho^2 \\sigma_X^2 \\sigma_Y^2) \\Bigg] \\\\[6pt]\n&= \\frac{\\sigma_X^4 + \\sigma_Y^4 + 2 \\rho^2 \\sigma_X^2 \\sigma_Y^2}{2(n-1)}. \\\\[12pt]\n\\end{align}$$\nUsing the equal variance assumption we have $\\sigma_X = \\sigma_Y = \\sigma$ so the moments reduce to:\n$$\\mathbb{E} \\bigg( \\frac{S_\\text{pooled}^2}{\\sigma^2} \\bigg) = 1\n\\quad \\quad \\quad\n\\mathbb{V} \\bigg( \\frac{S_\\text{pooled}^2}{\\sigma^2} \\bigg) = \\frac{1+\\rho^2}{n-1}.$$\nIt is usual to approximate the distribution of the quadratic form by a scaled chi-squared distribution using the method of moments.  Equating the first two moments to that distribution gives the variance requirement $\\mathbb{V}(S_\\text{pooled}^2/\\sigma^2) = 2/\\nu$, which then gives the degrees-of-freedom parameter:\n$$\\nu = \\frac{2(n-1)}{1+\\rho^2}.$$\nBear in mind that the degrees-of-freedom parameter here depends on the true correlation coefficient $\\rho$, and you may need to estimate this using the sample correlation in your actual problem.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/482118", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "9", "answer_count": 1, "answer_id": 0}}
{"Q": "Mean (or lower bound) of Gaussian random variable conditional on sum, $E(X^2| k \\geq|X+Y|)$ Suppose I have two mean zero, independent Gaussian random variables\n$X \\sim \\mathcal{N}(0,\\sigma_1^2)$ and\n$Y \\sim \\mathcal{N}(0,\\sigma_2^2)$.\nCan I say something about the conditional expectation $E(X^2|  k  \\geq|X+Y|)$?\nI think the expectation should be given by the double integral\n$$ E(X^2|  k  \\geq|X+Y|) =\\frac{\\int_{y=-\\infty}^\\infty \\int_{x= y - k}^{k-y} x^2 e^{-\\frac{x^2}{2\\sigma^2_1}}e^{-\\frac{y^2}{2\\sigma^2_2}} dx dy}{\\int_{y=-\\infty}^\\infty \\int_{x= y - k}^{k-y}  e^{-\\frac{x^2}{2\\sigma^2_1}}e^{-\\frac{y^2}{2\\sigma^2_2}} dx dy} \\,.$$\nIs it possible to get an exact expression or a lower bound for this expectation?\nEdit:\nBased on your comments I was able to get an intermediate expression for the nominator and the denominator.\nDenominator:\nIt is well known that if $X \\sim \\mathcal{N}(0,\\sigma_1^2)$ and\n$Y \\sim \\mathcal{N}(0,\\sigma_2^2)$ and $ X \\perp Y$, then $X + Y \\sim \\mathcal{N}(0,\\sigma_1^2 + \\sigma_2^2)$ and therefore\n\\begin{equation*}\n  \\begin{aligned}\n   \\Pr(|X+Y| \\leq k) &= \\Phi \\left( \\frac{k}{\\sigma_1 + \\sigma_2} \\right) - \\Phi \\left( \\frac{-k}{\\sigma_1 + \\sigma_2} \\right)\n  \\end{aligned}\n \\end{equation*}\nso that\n\\begin{equation*}\n  \\begin{aligned}\n   \\int_{y=-\\infty}^\\infty \\int_{x= y - k}^{k-y}  e^{-\\frac{x^2}{2\\sigma^2_1}}e^{-\\frac{y^2}{2\\sigma^2_2}} dx dy &= 2 \\pi \\sigma_1 \\sigma_2 \\Pr(|X+Y| \\leq k) \\\\ \n&= 2 \\pi \\sigma_1 \\sigma_2 \\left\\{\\Phi \\left( \\frac{k}{\\sigma_1 + \\sigma_2} \\right) - \\Phi \\left( \\frac{-k}{\\sigma_1 + \\sigma_2} \\right) \\right\\}\n  \\end{aligned}\n \\end{equation*}\nNominator:\n\\begin{equation*}\n  \\begin{aligned}\n   \\int_{y=-\\infty}^\\infty \\int_{x= y - k}^{k-y} x^2 e^{-\\frac{x^2}{2\\sigma^2_1}}e^{-\\frac{y^2}{2\\sigma^2_2}} dx dy & = \\int_{y=-\\infty}^\\infty 2\\int_{x= 0}^{k-y} x^2 e^{-\\frac{x^2}{2\\sigma^2_1}}e^{-\\frac{y^2}{2\\sigma^2_2}} dx dy, \\quad (-x)^2 = x^2 \\\\\u00a0\n    & = \\int_{y=-\\infty}^\\infty (2\\sigma_1)^{\\frac{3}{2}}\\int_{u= 0}^{\\frac{(k-y)^2}{2 \\sigma_1^2}} u^{\\frac{3}{2}-1} e^{-u }e^{-\\frac{y^2}{2\\sigma^2_2}} du dy, \\quad u = \\frac{x^2}{2 \\sigma_1^2} \\\\\u00a0\n    & = \\int_{y=-\\infty}^\\infty (2\\sigma_1)^{\\frac{3}{2}}\\Gamma\\left(\\frac{3}{2},\\frac{(k-y)^2}{2 \\sigma_1^2} \\right) e^{-\\frac{y^2}{2\\sigma^2_2}} dy, \\quad \\Gamma(s,x) = \\int_0^x t^{s-1} e^t dt \\\\\u00a0\n    & = (2\\sigma_1)^{\\frac{3}{2}} \\sqrt{2}\\sigma_2 \\int_{v=-\\infty}^\\infty \\Gamma\\left(\\frac{3}{2},\\frac{\\sigma_2^2}{\\sigma_1^2}\\left(\\frac{k}{\\sqrt{2}\\sigma_2}-v\\right)^2 \\right) e^{-v^2} dv, \\quad v = \\frac{y}{\\sqrt{2}\\sigma_2} \\\\\u00a0\n    & \\geq 4 \\sigma_1^{\\frac{3}{2}}\\sigma_2 \\Gamma\\left(\\frac{3}{2}\\right) \\int_{v=-\\infty}^\\infty\\left(1 + \\frac{2}{3}\\frac{\\sigma_2^2}{\\sigma_1^2}\\left(\\frac{k}{\\sqrt{2}\\sigma_2}-v\\right)^2 \\right)^{\\frac{1}{2}} e^{-v(v+1)}  dv\n  \\end{aligned}\n \\end{equation*}\nwhere the last inequality uses the bound from this post. Any ideas how ti simplify this further to get a nontrivial lower bound on the conditional expectation $E(X^2| k \\geq|X+Y|)$ are much appreciated.\n", "A": "Let's simplify a little.  Define\n$$(U,V) = \\frac{1}{\\sqrt{\\sigma_X^2+\\sigma_Y^2}}\\left(X+Y,\\ \\frac{\\sigma_Y}{\\sigma_X}X - \\frac{\\sigma_X}{\\sigma_Y}Y\\right).$$\nYou can readily check that $U$ and $V$ are uncorrelated standard Normal variables (whence they are independent). In terms of them,\n$$X = \\frac{\\sigma_X}{\\sqrt{\\sigma_X^2 + \\sigma_Y^2}} \\left(\\sigma_X U + \\sigma_Y V\\right) = \\alpha U + \\beta V$$\ndefines the coefficients of $X$ in terms of $(U,V).$  The question desires a formula for\n$$E\\left[X^2 \\mid |X+Y|\\ge k\\right] = E\\left[\\left(\\alpha U + \\beta V\\right)^2 \\mid |U| \\ge \\lambda\\right]$$\nwith $\\lambda =  k\\sqrt{\\sigma_X^2 + \\sigma_Y^2} \\ge 0.$\nExpanding the square, we find\n$$\\begin{aligned}\nE\\left[\\left(\\alpha U + \\beta V\\right)^2 \\mid |U| \\ge \\lambda\\right] &=  \\alpha^2E\\left[U^2 \\mid |U| \\ge \\lambda\\right] \\\\&+ 2\\alpha\\beta E\\left[UV \\mid |U| \\ge \\lambda\\right] \\\\&+ \\beta^2 E\\left[V^2 \\mid |U| \\ge \\lambda\\right].\n\\end{aligned}$$\nThe second term is zero because $E[V]=0$ and $V$ is independent of $U$.  The third term is $\\beta^2$ because the independence of $V$ and $U$ gives\n$$E\\left[V^2\\mid |U|\\ge \\lambda\\right] = E\\left[V^2\\right] = 1.$$\nThis leaves us to compute the first conditional expectation.  The standard (elementary) formula expresses it as the fraction\n$$E\\left[U^2 \\mid |U|\\ge \\lambda\\right] = \\frac{\\left(2\\pi\\right)^{-1/2}\\int_{|u|\\ge \\lambda} u^2 e^{-u^2/2}\\,\\mathrm{d}u}{\\left(2\\pi\\right)^{-1/2}\\int_{|u|\\ge \\lambda} e^{-u^2/2}\\,\\mathrm{d}u}$$\nThe denominator is $\\Pr(|U|\\ge \\lambda) = 2\\Phi(-\\lambda)$ where $\\Phi$ is the standard Normal distribution function.To compute the numerator, substitute $x = u^2/2$ to obtain\n$$\\frac{1}{\\sqrt{2\\pi}}\\int_{|u|\\ge \\lambda}u^2 e^{-u^2/2}\\,\\mathrm{d}u = \\frac{2^{3/2}}{\\sqrt{2\\pi}}\\int_{\\lambda^2/2}^\\infty x^{3/2\\,-1}\\ e^{-x}\\,\\mathrm{d}x = \\frac{1}{\\Gamma(3/2)}\\int_{\\lambda^2/2}^\\infty x^{3/2\\,-1}\\ e^{-x}\\,\\mathrm{d}x.$$\nThis equals $\\Pr(Z\\ge \\lambda^2/2)$ where $Z$ has a Gamma$(3/2)$ distribution.  It is a regularized incomplete gamma function, $P(3/2, \\lambda^2/2).$  Consequently, with $\\lambda \\ge 0,$\n\n$$E\\left[\\left(\\alpha U + \\beta V\\right)^2 \\mid |U| \\ge \\lambda\\right] =\\beta^2 +  \\frac{\\alpha^2 P(3/2, \\lambda^2/2)}{2 \\Phi(-\\lambda)}.$$\n\nTo illustrate, this R implementation of the conditional expectation (with a representing $\\alpha,$ b representing $\\beta,$ and $k$ representing $\\lambda$) uses pnorm for $\\Phi$ and pgamma for the Gamma distribution:\nf <- function(a, b, k) { \n  b^2 + a^2 * pgamma(k^2/2, 3/2, lower.tail = FALSE) / (2 * pnorm(-k))\n}\n\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/596285", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
