{"Q": "Calculating $\\operatorname{var} \\left(\\frac{X_1-\\bar{X}}{S}\\right)$ Suppose $X_1,X_2,\\ldots, X_n$ are random variables distributed independently as  $N(\\theta , \\sigma^2)$. define $$S^2=\\frac{1}{n-1}\\sum_{i=1}^{n} (X_i-\\bar{X})^2 ,\\qquad \\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n} X_i\\,.$$ \nTake $n=10$. How can $\\operatorname{var} \\left(\\dfrac{X_1-\\bar{X}}{S}\\right)$ be calculated?\n", "A": "I think it is possible to arrive at an integral representation of $\\text{Var}[\\frac{X_1-\\bar{X}}{S}]$. First, let us express the sample mean $\\bar{X}$ and the sample variance $S^2$ in terms of their counterparts for the observations other than $X_1$:\n\\begin{equation*}\n  \\bar{X}_* = \\frac{1}{n-1}(X_2+\\ldots+ X_n) \\quad\\text{ and }\\quad S_*^2 = \\frac{1}{n-2} \\sum_{i=2}^n (X_i-\\bar{X}_*)^2\n\\end{equation*}\nIt is not so difficult to prove that (see also here)\n\\begin{equation*}\n  \\bar{X} = \\frac{1}{n} X_1 + \\frac{n-1}{n} \\bar{X}_* \\quad\\text{ and }\\quad S^2 = \\frac{n-2}{n-1}S_*^2 + \\frac{1}{n}(X_1-\\bar{X}_*)^2\n\\end{equation*}\nWe may agree that $E[\\frac{X_1}{S}]=E[\\frac{\\bar{X}}{S}]=0$, so that $E[\\frac{X_1-\\bar{X}}{S}]=0$ and therefore $\\text{Var}[\\frac{X_1-\\bar{X}}{S}] = E[\\frac{(X_1-\\bar{X})^2}{S^2}]$. The quantity of which we need the expectation can be rewritten as\n\\begin{align*}\n  \\frac{(X_1-\\bar{X})^2}{S^2} & = \\frac{(X_1 - \\frac{1}{n}X_1 - \\frac{n-1}{n} \\bar{X}_*)^2}{\\frac{n-2}{n-1}S^2_* + \\frac{1}{n}(X_1-\\bar{X}_*)^2}\\\\\n  & = \\big(\\frac{n-1}{n}\\big)^2 \\frac{(X_1-\\bar{X}_*)^2}{\\frac{n-2}{n-1}S^2_* + \\frac{1}{n}(X_1-\\bar{X}_*)^2}\n\\end{align*}\nThe import thing now is that $X_1\\sim N(\\mu,\\sigma^2)$, $\\bar{X}_*\\sim N(\\mu,\\frac{1}{n-1}\\sigma^2)$ and $\\frac{n-2}{\\sigma^2}S^2_* \\sim \\chi^2_{n-2}$ are jointly independent. Define $Y=X_1-\\bar{X}_*$, which is $N(0,\\frac{n}{n-1}\\sigma^2)$ and therefore $\\frac{n-1}{n\\sigma^2} Y^2 \\sim \\chi^2_1$. Then\n\\begin{align*}\n E[\\frac{(X_1-\\bar{X})^2}{S^2}] & = \\big(\\frac{n-1}{n}\\big)^2 E[\\frac{Y^2}{\\frac{n-2}{n-1}S^2_* + \\frac{1}{n}Y^2}] =  \\frac{(n-1)^2}{n} E[\\frac{\\chi_1^2}{\\chi_{n-2}^2 + \\chi_1^2}]\\\\\n\\end{align*}\nwith $\\chi_1^2$ and $\\chi_{n-2}^2$ still independent. Expanding the expectation operator and using the density $f_{\\chi^2_m}(x)=(\\frac{x}{2})^{\\frac{m}{2}-1}\\frac{1}{2\\Gamma(m/2)}e^{-\\frac{x}{2}}$ of the $\\chi^2_m$-distribution we may numerically evaluate\n\\begin{align*}\n E[\\frac{(X_1-\\bar{X})^2}{S^2}] & = \\frac{(n-1)^2}{n} \\int_0^\\infty  \\int_0^\\infty  \\frac{a}{b+a} f_{\\chi^2_1}(a) f_{\\chi^2_{n-2}}(b) \\text{d}a\\text{d}b\n\\end{align*}\nUnfortunately, I see no easy way to do this.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/168306", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 0}}
{"Q": "Expectation of $\\frac{X_1^4}{(X_1^2 + \\cdots + X_d^2)^2}$ \nLet $X_1$, $X_2$, $\\cdots$, $X_d \\sim \\mathcal{N}(0, 1)$ and be independent. What is the expectation of $\\frac{X_1^4}{(X_1^2 + \\cdots + X_d^2)^2}$?\n\nIt is easy to find $\\mathbb{E}\\left(\\frac{X_1^2}{X_1^2 + \\cdots + X_d^2}\\right) = \\frac{1}{d}$ by symmetry. But I do not know how to find the expectation of $\\frac{X_1^4}{(X_1^2 + \\cdots + X_d^2)^2}$. Could you please provide some hints?\nWhat I have obtained so far\nI wanted to find $\\mathbb{E}\\left(\\frac{X_1^4}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$ by symmetry. But this case is different from that for $\\mathbb{E}\\left(\\frac{X_1^2}{X_1^2 + \\cdots + X_d^2}\\right)$ because $\\mathbb{E}\\left(\\frac{X_i^4}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$ may be not equal to $\\mathbb{E}\\left(\\frac{X_i^2X_j^2}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$. So I need some other ideas to find the expectation.\nWhere this question comes from\nA question in mathematics stack exchange asks for the variance of $\\|Ax\\|_2^2$ for a unit uniform random vector $x$ on $S^{d-1}$. My derivation shows that the answer depends sorely on the values of $\\mathbb{E}\\left(\\frac{X_i^4}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$ and $\\mathbb{E}\\left(\\frac{X_i^2X_j^2}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$ for $i \\neq j$. Since\n$$\n\\sum_{i \\neq j}\\mathbb{E} \\left( \\frac{X_i^2X_j^2}{(X_1^2 + \\cdots + X_d^2)^2}\\right) + \\sum_i \\mathbb{E}\\left(\\frac{X_i^4}{(X_1^2 + \\cdots + X_d^2)^2}\\right) = 1\n$$\nand by symmetry, we only need to know the value of $\\mathbb{E}\\left(\\frac{X_1^4}{(X_1^2 + \\cdots + X_d^2)^2}\\right)$ to obtain other expectations.\n", "A": "This answer expands @Glen_b's answer.\n\n\nFact 1: If $X_1$, $X_2$, $\\cdots$, $X_n$ are independent standard normal distribution random variables, then the sum of their squares has the chi-squared distribution with $n$ degrees of freedom. In other words,\n  $$\nX_1^2 + \\cdots + X_n^2 \\sim \\chi^2(n)\n$$\n\nTherefore, $X_1^2 \\sim \\chi^2(1)$ and $X_2^2 + \\cdots + X_d^2 \\sim \\chi^2(d-1)$.\n\nFact 2: If $X \\sim \\chi^2(\\lambda_1)$ and $Y \\sim \\chi^2(\\lambda_2)$, then\n  $$\n\\frac{X}{X + Y} \\sim \\texttt{beta}(\\frac{\\lambda_1}{2}, \\frac{\\lambda_2}{2})\n$$\n\nTherefore, $Y = \\frac{X_1^2}{X_1^2 + \\cdots + X_d^2} \\sim \\texttt{beta}(\\frac{1}{2}, \\frac{d-1}{2})$.\n\nFact 3: If $X \\sim \\texttt{beta}(\\alpha, \\beta)$, then\n  $$\n\\mathbb{E}(X) = \\frac{\\alpha}{\\alpha + \\beta}\n$$\n  and\n  $$\n\\mathbb{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n$$\n\nTherefore,\n$$\\mathbb{E}(Y) = \\frac{1}{d}$$\nand\n$$\n\\mathbb{Var}(Y) = \\frac{2(d-1)}{d^2(d+2)}\n$$\n\nFinally,\n$$\n\\mathbb{E}(Y^2) = \\mathbb{Var}(Y) + \\mathbb{E}(Y)^2 = \\frac{3d}{d^2(d+2)}.\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/222915", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "10", "answer_count": 2, "answer_id": 1}}
{"Q": "how to find expected value? The random variables X and Y have joint probability function $p(x,y)$ for $x = 0,1$ and \n$y = 0,1,2$. \nSuppose $3p(1,1) = p(1,2)$, and $p(1,1)$ maximizes the variance of $XY$. \nCalculate the probability that $X$ or $Y$ is $0$. \nSolution: Let $Z = XY$. Let $a, b$, and $c$ be the probabilities that $Z$ takes on the values $0, 1$, and $2$, respectively. \nWe have $b = p(1,1)$ and $c = p(1,2)$ and thus $3b = c$. And because the probabilities sum to $1,  a = 1 \u2013 b \u2013 c = 1 \u2013 4b$.  \nThen, $$E(Z) = b + 2c = 7b,$$$$ E(Z\\cdot Z) = b + 4c = 13b.$$ Then, $$Var (Z) = 13b-19b^2.$$$$ \\frac{dVar(Z)}{db}=13-98b=0 \\implies b =\\frac{ 13}{98}.$$ The probability that either $X$ or $Y$ is zero is the same as the probability that $Z$ is $0$ which is as $a = 1 \u2013 4b = \\frac{46}{98} = \\frac{23}{49}$. \nI am not sure how they got: $E(Z) = b + 2c = 7b$. Can someone explain this step where $b$ and $2c$ come from?\n", "A": "\\begin{align}\\mathbb{E}(Z)&=0\\cdot P(Z=0)+1\\cdot P(Z=1)+2\\cdot P(Z=2)\\\\\n&=1\\cdot P(Z=1)+2\\cdot P(Z=2)\\\\\n&=1\\cdot P(X=1,Y=1)+2\\cdot P(X=1,Y=2)\\\\\n&=b+2c\n\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/313379", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Coin Tossings probability I want to find the probability that in ten tossings a coin falls heads at least five times in succession. Is there any formula to compute this probability?\nAnswer provided is $\\frac{7}{2^6}$\n", "A": "Corrected answer after Orangetree pointed out I forgot to take into account events were not mutually exclusive.\nYou need to think about how many different coin tossing sequences give at least $5$ consecutive heads, and how many coin tossing sequences there are in total, and then take the ratio of the two.\nClearly there is $2^{10}$ coin tossing sequences in total, since each of $10$ coins have $2$ possible outcomes.\nTo see how many sequences have $5$ consecutive heads consider the following. In any sequence\n$$X-X-X-X-X-X-X-X-X-X$$\nWe can put the $5$ consecutive heads in $5$ places. This can be seen by putting it at the start of thee sequence, and then moving it over step by step.\n$$H-H-H-H-H-X-X-X-X-X$$\n$$X-H-H-H-H-H-X-X-X-X$$\n$$X-X-H-H-H-H-H-X-X-X$$\n$$X-X-X-H-H-H-H-H-X-X$$\n$$X-X-X-X-H-H-H-H-H-X$$\n$$X-X-X-X-X-H-H-H-H-H$$\nFor the first scenario, we have $2^5$ possible sequences since the remaining coins can be either heads or tails without affecting the result.\nFor the second scenario, we can only take $2^4$ since we need to fix the first coin as tails to avoid counting sequences more than once.\nSimilarly, for the third scenario we can count $2^4$ un-counted sequences, as we fix the coin preceeding the sequence as tails.. and so on\nThis gives total number of ways \n$$2^5 + 2^4 + 2^4 + 2^4 + 2^4 + 2^4  = 2^4(2+1+1+1+1+1) = 2^4\\times 7$$\nHence the probability should be\n$$\\frac{2^4 \\times 7}{2^{10}} = \\frac{7}{2^6}$$\nTo actually answer your question if a general formula exists, the reasoning above can be extended to give the probability of at least $m$ heads out of $n$ coin tosses as\n$$\\frac{2^{m-1}(2+(n-m))}{2^n} = \\frac{2+n-m}{2^{n-m+1}}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/369157", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 3, "answer_id": 1}}
{"Q": "Joint distribution of X and Y bernoulli random variables A box contains two coins: a regular coin and a biased coin with $P(H)=\\frac23$. I choose a coin at random and toss it once. I define the random variable X as a Bernoulli random variable associated with this coin toss, i.e., X=1 if the result of the coin toss is heads and X=0 otherwise. Then I take the remaining coin in the box and toss it once. I define the random variable Y as a Bernoulli random variable associated with the second coin toss.\na)Find the joint PMF of X and Y.\nb)Are X and Y independent?\nMy attempt to answer this question:\nLet A be the event that first coin, I pick is the regular(fair) coin. Then conditioning on that event, I can find joint PMF. Once conditioned, I can decide if X and Y are independent(conditionally).\n$P(A)=\\frac12,P(A^c)=\\frac12$.\nIn the event A, $P(X=1)=\\frac12,P(Y=1)=\\frac23$.\nIn the event$A^c, P(X=1)=\\frac23, P(Y=1)=\\frac12$\nSo, $P_{X,Y}(x,y)= P(X=x, Y=y|A)P(A) + P(X=x, Y=y|A^c)P(A^c)$\n$P_{X,Y}(x,y)=P_{\\frac12}(x)P_{\\frac23}(y)(\\frac12) + P_{\\frac23}(x)P_{\\frac12}(y)(\\frac12)$\nNow, how can we find Joint PMF of X and Y using Bernoulli distribution?\n", "A": "The joint pmf can be described by a 2-by-2 contingency table that shows the probabilities of getting $X=1$ and $Y=1$, $X=1$ and $Y=0$, $X=0$ and $Y=1$, $X=0$ and $Y=0$.\nSo you'll have:\n\n\n\n\n\n$X=0$\n$X=1$\n\n\n\n\n$Y=0$\n$\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}\\cdot\\frac{1}{2}=\\frac{1}{6}$\n$\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{1}{3}+\\frac{1}{2}\\cdot\\frac{2}{3}\\cdot\\frac{1}{2}=\\frac{1}{4}$\n\n\n$Y=1$\n$\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{2}{3}+\\frac{1}{2}\\cdot\\frac{1}{3}\\cdot\\frac{1}{2}=\\frac{1}{4}$\n$\\frac{1}{2}\\cdot\\frac{1}{2}\\cdot\\frac{2}{3}+\\frac{1}{2}\\cdot\\frac{2}{3}\\cdot\\frac{1}{2}=\\frac{1}{3}$\n\n\n\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/592258", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 0}}
{"Q": "A three dice roll question I got this question from an interview.\nA and B are playing a game of dice as follows. A throws two dice and B throws a single die. A wins if the maximum of the two numbers is greater than the throw of B. What is the probability for A to win the game?\nMy solution.\nIf $(X,Y)$ are the two random throws of A and $Z$ is the random throw of B, then the problem asks (if I guessed correctly) to compute $P(\\max(X,Y)>Z)$. But how to do it? I highly appreciate your help.\n", "A": "I will take a less formal approach, in order to illustrate my thinking.\nMy first instinct was to visualize the usual $6 \\times 6$ array of outcomes $(X,Y)$ of $A$'s dice rolls, and looking at when the larger of the two values is less than or equal to some value:\n$$\\begin{array}{cccccc}\n(1,1) & (2,1) & (3,1) & (4,1) & (5,1) & (6,1) \\\\\n(1,2) & (2,2) & (3,2) & (4,2) & (5,2) & (6,2) \\\\\n(1,3) & (2,3) & (3,3) & (4,3) & (5,3) & (6,3) \\\\\n(1,4) & (2,4) & (3,4) & (4,4) & (5,4) & (6,4) \\\\\n(1,5) & (2,5) & (3,5) & (4,5) & (5,5) & (6,5) \\\\\n(1,6) & (2,6) & (3,6) & (4,6) & (5,6) & (6,6) \\\\\n\\end{array}$$\nIt's intuitively clear from this diagram that the number of ordered pairs whose maximum is at most $k$ is $k^2$, for $k \\in \\{1, 2, 3, 4, 5, 6\\}$.  This is because geometrically, the set of such outcomes are arranged in a series of nested squares in the array.  So it follows that for each of the six equiprobable outcomes for $B$'s die roll $Z \\in \\{1, 2, 3, 4, 5, 6\\}$, $A$ will lose with probability $z^2/6^2$, hence the total probability of $A$ losing to $B$ is simply $$\\frac{1^2 + \\cdots + 6^2}{6^2(6)} = \\frac{6(7)(13)}{6} \\cdot \\frac{1}{6^3} = \\frac{7(13)}{6^3}.$$  Hence $A$ wins with probability $1 - \\frac{7(13)}{6^3} = \\frac{125}{216}$.\nThis line of reasoning is what I would use if I had no access to pencil or paper and had to answer the question mentally, reserving the computational part to the very end.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/600294", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "13", "answer_count": 9, "answer_id": 1}}
