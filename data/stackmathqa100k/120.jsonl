{"Q": "Vector space of $\\mathbb{C}^4$ and its basis, the Pauli matrices How do I write an arbitrary $2\\times 2$ matrix as a linear combination of the three Pauli Matrices and the $2\\times 2$ unit matrix? \nAny example for the same might help ? \n", "A": "A slow construction would go...\n$$$$\n$$\r\n\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\r\n=\r\na\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\r\n+b\\begin{pmatrix}0&1\\\\0&0\\end{pmatrix}\r\n+c\\begin{pmatrix}0&0\\\\1&0\\end{pmatrix}\r\n+d\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\r\n$$\n\n$$\r\n\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\r\n=\\frac{1}{2}\r\n\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix}\r\n+\r\n\\frac{1}{2}\r\n\\begin{pmatrix}1&0\\\\0&-1\\end{pmatrix}\r\n=\\frac{1}{2}1_2+\\frac{1}{2}\\sigma_3\r\n$$\n$$\r\n\\begin{pmatrix}0&1\\\\0&0\\end{pmatrix}\r\n=\\ ...\r\n$$\n\n$$\r\n\\Longrightarrow  \\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\r\n=\r\n\\frac{a}{2}1_2+\\frac{a}{2}\\sigma_3+\\ ...\\ (\\text{other combintations of the four matrices})\r\n$$\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/23846", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 3, "answer_id": 0}}
{"Q": "Equivalent Rotation using Baker-Campbell-Hausdorff relation Is there a way in which one can use the BCH relation to find the equivalent angle and the axis for two rotations? I am aware that one can do it in a precise way using Euler Angles but I was wondering whether we can use just the algebra of the rotation group to perform the same computation?\n", "A": "As $\\mathrm{SO}(3)$ is a connected group, $\\exp(\\mathsf{L}(\\mathrm{SO}(3))) = \\mathrm{SO}(3)$ and hence this should \u2013 in theory \u2013 work. Let us work in the fundamental representation of $\\mathrm{SO}(3)$, that is orthogonal 3x3 matrices. \nAssume you have a rotation $B$ acting first and a second rotation $A$, the resulting rotation is then given by $AB \\equiv C \\in \\mathrm{SO}(3)$. Furthermore, we can express $A$, $B$ and $C$ by $\\exp(a)$, $\\exp(b)$ and $\\exp(c)$ for $a,b,c \\in \\mathsf{L}(\\mathrm{SO}(3))$. We then have\u00b9\n$$ \\exp(a) \\exp(b) = AB = C = \\exp(c) = \\exp\\left(a + b + \\frac{1}{2}[a,b] + \\frac{1}{12} [ a, [a,b]] - \\frac{1}{12}[b,[a,b]]+ \\ldots\\right)\\quad.$$\nNow, the problem with verifying this by an example is that these commutators are rather ugly. I shall do two examples:\nFirst example: Two rotations about the $x$ axis\nTake $A$ to rotate about $(1,0,0)$ by $\\theta$ and $B$ to rotate about the same axis by $\\phi$. We then have\n$$ A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(\\theta) & -\\sin(\\theta) \\\\ 0 & \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}$$\nand similarly for $B$. The associated $a$ is then simply:\n$$ a = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -\\theta \\\\ 0 & \\theta & 0 \\end{pmatrix}\\quad $$\nand again similarly for $b$ with $\\theta \\to \\phi$. You can check easily that $\\exp(a)$ gives you indeed $A$. Now since $a$ and $b$ commute, we have $[a,b] = 0$ and hence $c = a + b$ - which is\n$$ c = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -\\theta -\\phi \\\\ 0 & \\theta + \\phi & 0 \\end{pmatrix}\\quad.$$\nThis very likely illuminates better than $AB$ that two rotations about the same axis are equivalent to one rotation by the sum of the angles. You can again check that $\\exp(c)$ gives you $C$.\nSecond Example: One rotation about $y$, a second about $x$.\nThis one is more difficult, as we will have to calculate annoying commutators. The result presented here will hence only be approximate, not exact.\nTake\n$$ A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\cos(\\theta) & -\\sin(\\theta) \\\\ 0 & \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix} \\qquad B = \\begin{pmatrix} \\cos(\\phi) & 0 & \\sin(\\phi) \\\\ 0 & 1 & 0 \\\\ -\\sin(\\phi) & 0 & \\cos(\\phi) \\end{pmatrix} \\quad .$$\nYou can calculate that\n$$ AB = C = \\begin{pmatrix} \\cos(\\phi) & 0 & \\sin(\\phi) \\\\ \\sin(\\theta) \\sin(\\phi) & \\cos(\\theta) & -\\sin(\\theta) \\cos(\\phi) \\\\ \\sin(\\phi)\\cos(\\theta) & \\sin(\\theta) & \\cos(\\phi)\\cos(\\theta) \\end{pmatrix} \\quad .$$\nSimilarly to the above, we have\n$$ a = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & -\\theta \\\\ 0 & \\theta & 0 \\end{pmatrix} \\qquad b = \\begin{pmatrix} 0 & 0 & \\phi \\\\ 0 & 0 & 0 \\\\ -\\phi & 0 & 0 \\end{pmatrix} \\quad.$$\nNow the tricky part is to calculate\n$$ c = a + b + \\frac{1}{2} [ a,b] + \\frac{1}{12} [ a, [a,b]] - \\frac{1}{12} [b,[a,b]] + \\ldots $$\nto such a precision that $\\exp(c)$ gives remotely sensible results. At this, I mostly failed, but here's what I got:\n$$ \\frac{1}{2} [ a,b] = \\frac{1}{2} \\begin{pmatrix} 0 & -\\theta\\phi & 0 \\\\ \\theta\\phi & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\\quad,$$\nwhich looks an awful lot like the element of the Lie algebra basis corresponding to a rotation about the $z$ axis, but unfortunately doesn\u2019t fit in at all (something linear in either $a$ or $b$ would have been nice\u2026). I then went on to calculate $[a,[a,b]]$ and $[b,[a,b]]$ and arrived at\n$$ c \\approx \\begin{pmatrix} 0 & -\\frac{1}{2}\\theta\\phi & \\phi - \\frac{1}{12} \\theta^2 \\phi \\\\ \\frac{1}{2} \\theta \\phi & 0 & -\\theta + \\frac{1}{12} \\theta\\phi^2 \\\\ -\\phi + \\frac{1}{12} \\theta^2 \\phi & \\theta - \\frac{1}{12} \\theta \\phi^2 & 0 \\end{pmatrix} \\quad . $$\nThe nice thing here is that this is still an antisymmetric matrix and hence (can be) in $\\mathsf{L}(\\mathrm{SO}(3))$. In order to now compare this to anything, we have to approximate $C$. Recall the expression from above. As a first approximation, I will set $\\cos(x) = 1 - \\frac{1}{2}x^2$, $\\sin(x) = x - \\frac{1}{6} x^3$. I then get\n$$ C \\approx \\begin{pmatrix} 1 - \\frac{\\phi^2}{2} & 0 & \\phi - \\frac{\\phi^3}{6} \\\\ \\left(\\theta - \\frac{\\theta^3}{6}\\right) \\left(\\phi - \\frac{\\phi^3}{6}\\right) & 1 - \\frac{\\theta^2}{2} & -\\left(1-\\frac{\\phi^2}{2}\\right)\\left(\\theta - \\frac{\\theta^3}{6}\\right) \\\\ -\\left(1-\\frac{\\theta^2}{2}\\right)\\left(\\phi-\\frac{\\phi^3}{6}\\right) & \\theta - \\frac{\\theta^3}{6} & \\left(1 - \\frac{\\theta^2}{2}\\right)\\left(\\phi - \\frac{\\phi^3}{6}\\right) \\end{pmatrix} \\quad ,$$\nexpanding out the brackets and throwing away anything of order four, I arrive at\n$$ C \\approx \\begin{pmatrix} 1 - \\frac{\\phi^2}{2} & 0 & \\phi - \\frac{\\phi^3}{6} \\\\\n\\theta\\phi & 1 - \\frac{\\theta^2}{2} & -\\theta + \\frac{\\phi^2\\theta}{2} \\\\ -\\phi + \\frac{\\theta^2\\phi}{2} & \\theta - \\frac{\\theta^3}{6} & \\phi - \\frac{\\theta^2\\phi}{2} \\end{pmatrix}\\quad.$$\nThis expression should be roughly equal to\n$$ 1_3 + c + \\frac{1}{2} c^2 + \\frac{1}{6} c^3 \\quad,$$\nwhich is the expansion of $\\exp(c)$. After again throwing away everything of order four, we arrive at\n$$ \\exp(c) \\approx \\begin{pmatrix}\n1-\\frac{\\phi^2}{2} & 0 & \\phi - \\frac{\\phi^3}{6} \\\\\n\\theta\\phi & 1 - \\frac{\\theta^2}{2} & -\\theta+\\frac{\\theta^3}{6} +\\frac{\\theta\\phi^2}{2}\\\\\n-\\phi +\\frac{\\theta^2 \\phi}{2} & \\theta - \\frac{\\theta^3}{6} & 1 - \\frac{\\theta^2}{2} - \\frac{\\phi^2}{2}\n\\end{pmatrix} \\quad .$$\nThe remaining \u2018wrong\u2019 terms here most probably cancel with higher orders of $c$, but I have to admit I am slightly too lazy for that.\nConclusion\nThe main problem with the BCH formula is really that, in general, $[a,b] \\neq 0$ and you hence most often not even get an exact expression for $c$  \u2013 from which one could most likely deduce angle and axis of rotation without evaluating that pesky exponential. Without an exact expression for $c$, however, all is lost, as non-exact expressions merely rely on the fact that for infinitesimal angles of rotation, all rotations commute.\nI would love to hear other opinions, though, especially regarding the \u2018theoretical\u2019 part what one could do with $c$, if it was known exactly.\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/29100", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "8", "answer_count": 2, "answer_id": 1}}
{"Q": "$N$ coupled quantum harmonic oscillators I want to find the wave functions of $N$ coupled quantum harmonic oscillators having the following hamiltonian: \n\\begin{eqnarray}\nH &=& \\sum_{i=1}^N \\left(\\frac{p^2_i}{2m_i} + \\frac{1}{2}m_i\\omega^2 x^2_i +  \\frac{\\kappa}{2} (x_i-x_{i+1})^2 \\right)\\,, \\qquad x_{N+1}=0\\,,\\\\\n&=& \\frac{1}{2}p^T Mp + \\frac{1}{2}x^TKx\\,,\n\\end{eqnarray}\nwhere $M=\\text{diag}(\\frac{1}{m_1}, \\cdots,\\frac{1}{m_N})$ and $K$ is a real symmetric $N\\times N$ matrix with positive eigenvalues,\n\\begin{equation}\nK=\n\\begin{pmatrix}\nk'_1& -\\kappa & 0 & \\cdots & 0 \\\\\n-\\kappa  & k'_2& -\\kappa & \\ddots & \\vdots \\\\\n0 & -\\kappa  & \\ddots& \\ddots & 0\\\\\n\\vdots & \\ddots &\\ddots & k'_{N-1}&-\\kappa \\\\\n0&\\cdots & 0 & -\\kappa & k'_N\n\\end{pmatrix}\n\\end{equation}\nwith $k'_i = m_i\\omega^2+2\\kappa$ but $k'_{1,N} = m_{1,N}\\omega^2+\\kappa$. By choosing a basis which diagonalizes the matrix $K$, the hamiltonian can be express as the sum of uncoupled harmonic oscillators hamiltonian.\nAs an example, consider two coupled quantum harmonic oscillators with hamiltonian\n\\begin{equation}\nH = \\frac{p^2_1}{2m_1} + \\frac{p^2_2}{2m_2} + \\frac{1}{2}m_1\\omega^2 x^2_1 + \\frac{1}{2}m_2 \\omega^2 x^2_2 +  \\frac{\\kappa}{2} (x_1-x_2)^2 \\,.\n\\end{equation}\nWe make the following changes of variables (normal coordinates)\n\\begin{eqnarray}\nx &=& \\frac{x_1 - x_2}{\\sqrt{2}} \\,, \\\\\nX &=& \\frac{m_1 x_1 + m_2 x_2}{M\\sqrt{2}}\\,,\n\\end{eqnarray}\nor equivalently,\n\\begin{eqnarray}\nx_1 &=& \\frac{1}{\\sqrt{2}}\\left(X + \\frac{m_2}{M}x\\right) \\,, \\\\\nx_2 &=& \\frac{1}{\\sqrt{2}}\\left(X - \\frac{m_1}{M}x\\right) \\,,\n\\end{eqnarray}\nwhere $M=(m_1+m_2)/2$. Then the hamiltonian becomes\n\\begin{equation}\nH = \\frac{p^2_x}{2\\mu} + \\frac{1}{2}\\mu\\omega_-^2 x^2 + \\frac{p^2_X}{2M}  + \\frac{1}{2}M\\omega_+^2 X^2 \\,.\n\\end{equation}\nwhere $\\displaystyle\\mu = \\frac{m_1m_2}{M}$ and $\\omega_+^2=\\omega^2$ and $\\omega_-^2 = \\omega^2 + 2\\kappa/\\mu$.\nThe wave functions are\n\\begin{equation}\n\\Psi_{mn}(x_1,x_2) =  \\frac{1}{\\sqrt{\\pi x_0X_0}}\\frac{e^{-x^2/2x_0^2}}{\\sqrt{m!\\,2^m}}\\frac{e^{-X^2/2X_0^2}}{\\sqrt{n!\\,2^n}}H_m\\left(\\frac{x}{x_0} \\right)H_n\\left(\\frac{X}{X_0} \\right) \\,,\n\\end{equation}\nwhere $x=x(x_1,x_2)$ and $X=X(x_1,x_2)$ and $\\displaystyle x_0=\\sqrt{\\frac{\\hbar}{\\mu\\omega_-}}$ and $\\displaystyle X_0=\\sqrt{\\frac{\\hbar}{M\\omega_+}}$.\nHow does all this work using matrices \"formalism\"? And how to extend it to $N$ CQHO?\nUltimately, I would like to redemonstrate (8) and (13) in http://arxiv.org/pdf/hep-th/9303048.pdf\n", "A": "(If I did not make obvious algebra errors) the elegant solution to this problem is to realize that the matrix\n$$\n\\Lambda=\\left(\\begin{array}{lllr}\n0&1&0\\ldots&0\\\\\n0&0&1&\\ldots 0\\\\\n\\vdots&\\vdots&\\vdots&1\\\\\n1&0&0\\ldots&0\n\\end{array}\\right)\n$$\nactually commutes with $H$ since $\\Lambda$ basically maps $x_{i+1}$ to $x_i$. \nAs a result, $H$ and $\\Lambda$ have a common set of eigenvectors.\nSince $\\Lambda^n=1_{n\\times n}$, the eigenvalues of $\\Lambda$ satisfy $\\lambda_k^n=1$ so are the $n$\u2019th root of unity.\n$$\n\\lambda_k=e^{2\\pi i k/n}=\\omega_n^k\\, .\n$$\nThe eigenvectors are then easily found to be the Fourier vectors, i.e.\n$$\nv_k= \\frac{1}{\\sqrt{n}}\\left(\\begin{array}{c} \n\\omega_n^k\\\\\n(\\omega_n^k)^2\\\\\n\\vdots \\\\\n1\n\\end{array}\\right)\\, .\n$$\nWith these you can construct the matrix $U$ of eigenvectors that will diagonalise $H$.\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/209424", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 2, "answer_id": 1}}
{"Q": "Projectiles on inclined planes with coefficient of restitution \nThis is the problem I am currently attempting. So far, I've resolved the velocity parallel and perpendicular to the plane to get, perpendicular: $u \\sin \\theta$ upon launch and $-u\\sin \\theta$ on landing.\nParallel: $u \\cos \\theta - 2u \\sin \\theta \\tan \\alpha$.\nWhere do I go from here?\n", "A": "@EDIT: SOLVED IT! :) The key part I was missing was that $e$ only acts on the component perpendicular to the slope (the y-component), i.e $u_{r,x} = v_x$ and $u_{r,x} = -e v_y$. Huge thanks to @Floris for spotting this, and all the help!!\nAlways start with a diagram :)\n\nI tried solving this two ways: relative to vertical/horizontal and relative to the slope. The reflection for vertical/horizontal is horrendous, and it works out much neater to just resolve relative to the slope.\nRelative to the slope of the plane\n\n\nProjection\n\\begin{align} \n{v_x \\choose v_y} &= {u_x + a_x t_1\\choose u_y  + a_y t_1} \\\\\n{s_x \\choose 0} &= {u_x t_1 + \\frac{1}{2}a_x t_1^2\\choose u_y t_1 + \\frac{1}{2}a_y t_1^2}\n\\end{align}\nwhere $u_x = u\\cos(\\theta)$, $u_y = u\\sin(\\theta)$, $a_x = -mg \\sin(\\alpha)$, $a_y = -mg \\cos(\\alpha)$ and $s_y = 0$.\nReflection\n$$\n\\vec{u_{r}} = \\vec{v} - 2(\\vec{v} \\cdot \\hat{n})\\hat{n}\n$$\nwhere $\\hat{n} = {0 \\choose 1}$ is the unit normal vector to the slope.\n$$\n{u_{r,x} \\choose u_{r,y}} = {v_x \\choose v_y} - 2 ({v_x \\choose v_y} \\cdot {0 \\choose 1}) {0 \\choose 1}\n$$\nRearrange the equation and remember that the velocity perpendicular to the plane is reduced by a factor of $e$.\n$$\n{u_{r,x} \\choose u_{r,y}} = {v_x \\choose -ev_y}\n$$\nRebound\n\\begin{align} \n{-s_x \\choose -s_y} \n&= {u_{r,x} t_2 + \\frac{1}{2}a_x t_2^2\\choose u_{r,y} t_2 + \\frac{1}{2}a_y t_2^2} \n\\\\\n{s_x \\choose 0} &= {-v_x t_2 - \\frac{1}{2}a_x t_2^2 \\choose -ev_y t_2 + \\frac{1}{2}a_y t_2^2}\n\\end{align} \nBy using $s_y = 0$, we can immediately solve for $t_1$ and $t_2$\n$$\nt_1 = -\\frac{2u_y}{a_y} \\quad t_2 = \\frac{2ev_y}{a_y}\n$$\nBy plugging our solution for $t_1$ into $v_y = u_y  + a_y t_1$, we get\n$$\nv_y = -u_y\n$$\nwhich in hindsight is obvious, because acceleration perpendicular to the plane is constant.\nNow for the fun part: setting $s_x$ during projection equal to the $s_x$ during the rebound.\n$$\nu_x t_1 + \\frac{1}{2}a_x t_1^2 = -v_x t_2 - \\frac{1}{2}a_x t_2^2\n$$\nPlug in for time\n$$\n[u_x + \\frac{1}{2}a_x (-\\frac{2u_y}{a_y}) ](-\\frac{2u_y}{a_y}) = [-v_x - \\frac{1}{2}a_x \\frac{2ev_y}{a_y}] \\frac{2ev_y}{a_y}\n$$\nCancel $\\frac{2}{a_y}$ from both sides\n$$\n[- u_x + u_y\\frac{a_x}{a_y} ]u_y = [-v_x - ev_y \\frac{a_x}{a_y}] ev_y\n$$\nMultiply both sides by $-1$ and factor out $u_y$ from the left and $v_y$ from the right.\n$$\n[\\frac{u_x}{u_y} - \\frac{a_x}{a_y} ]u_y^2 = [\\frac{v_x}{v_y} + e\\frac{a_x}{a_y}] ev_y^2\n$$\nRemember that $v_y = -u_y$, so we can cancel $v_y^2$ and $u_y^2$ from both sides. \n$$\n\\frac{u_x}{u_y} - \\frac{a_x}{a_y} = e\\frac{v_x}{v_y} + e^2\\frac{a_x}{a_y}\n$$\nNow we want to plug in for $v_x/v_y$\n$$\nv_x = u_x + a_x t_1 = u_x -2 \\frac{a_x}{a_y}u_y = (\\frac{u_x}{u_y} - 2 \\frac{a_x}{a_y})u_y \\\\ \n\\therefore \\frac{v_x}{v_y} = 2 \\frac{a_x}{a_y} - \\frac{u_x}{u_y}\n$$\nPlug this back in\n$$\n\\frac{u_x}{u_y} - \\frac{a_x}{a_y} = e(2 \\frac{a_x}{a_y} - \\frac{u_x}{u_y}) + e^2\\frac{a_x}{a_y}\n$$\nThis is a quadratic in $e$, so rearrange into an obviously quadratic form\n$$\n\\frac{a_x}{a_y} e^2 + (2 \\frac{a_x}{a_y} - \\frac{u_x}{u_y})e + \\frac{a_x}{a_y} - \\frac{u_x}{u_y} = 0 \n$$\nSolve using the quadratic formula $e = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$\n\\begin{align} \nb^2 - 4ac &= (2 \\frac{a_x}{a_y} - \\frac{u_x}{u_y})^2 - 4 (\\frac{a_x}{a_y})(\\frac{a_x}{a_y} - \\frac{u_x}{u_y}) \\\\\n&= 4 (\\frac{a_x}{a_y})^2 - 4 \\frac{a_x}{a_y}\\frac{u_x}{u_y} + (\\frac{u_x}{u_y})^2 - 4(\\frac{a_x}{a_y})^2 + 4\\frac{a_x}{a_y}\\frac{u_x}{u_y} \\\\\n&= (\\frac{u_x}{u_y})^2\n\\end{align} \n$$\ne = \\frac{(\\frac{u_x}{u_y} - 2 \\frac{a_x}{a_y}) \\pm \\frac{u_x}{u_y}}{2\\frac{a_x}{a_y}} \\\\\n\\therefore \\quad\ne_- = -1, \\quad\ne_+ = \\frac{u_x}{u_y}\\frac{a_y}{a_x} - 1\n$$\nRemember the definitions from the start: $u_x = u\\cos(\\theta)$, $u_y = u\\sin(\\theta)$, $a_x = -mg \\sin(\\alpha)$, $a_y = -mg \\cos(\\alpha)$,  so\n$$\n\\frac{u_x}{u_y} = \\cot(\\theta) \\quad \\frac{a_x}{a_y} = \\tan(\\alpha)\n$$\nPlug this into our expression for $e_+$ and voila!\n$$\ne_+ = \\cot(\\theta) \\cot(\\alpha) - 1\n$$\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/232303", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Non-zero components of the Riemann tensor for the Schwarzschild metric Can anyone tell me which are the non-zero components of the Riemann tensor for the Schwarzschild metric?\nI've been searching for these components for about 2 weeks, and  I've found a few sites, but the problem is that each one of them shows different components, in number and form. I\u00b4ve calculated a few components but I don't know if they are correct. I'm using the form of the metric:\n$$ds^2 = \\left(1-\\frac{2m}{r}\\right)dt^2 + \\left(1-\\frac{2m}{r}\\right)^{-1} dr^2 + r^2 d\\theta^2 + r^2\\sin^2\\theta \\, d\\phi^2.$$\n", "A": "According to Mathematica, and assuming I haven't made any silly errors typing in the metric, I get the non-zero components of $R^\\mu{}_{\\nu\\alpha\\beta}$ to be:\n{1, 2, 1, 2} -> (2 G M)/(r^2 (-2 G M + c^2 r)),\n{1, 2, 2, 1} -> -((2 G M)/(r^2 (-2 G M + c^2 r))),\n{1, 3, 1, 3} -> -((G M)/(c^2 r)),\n{1, 3, 3, 1} -> (G M)/(c^2 r),\n{1, 4, 1, 4} -> -((G M Sin[\\[Theta]]^2)/(c^2 r)),\n{1, 4, 4, 1} -> (G M Sin[\\[Theta]]^2)/(c^2 r),\n{2, 1, 1, 2} -> (2 G M (-2 G M + c^2 r))/(c^4 r^4),\n{2, 1, 2, 1} -> -((2 G M (-2 G M + c^2 r))/(c^4 r^4)),\n{2, 3, 2, 3} -> -((G M)/(c^2 r)),\n{2, 3, 3, 2} -> (G M)/(c^2 r),\n{2, 4, 2, 4} -> -((G M Sin[\\[Theta]]^2)/(c^2 r)),\n{2, 4, 4, 2} -> (G M Sin[\\[Theta]]^2)/(c^2 r),\n{3, 1, 1, 3} -> (G M (2 G M - c^2 r))/(c^4 r^4),\n{3, 1, 3, 1} -> (G M (-2 G M + c^2 r))/(c^4 r^4),\n{3, 2, 2, 3} -> (G M)/(r^2 (-2 G M + c^2 r)),\n{3, 2, 3, 2} -> (G M)/(r^2 (2 G M - c^2 r)),\n{3, 4, 3, 4} -> (2 G M Sin[\\[Theta]]^2)/(c^2 r),\n{3, 4, 4, 3} -> -((2 G M Sin[\\[Theta]]^2)/(c^2 r)),\n{4, 1, 1, 4} -> (G M (2 G M - c^2 r))/(c^4 r^4),\n{4, 1, 4, 1} -> (G M (-2 G M + c^2 r))/(c^4 r^4),\n{4, 2, 2, 4} -> (G M)/(r^2 (-2 G M + c^2 r)),\n{4, 2, 4, 2} -> (G M)/(r^2 (2 G M - c^2 r)),\n{4, 3, 3, 4} -> -((2 G M)/(c^2 r)),\n{4, 3, 4, 3} -> (2 G M)/(c^2 r),\n\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/295814", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 3, "answer_id": 0}}
{"Q": "Scattering amplitude with a change in basis of fields Suppose I know the Feynman rules for the scattering process $\\pi^j \\pi^k \\rightarrow \\pi^l \\pi^m$ where $j,k,l,m$ can be $1, 2$ or $3$. Define the charged pion fields as $\\pi^\\pm=\\frac{1}{\\sqrt{2}}(\\pi^1 \\pm i \\pi^2)$ and neutral pion field as $\\pi^0=\\pi^3$. I would like to derive the scattering amplitudes for processes like $\\pi^+ \\pi^- \\rightarrow \\pi^+ \\pi^-$ from my knowledge of scattering amplitude of $\\pi^j \\pi^k \\rightarrow \\pi^l \\pi^m$. How should I proceed?\nI suppose it can be done by clever change of indices in Feynman rules, but I am unable to see how exactly. \n", "A": "I assume your Lagrangian might have a term of the form $\\bar{N}\\vec{\\pi}.\\vec{\\tau}\\gamma^5N$, or something with $\\vec{\\pi}.\\vec{\\tau}$. One method is to expand the following and see how $\\pi^+$ and $\\pi^-$ come into your Lagrangian,\n\\begin{align*}\n\\vec{\\pi}.\\vec{\\tau} &= \\pi^1\\sigma^1 + \\pi^2\\sigma^2 +\\pi^3\\sigma^3 \\\\\n        &= \\begin{pmatrix}\n         0 && \\pi^1 \\\\\n         \\pi^1 && 0\n         \\end{pmatrix}\n         + \n         \\begin{pmatrix}\n          0 && -i \\pi^2 \\\\\n          i \\pi^2 && 0 \n         \\end{pmatrix}\n         + \n         \\begin{pmatrix}\n          \\pi^3 && 0 \\\\\n          0 && -\\pi^3\n         \\end{pmatrix} \\\\\n        &= \\begin{pmatrix}\n         \\pi^3 && \\pi^1 - i \\pi^2 \\\\\n         \\pi^1 + i\\pi^2 && -\\pi^3\n         \\end{pmatrix} \\\\\n        &= \\begin{pmatrix}\n         \\pi^0 && \\sqrt{2}\\pi^- \\\\\n         \\sqrt{2}\\pi^+ && -\\pi^0\n         \\end{pmatrix}.\n\\end{align*}\nThen expand your Lagrangian in these fields, \n\\begin{align*}\n    \\bar{N} \\vec{\\pi}.\\vec{\\tau}\\gamma^5 N &= \\begin{pmatrix} \\bar{p} && \\bar{n} \\end{pmatrix}\n         \\begin{pmatrix}\n         \\pi^0 && \\sqrt{2}\\pi^- \\\\\n         \\sqrt{2}\\pi^+ && -\\pi^0\n         \\end{pmatrix}\\gamma^5\n         \\begin{pmatrix}\n          p \\\\ n \n         \\end{pmatrix}\\\\\n&= \\bar{p}\\pi^0\\gamma^5 p + \\sqrt{2} \\bar{p}\\pi^-\\gamma^5 n + \\sqrt{2} \\bar{n} \\pi^+ \\gamma^5 p - \\bar{n} \\pi^0 \\gamma^5 n\n\\end{align*}\nAfter expanding this out you can read off your Feynman rules with these new fields. \n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/402495", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Computation of the self-energy term of the exact propagator for $\\varphi^3$ theory in Srednicki In M. Srednicki \"Quantum field theory\", Section 14 -Loop corrections to the propagator-, the exact propagator $\\mathbf {\\tilde \\Delta} (k^2)$ is stated as  \n$$\\frac{1}{i} \\mathbf {\\tilde \\Delta} (k^2) = \\frac{1}{i} \\tilde \\Delta (k^2) + \\frac{1}{i} \\tilde \\Delta (k^2) [i \\Pi (k^2)] \\frac{1}{i} \\tilde \\Delta (k^2) + O(g^4)\\tag{14.2}$$\nwhere:\n$\\tilde \\Delta (k^2) = \\frac{1}{k^2 + m^2 - i \\epsilon}$ free-field propagator\n$i \\Pi (k^2) = \\frac{1}{2} (i g)^2 (\\frac{1}{i})^2 \\int \\frac{d^d l}{(2 \\pi)^d} \\tilde \\Delta ((l + k)^2) \\tilde \\Delta (l^2) - i (A k^2 + B m^2) + O(g^4)$ self-energy  \nMany pages are then dedicated to the laborious calculation of the self-energy term. I could understand all of the many passages, however almost at the end of the demonstration to compute the expression  \n$$\\Pi (k^2) = \\frac{1}{2} \\alpha \\int_0 ^1 dx D ln (D / D_0) - \\frac{1}{12} \\alpha (k^2 + m^2) + O(\\alpha^2)\\tag{14.43}$$\nwhere:  \n$D = x (1 - x) k^2 + m^2$\n$D_0 = D \\vert_{k^2 = -m^2}$ \nthe integral over $x$ is solved in closed form as  \n$$\\Pi (k^2) = \\frac{1}{12} \\alpha [c_1 k^2 + c_2 m^2 + 2 k^2 f(r)] + O(\\alpha^2)\\tag{14.44}$$\nwhere:  \n$c_1 = 3 - \\pi \\sqrt{3}$\n$c_2 = 3 - 2 \\pi \\sqrt{3}$\n$f(r) = r^3 tanh^{-1}(1/r)$\n$r = (1 + 4 m^2 / k^2)^{1/2}$ \nMy question is: How to move from Eq. (14.43) to Eq. (14.44)? Any hint, or any link where I can find it?\n", "A": "It is very easy to integrate it with Mathematica. From Eq.(14.14) and Eq.(14.42) we have $D=x(1-x)k^2+m^2$ and $D_0=[1-x(1-x)]m^2$, so\n$$\n\\frac\\alpha2\\int_0^1\\text{d}x\\ D\\ln\\frac D{D_0}=\\frac\\alpha{12}\\left[4(k^2+m^2)-\\sqrt3(k^2+2m^2)\\pi+2\\sqrt{\\frac{(k^2+4m^2)^3}{k^2}}\\arctan\\sqrt{\\frac{k^2}{k^2+4m^2}}\\right].\n$$\nTherefore,\n$$\n\\begin{aligned}\n\\Pi(k^2)=&\\ \\frac\\alpha2\\int_0^1\\text{d}x\\ D\\ln\\frac D{D_0}-\\frac\\alpha{12}(k^2+m^2)+\\mathcal O(\\alpha^2) \\\\\n=&\\ \\frac\\alpha{12}\\left[(3-\\pi\\sqrt3)k^2+(3-2\\pi\\sqrt3)m^2+2k^2\\left(1+\\frac{4m^2}{k^2}\\right)^\\frac32\\arctan\\frac1{\\left(1+\\frac{4m^2}{k^2}\\right)^\\frac12}\\right]+\\mathcal O(\\alpha^2) \\\\\n=&\\ \\frac\\alpha{12}\\left[c_1k^2+c_2m^2+2k^2f(r)\\right]+\\mathcal O(\\alpha^2).\n\\end{aligned}\n$$\nThat's all.\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/485832", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Issues with Feynman parameters As a sanity check, I have tried to evaluate a Feynman parameter integral, and have been unable to reproduce the textbook result. I wish to verify the identity\n$$\\frac{1}{ABC} = \\int\\limits_0^1\\int\\limits_0^1\\int\\limits_0^1dxdydz\\frac{2\\delta(x+y+z-1)}{[Ax + By + Cz]^3} ~\\hat{=}~I.$$\nWe can use the delta function to do the integral over $dz$.\n$$I = \\int\\limits_0^1\\int\\limits_0^1 dxdy \\frac{2}{[C + (A-C)x + (B-C)y]^3}.$$\nTo do the integral over $y$ we introduce the substitution\n$$w = C + (A-C)x + (B-C)y$$\nThis will give a factor of $(B-C)^{-1}$, and change the limits\n\\begin{align*}\nI &= \\int\\limits_0^1 \\frac{1}{B-C}dx \\int\\limits_{C + (A-C)x}^{B+(A-C)x}dw \\frac{2}{w^3} \\\\\n&= \\int\\limits_0^1 dx \\frac{1}{B-C}\\left[\\frac{1}{[C+(A-C)x]^2} - \\frac{1}{[B+(A-C)x]^2}\\right]\n\\end{align*}\nThe first integral is the Feynman parameter identity for 2 terms in the denominator, which is much easier to verify and gives $1/AC$. The second can be solved by essentially the same substitution as above.\n$$w = B + (A-C)x$$\nwhich gives a factor of $(A-C)^{-1}$ in similar fashion to before.\n\\begin{align*}\nI &= \\frac{1}{(B-C)AC} - \\frac{1}{(B-C)(A-C)}\\int\\limits_{B}^{A+B-C}\\frac{dw}{w^2} \\\\\n&= \\frac{1}{(B-C)AC} - \\frac{1}{(B-C)(A-C)}\\left[\\frac{1}{B} - \\frac{1}{A+B-C}\\right]\\\\\n&= \\frac{1}{B-C}\\left[\\frac{1}{AC} - \\frac{1}{A-C}\\frac{A-C}{B(A+B-C)}\\right] \\\\\n&= \\frac{1}{B-C}\\frac{B(A+B-C) - AC}{ABC(A+B-C)} \\\\\n&= \\frac{1}{ABC} + \\frac{1}{AB(A+B-C)}\n\\end{align*}\nI don't know where this extra term has come from, and I can't seem to figure out where I went wrong.\n", "A": "The problem is in the very first step sadly. When you resolve the $\\delta$ function you are putting\n$$\nz = 1 - x - y\\,.\n$$\nThis will hold only when $x+y \\leq 1$ because you are integrating only in the region $z \\in [0,1]$. In other words, the zero of the $\\delta$ function sometimes falls outside of the region of integration and thus gives no contribution. The correct way to resolve the $\\delta$ function is\n$$\nI = \\int_0^1 \\mathrm{d}x \\int_{0}^{1-x}\\mathrm{d}y \\frac{2}{[C + (A-C)x + (B-C)y]^3}\\,.\n$$\nIf you follow the same exact steps as you did in your post with the limits modified this way, you'll get the right answer.\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/551626", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Finding common eigenvectors for two commuting hermitian matrices Let $A = \\begin{bmatrix}\n1 &0  &0 \\\\ \n 0&  0& 0\\\\ \n 0&0  &1 \n\\end{bmatrix}$ and  $B = \\begin{bmatrix}\n0 &0  &1 \\\\ \n 0&  1& 0\\\\ \n 1&0  &0 \n\\end{bmatrix}$ the representation of two hermitian operators in a $(\\phi_{1},\\phi_{2},\\phi_{3})$ basis.  Find a common basis of eigenvectors of the two operators...\nSo... is easily shown that both matrices commute and are hermitian, the corresponding eigenvalues and eigenvectors are:\n\n*\n\n*For $A$: $a_1 = 0$  with corresponding $\\begin{bmatrix}\n0\\\\ \n1\\\\\n0 \n\\end{bmatrix}$ Eigenvector, $a_2 = 1$ with corresponding $\\begin{bmatrix}\n1\\\\ \n0\\\\\n0 \n\\end{bmatrix} , \\begin{bmatrix}\n0\\\\ \n0\\\\\n1 \n\\end{bmatrix}$ Eigenvectors\n\n\n*For $B$: $b_1 = 1$ with Eigenvectors $\\begin{bmatrix}\n1\\\\ \n0\\\\\n1 \n\\end{bmatrix}, \\begin{bmatrix}\n0\\\\ \n1\\\\\n0 \n\\end{bmatrix}$ , $b_2 = -1 $ with corresponding $\\begin{bmatrix}\n1\\\\ \n0\\\\\n-1 \n\\end{bmatrix}$ Eigenvector\nHow can I find a common set of Eigenvectors?\n", "A": "Just use the matrix of the eigenvectors of B:\n$$\nU = \\left( \\begin{matrix}\n1&1 &0 \\\\ \n0& 0&1\\\\ \n1& -1&0\n\\end{matrix}  \\right)\n$$\nWith this matrix, you find that:\n$$\nU^{-1}AU = \\left( \\begin{matrix}\n1&0&0 \\\\ \n0& 1&0\\\\ \n0& 0& 0\n\\end{matrix}  \\right)$$\nand $$\nU^{-1}BU = \\left( \\begin{matrix}\n1&0&0 \\\\ \n0& -1&0\\\\ \n0& 0& 1\n\\end{matrix}  \\right),\n$$\nwhich are both diagonal matrices.\n", "meta": {"language": "en", "url": "https://physics.stackexchange.com/questions/723538", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
