{"Q": "Probability of getting imaginary roots Let $X$ and $Y$ be independent and identically distributed uniform random variable over $(0,1)$. Let $S=X+Y$. Find the probability that the quadratic equation $9x^2+9Sx+1=0$ has no real roots.  \nWhat I attempted \nFor no real roots we must have $$(9S)^2-4\\cdot 1\\cdot 9<0$$\nSo, we need $P[(9S)^2-4.1.9<0]=P(9S^2<4)=P(S^2<\\frac{4}{9})=P(-\\frac{2}{3}<S<\\frac{2}{3})=P(0<S<\\frac{2}{3})$\n[As $S$ can never be less than $0$]  \nNow, $\\displaystyle P\\left(0<S<\\frac{2}{3}\\right)=P\\left(0<X+Y<\\frac{2}{3}\\right)=\\int_{0}^{ \\frac{2}{3}}\\int_{0}^{ \\frac{2}{3}-x}f_X(x)\\cdot f_Y(y)\\,dy\\,dx= \\int_{0}^{ \\frac{2}{3}}\\int_{0}^{ \\frac{2}{3}-x}\\,dy\\,dx. $  \nNow, $\\displaystyle \\int_{0}^{ \\frac{2}{3}}\\int_{0}^{ \\frac{2}{3}-x}\\,dy\\,dx=\\int_{0}^{ \\frac{2}{3}}\\left(\\frac{2}{3}-x\\right)\\,dx=\\frac{1}{2}\\left(\\frac{4}{9}\\right)=\\frac{2}{9} $    \nAm I correct ?\n", "A": "Yes.\nAnd nowadays, it's easy to check for gross errors by simulation. Here is a MATLAB simulation:\n>> n = 1e8; sum((9*sum(rand(n,2),2)).^2-36 < 0)/n\nans =\n    0.2223\n\nIn the real world, it's always good to check, or at least partially check, your work by different methods.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/345815", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Conditional probability doubt Assume\n$$P(B|A) = 1/5,\\ P(A) = 3/4,\\ P(A \\cap B) = 3/20,\\ \\textrm{and}\\ P(\u00acB|\u00acA) = 4/7.$$\nFind $P(B)$.\nWhat I tried: $$P(B)=\\dfrac{ P(A \\cap B)}{P(B|A)}=(3/20)/(1/5) = 3/4.$$ \nAnswer is $P(B)=9/35.$\nWhere have I made the mistake? \n", "A": "The probability of B can be split into the probability given A and given not A\n$$P(B) = P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A)$$\nThe negations can be replaced by one minus the actual and vice versa\n$$P(B) = \\frac{1}{5} \\cdot \\frac{3}{4} + (1-P(\\neg B| \\neg A)) \\cdot (1-P(A))$$\n$$P(B) = \\frac{3}{20} + (1-\\frac{4}{7})\\cdot (1-\\frac{3}{4})$$ \n$$P(B) = \\frac{3}{20} + \\frac{3}{7} \\cdot \\frac{1}{4}$$\n$$p(B) = \\frac{3}{20} + \\frac{3}{28}$$\n$$p(B) = \\frac{21}{140} + \\frac{15}{140}$$\n$$P(B) = \\frac{36}{140}$$\n$$P(B) = \\frac{9}{35}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/367300", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Expectation of reciprocal of $(1-r^{2})$ \nIf $r$ is the coefficient of correlation for a sample of $N$ independent observations from a bivariate normal population with population coefficietn of correlation zero, then $E(1-r^2)^{-1}$ is\n  (a) $\\quad\\frac{(N-3)}{(N-4)}$\n\nI tried finding expectation from the density function but then realised that I was solving with the density function of $r$ and not it's square. I don't know the density function of $r^{2}$. I am stuck. Kindly help.\n", "A": "From the problem statement, you are given that a sample of $N$ observations are made from a bivariate normal population with correlation coefficient equal to zero.  Under these assumptions, the probability density function (PDF) for $r$ simplifies greatly to: \n\\begin{eqnarray*}\nf_{R}(r) & = & \\frac{\\left(1-r^{2}\\right)^{\\frac{N-4}{2}}}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{N-2}{2}\\right)}\n\\end{eqnarray*}\nover the support from $-1 < r < 1$ and zero otherwise (see here).\nNow, since we know the PDF of $r$, we can readily find the expected value of the transformation $g(r)=\\left(1-r^{2}\\right)^{-1}$ by applying the Law of the Unconscious Statistician.  This yields:\n\\begin{align}\nE[(1-r^2)^{-1}] & =  \\intop_{-\\infty}^{\\infty}g(r)f_{R}(r)dr \\\\\n & =  \\intop_{-1}^{1}\\frac{\\left(1-r^{2}\\right)^{-1}\\left(1-r^{2}\\right)^{\\frac{N-4}{2}}}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{N-2}{2}\\right)}dr\\\\\n & =  \\frac{1}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{N-2}{2}\\right)}\\intop_{-1}^{1}\\left(1-r^{2}\\right)^{\\frac{N-4}{2}-\\frac{2}{2}}dr\\\\\n & =  \\frac{1}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{N-2}{2}\\right)}\\intop_{-1}^{1}\\left(1-r^{2}\\right)^{\\frac{N-6}{2}}dr \\\\\n & =  \\frac{1}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M}{2}\\right)}\\intop_{-1}^{1}\\left(1-r^{2}\\right)^{\\frac{M-4}{2}}dr&&\\mbox{(Letting $M=N-2$)}\n\\end{align}\n\nYou should note that the last integrand looks similar to the PDF of $r$, $f_R(r)$, and is simply missing a normalizing constant $1/\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M-2}{2}\\right)$.  Multiplying the top and bottom by $\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M-2}{2}\\right)$ (which is simply equal to 1 and does not change the last line) and rearranging terms gives:\n\\begin{eqnarray*}\n\\frac{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M-2}{2}\\right)}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M}{2}\\right)}\\intop_{-1}^{1}\\underbrace{\\frac{\\left(1-r^{2}\\right)^{\\frac{M-4}{2}}}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M-2}{2}\\right)}}_{\\text{PDF of $f_R(r)$ integrates to 1}}dr & = & \\frac{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M-2}{2}\\right)}{\\text{Beta}\\left(\\frac{1}{2},\\,\\frac{M}{2}\\right)}\\\\\n\\end{eqnarray*}.\nNow, we can simply expand out this last term by definition of the Beta function to yield:\n\\begin{eqnarray*}\n\\frac{{{\\Gamma\\left(\\frac{1}{2}\\right)\\Gamma\\left(\\frac{M-2}{2}\\right)}}}{{{\\Gamma\\left(\\frac{1}{2}+\\frac{M-2}{2}\\right)}}}\\frac{{{\\Gamma\\left(\\frac{1}{2}+\\frac{M}{2}\\right)}}}{{{\\Gamma\\left(\\frac{1}{2}\\right)\\Gamma\\left(\\frac{M}{2}\\right)}}} & = & \\frac{{{\\Gamma\\left(\\frac{M-2}{2}\\right)\\Gamma\\left(\\frac{1}{2}+\\frac{M}{2}\\right)}}}{{{\\Gamma\\left(\\frac{1}{2}+\\frac{M-2}{2}\\right)}}\\Gamma\\left(\\frac{M}{2}\\right)}\n\\end{eqnarray*}\n\nTo simplify further, we must make use of an identity of the Gamma function.  The recursion identity of the Gamma function states that for $\\alpha \\gt 0$, $\\Gamma(a+1)=a\\Gamma(a)$. Since $M > 0$, we can apply this recursion identity to the $\\Gamma\\left(\\frac{1}{2}+\\frac{M}{2}\\right)$ term in the numerator and the $\\Gamma\\left(\\frac{M}{2}\\right)$ term in the denominator to get:\n\n\\begin{eqnarray*}\n\\frac{{{\\Gamma\\left(\\frac{M-2}{2}\\right)\\left(\\frac{1}{2}+\\frac{M}{2}-1\\right)\\Gamma\\left(\\frac{1}{2}+\\frac{M}{2}-1\\right)}}}{{{\\Gamma\\left(\\frac{M-1}{2}\\right)}}\\left(\\frac{M}{2}-1\\right)\\Gamma\\left(\\frac{M}{2}-1\\right)} & = & \\frac{{{\\Gamma\\left(\\frac{M-2}{2}\\right)\\left(\\frac{M-1}{2}\\right)\\Gamma\\left(\\frac{M-1}{2}\\right)}}}{{{\\Gamma\\left(\\frac{M-1}{2}\\right)}}\\left(\\frac{M-2}{2}\\right)\\Gamma\\left(\\frac{M-2}{2}\\right)}\\\\\n & = & \\frac{{{\\frac{M-1}{2}}}}{\\frac{M-2}{2}}\\\\\n & = & {{\\left(\\frac{M-1}{2}\\right)}}\\left(\\frac{2}{M-2}\\right)\\\\\n & = & \\frac{M-1}{M-2}\n\\end{eqnarray*}\n\nFinally, if we replace $M$ with $N-2$ from our original definition of $M$, we obtain:\n\\begin{eqnarray*}\n\\frac{(N-2)-1}{(N-2)-2} & = & \\frac{N-2-1}{N-2-2}\\\\\n & = & \\frac{N-3}{N-4}\n\\end{eqnarray*}\nand this completes the proof.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/384897", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Iterated expectations and variances examples Suppose we generate a random variable $X$ in the following way. First we flip a fair coin. If the coin is heads, take $X$ to have a $Unif(0,1)$ distribution. If the coin is tails, take $X$ to have a $Unif(3,4)$ distribution.\nFind the mean and standard deviation of $X$.\nThis is my solution.  I wanted to check if it's correct or if there's a better approach. \nLet $Y$ denote the random variable that is $1$ if the coin lands on a head and $0$ otherwise \nFirstly $\\mathbb{E}(\\mathbb{E}(X|Y)) = \\mathbb{E}(X)$\nThus $\\mathbb{E}(\\mathbb{E}(X|Y)) = \\frac{1}{2} \\cdot \\mathbb{E}(X|Y=0) + \\frac{1}{2} \\cdot \\mathbb{E}(X|Y=1) = \\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{7}{2}=2$\nSecondly $\\mathbb{V}(X) = \\mathbb{E}(\\mathbb{V}(X|Y))+\\mathbb{V}(\\mathbb{E}(X|Y))$\nNow $\\mathbb{V}(X|Y = 0) = \\mathbb{V}(X|Y=1) = \\frac{1}{12}$. Thus $\\mathbb{E}(\\mathbb{V}(X|Y)) = \\frac{1}{12}$. Next calculating $\\mathbb{V}(\\mathbb{E}(X|Y)) = \\mathbb{E}(\\mathbb{E}(X^2|Y)) - (\\mathbb{E}(\\mathbb{E}(X|Y)))^2 = (\\frac{1}{2} \\cdot \\frac{1}{4} + \\frac{49}{4} \\cdot \\frac{1}{2}) - (2)^2 = \\frac{50}{8} - 4.$ \n", "A": "There are generally two ways to approach these types of problems: by (1) Finding the second stage expectation $E(X)$ with the theorem\nof total expectation; or by (2) Finding the second stage expectation\n$E(X)$, using $f_{X}(x)$. These are equivalent methods, but you\nmight find one easier to comprehend, so I present them both in detail\nbelow for $E(X)$. The approach is similar for $Var(X)$, so I exclude\nits presentation, but can update my answer if you really need it.\nMethod (1) Finding the second stage expectation $E(X)$ with the theorem of total expectation\nIn this case, the Theorem of Total Expectation states that:\n\\begin{eqnarray*}\nE(X) & = & \\sum_{y=0}^{1}E(X|Y=y)P(Y=y)\\\\\n & = & \\sum_{y=0}^{1}E(X|Y=y)f_{Y}(y)\n\\end{eqnarray*}\nSo, we simply need to find the corresponding terms in the line above\nfor $y=0$ and $y=1$. We are given the following:\n\\begin{eqnarray*}\nf_{Y}(y) & = & \\begin{cases}\n\\frac{1}{2} & \\text{for}\\,y=0\\,(heads),\\,1\\,(tails)\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\end{eqnarray*}\nand\n\\begin{eqnarray*}\nf_{X|Y}(x|y) & = & \\begin{cases}\n1 & \\text{for}\\,3<x<4;\\,y=0\\\\\n1 & \\text{for}\\,0<x<1;\\,y=1\n\\end{cases}\n\\end{eqnarray*}\nNow, we simply need to obtain $E(X|Y=y)$ for each realization of $y$:\n\\begin{eqnarray*}\nE(X|Y=y) & = & \\int_{-\\infty}^{\\infty}xf_{X|Y}(x|y)dx\\\\\n & = & \\begin{cases}\n\\int_{3}^{4}x(1)dx & \\text{for}\\,y=0\\\\\n\\int_{0}^{1}x(1)dx & \\text{for}\\,y=1\n\\end{cases}\\\\\n & = & \\begin{cases}\n\\left.\\frac{x^{2}}{2}\\right|_{x=3}^{x=4} & \\text{for}\\,y=0\\\\\n\\left.\\frac{x^{2}}{2}\\right|_{x=0}^{x=1} & \\text{for}\\,y=1\n\\end{cases}\\\\\n & = & \\begin{cases}\n\\frac{7}{2} & \\text{for}\\,y=0\\\\\n\\frac{1}{2} & \\text{for}\\,y=1\n\\end{cases}\n\\end{eqnarray*}\nSo, substituting each term into the Theorem of Total Expectation above\nyields:\n\\begin{eqnarray*}\nE(X) & = & \\sum_{y=0}^{1}E(X|Y=y)f_{Y}(y)\\\\\n & = & E(X|Y=0)f_{Y}(0)+E(X|Y=1)f_{Y}(1)\\\\\n & = & \\left(\\frac{7}{2}\\right)\\left(\\frac{1}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\\\\n & = & 2\n\\end{eqnarray*}\nMethod (2) Finding the second stage expectation $E(X)$, using $f_{X}(x)$\nTo use this method, we first find the $f_{X,Y}(x,y)$ and $f_{X}(X)$.\nTo begin, recall that $f_{X,Y}(x,y)$ is given by:\n\\begin{eqnarray*}\nf_{X,Y}(x,y) & = & f_{X|Y}(x|y)f_{Y}(y)\\\\\n & = & \\begin{cases}\n\\left(1\\right)\\left(\\frac{1}{2}\\right) & \\text{for}\\,3<x<4;\\,y=0\\\\\n\\left(1\\right)\\left(\\frac{1}{2}\\right) & \\text{for}\\,0<x<1;\\,y=1\n\\end{cases}\\\\\n\\end{eqnarray*}\nand we can find $f_{X}(x)$ by summing out the $y$ component:\n\\begin{eqnarray*}\nf_{X}(x) & = & \\sum_{y=0}^{1}f_{X,Y}(x,y)\\\\\n & = & f_{X,Y}(x,0)+f_{X,Y}(x,1)\\\\\n & = & \\frac{1}{2}I(3\\le x\\le4)+\\frac{1}{2}I(0\\le x\\le1)\n\\end{eqnarray*}\nAnd now, we can just find $E(X)$ using the probability density function of $f_{X}(x)$ as\nusual:\n\\begin{eqnarray*}\nE(X) & = & \\int_{-\\infty}^{\\infty}xf_{X}(x)dx\\\\\n & = & \\int_{-\\infty}^{\\infty}x\\left[\\frac{1}{2}I(3\\le x\\le4)+\\frac{1}{2}I(0\\le x\\le1)\\right]dx\\\\\n & = & \\frac{1}{2}\\int_{-\\infty}^{\\infty}xI(3\\le x\\le4)dx+\\frac{1}{2}\\int_{-\\infty}^{\\infty}xI(0\\le x\\le1)dx\\\\\n & = & \\frac{1}{2}\\int_{3}^{4}xdx+\\frac{1}{2}\\int_{0}^{1}xdx\\\\\n & = & \\left(\\frac{1}{2}\\right)\\left.\\left(\\frac{x^{2}}{2}\\right)\\right|_{x=3}^{x=4}+\\left(\\frac{1}{2}\\right)\\left.\\left(\\frac{x^{2}}{2}\\right)\\right|_{x=0}^{x=1}\\\\\n & = & \\left(\\frac{1}{2}\\right)\\left(\\frac{7}{2}\\right)+\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\\\\n & = & 2\n\\end{eqnarray*}\nthe same two approaches can be used to compute $Var(X)$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/404102", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 4, "answer_id": 1}}
{"Q": "Can I find $f_{x,y,z}(x,y,z)$ from $f_{x,y+z}(x,y+z)$? Suppose I know densities $f_{x,y+z}(x,y+z)$, $f_y(y)$, $f_z(z)$, and $y$ and $z$ are independent. Given this information, can I derive $f_{x,y,z}(x,y,z)$?\n", "A": "It is tempting to think so, but a simple counterexample with a discrete probability distribution shows why this is not generally possible.\nLet $(X,Y,Z)$ take on the eight possible values $(\\pm1,\\pm1,\\pm1).$  Let $0\\le p\\le 1$ be a number and use it to define a probability distribution $\\mathbb{P}_p$ as follows:\n\n*\n\n*$\\mathbb{P}_p=1/8$ whenever $Y+Z\\ne 0.$\n\n\n*$\\mathbb{P}_p(1,-1,1) = \\mathbb{P}_p(-1,-1,1)=p/4.$\n\n\n*$\\mathbb{P}_p(1,1,-1) = \\mathbb{P}_p(-1,1,-1)=(1-p)/4.$\nThese probabilities evidently are positive and sum to $1:$\n$$\\begin{array}{crrr|rc}\n\\mathbb{P}_p & X & Y & Z & Y+Z & \\Pr(Y,Z)\\\\\n\\hline\n\\frac{1}{8} & \\color{gray}{-1} & \\color{gray}{-1} & \\color{gray}{-1} & -2 & \\frac{1}{4}\\\\\n\\frac{1}{8} & 1 & \\color{gray}{-1} & \\color{gray}{-1} & -2 & \\cdot\\\\\n\\frac{p}{4} & \\color{gray}{-1} & \\color{gray}{-1} & 1 & 0&\\frac{1}{4}\\\\\n\\frac{1-p}{4} & 1 & \\color{gray}{-1} & 1 & 0 & \\cdot\\\\\n\\frac{1-p}{4} & \\color{gray}{-1} & 1 & \\color{gray}{-1} & 0 & \\frac{1}{4}\\\\\n\\frac{p}{4} & 1 & 1 & \\color{gray}{-1} & 0 & \\cdot\\\\\n\\frac{1}{8} & \\color{gray}{-1} & 1 & 1 & 2 & \\frac{1}{4}\\\\\n\\frac{1}{8} & 1 & 1 & 1 & 2 & \\cdot\\\\\n\\end{array}$$\nNote two things:\n\n*\n\n*$Y$ and $Z$ are independent Rademacher variables.  That is, $\\mathbb{P}_p(Y=y,Z=z)=1/4$ for all $y,z\\in\\{-1,1\\}.$\n\n\n*The joint distribution of $(X,Y+Z)$ does not depend on $p,$ as you may check by adding (2) and (3) to deduce that $\\mathbb{P}_p(X=x\\mid Y+Z=0)=1/2$ for $x\\in\\{-1,1\\}.$  (Thus, $X$ is independent of $Y+Z.$)\nThe value of $p$ does not appear in the marginal distributions of $Y$ and $Z$ nor in the joint distribution of $(X,Y+Z).$  Thus, these distributions do not determine $p.$  Nevertheless, different values of $p$ produce different distributions of $(X,Y,Z)$: that's the counterexample.\nIf you must have an example involving continuous distributions (with densities), then add an independent standard trivariate Normal variable to $(X,Y,Z).$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/520274", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 1, "answer_id": 0}}
{"Q": "Integration of a equation $$\\int_{x}^{y}\\left[\\sum_{i=1}^{N}\\sqrt{a}\\cos\\left(\\frac{2\\pi(d_{i}-a)}{\\lambda} \\right)\\right]^{\\!2}da$$\nCan anyone solve this integration for me\nI don't know how the summation and integration will behave with each other\n", "A": "You have to square the sum first. Note that the $\\sqrt{a}$ is common to all terms in\n$$\\sqrt{a}\\cos\\left(\\frac{2\\pi(d_{i}-a)}{\\lambda} \\right)\\cdot \\sqrt{a}\\cos\\left(\\frac{2\\pi(d_{j}-a)}{\\lambda} \\right),$$\nso it can factor out as $a.$ That is, we have\n$$\\int_{x}^{y}a\\left[\\sum_{i=1}^{N}\\cos\\left(\\frac{2\\pi(d_{i}-a)}{\\lambda} \\right)\\right]^{\\!2}da.$$\nWe must now square the sum in order to proceed:\n\\begin{align*}\n&\\phantom{=}\\left[\\sum_{i=1}^{N}\\cos\\left(\\frac{2\\pi(d_{i}-a)}{\\lambda} \\right)\\right]^{\\!2}\\\\\n&=\\sum_{i,j=1}^{N}\\cos\\left(\\frac{2\\pi(d_i-a)}{\\lambda} \\right)\\cos\\left(\\frac{2\\pi(d_j-a)}{\\lambda} \\right)\\\\\n&=\\sum_{i,j=1}^{N}\\cos\\left(\\frac{2\\pi d_i}{\\lambda}-\\frac{2\\pi a}{\\lambda} \\right)\\cos\\left(\\frac{2\\pi d_j}{\\lambda}-\\frac{2\\pi a}{\\lambda}\\right)\\\\\n&=\\frac12\\sum_{i,j=1}^{N}\\left[\\cos\\left(\\frac{2\\pi d_i}{\\lambda}-\\frac{2\\pi d_j}{\\lambda} \\right)+\\cos\\left(\\frac{2\\pi d_i}{\\lambda}+\\frac{2\\pi d_j}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)\\right]\\\\\n&=\\frac12\\sum_{i,j=1}^{N}\\left[\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)+\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)\\right].\n\\end{align*}\nNow we can move the integral inside the sum:\n\\begin{align*}\n\\int&=\\int_x^y\\frac{a}{2}\\sum_{i,j=1}^{N}\\left[\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)+\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)\\right]da\\\\\n&=\\frac12\\sum_{i,j=1}^{N}\\int_x^y a\\left[\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)+\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)\\right]da\\\\\n&=\\frac12\\sum_{i,j=1}^{N}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)da\\right].\n\\end{align*}\nTo continue, we note that according to the comments, there exists $k,\\;1\\le k\\le N$ such that $d_k=a.$ Without loss of generality, we will just assume that $k=N,$ so that $d_N=a.$ Now with the double-sum over $i$ and $j,$ we have four cases to deal with:\n\n*\n\n*$i\\not=N, j\\not=N.$ The integral is then\n\\begin{align*}\n&\\phantom{=}\\frac12\\sum_{i,j=1}^{N-1}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)da\\right]\\\\\n&=\\frac12\\sum_{i,j=1}^{N-1}\\Bigg[\\left(\\frac{y^2}{2}-\\frac{x^2}{2}\\right)\\cos\\left(\\frac{2\\pi(d_i-d_j)}{\\lambda}\\right)\\\\\n&\\qquad+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_i+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)da\\Bigg]\n\\end{align*}\n\n*$i\\not=N, j=N.$ For this case, there are $N-1$ terms, and the integral is\n\\begin{align*}\n&\\phantom{=}\\frac12\\sum_{i=1}^{N-1}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(d_i-a)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_i+a)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)da\\right]\\\\\n&=\\frac12\\sum_{i=1}^{N-1}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(d_i-a)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_i-a)}{\\lambda}\\right)da\\right]\\\\\n&=\\sum_{i=1}^{N-1}\\int_x^y a\\cos\\left(\\frac{2\\pi(d_i-a)}{\\lambda}\\right)da.\n\\end{align*}\n\n*$i=N, j\\not=N.$ For this case, there are $N-1$ terms, and the integral is\n\\begin{align*}\n&\\phantom{=}\\frac12\\sum_{j=1}^{N-1}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(a-d_j)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(a+d_j)}{\\lambda}-\\frac{4\\pi a}{\\lambda}\\right)da\\right]\\\\\n&=\\frac12\\sum_{j=1}^{N-1}\\left[\\int_x^y a\\cos\\left(\\frac{2\\pi(d_j-a)}{\\lambda}\\right)da+\\int_x^ya\\cos\\left(\\frac{2\\pi(d_j-a)}{\\lambda}\\right)da\\right]\\\\\n&=\\sum_{j=1}^{N-1}\\int_x^y a\\cos\\left(\\frac{2\\pi(d_j-a)}{\\lambda}\\right)da.\n\\end{align*}\nThis is the same expression as in Case 2, so we can consolidate these two cases into one case that's doubled: Case 2 and 3:\n$$2\\sum_{j=1}^{N-1}\\int_x^y a\\cos\\left(\\frac{2\\pi(d_j-a)}{\\lambda}\\right)da.$$\n\n*$i=j=N.$ For this case, there's only $1$ term, and the integral is\n$$\\frac12\\left[\\int_x^y a\\,da+\\int_x^ya\\,da\\right]=\\int_x^ya\\,da=\\frac{y^2}{2}-\\frac{x^2}{2}.$$\nThis is getting rather unwieldy to continue writing down. I would just remark that the integral\n$$\\int_x^y a\\cos(c+a)\\,da=\\cos(c+y)+y\\sin(c+y)-\\cos(c+x)-x\\sin(c+x).$$\nAll the remaining integrals are of this form. I'll let you finish.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/527167", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Covariance in the errors of random variables I have two computed variables say $x\\sim N(\\mu_{x}, \\sigma_{x})$ and $y\\sim N(\\mu_y, \\sigma_y)$. Additionally, the $\\sigma_x$ and $\\sigma_y$ are both computed from different types of errors (different components used to compute $\\mu_x$ and $\\mu_y$).\n$$\\begin{align}\n\\sigma_x & = \\sqrt{A_x^2 + B_x^2 + C_x^2 + D_x^2}\\\\\n\\sigma_y & = \\sqrt{A_y^2 + B_y^2 + C_y^2 + D_y^2}\n\\end{align}$$\nMy goal is to find the covariance in $\\sigma_x$ and $\\sigma_y$.\nI know that (assuming A, B, C, D are independent from each other, thus cross terms are zero) for,\n\\begin{align}\n\\text{cov}([A_x, B_x, C_x, D_x], [A_y, B_y, C_y, D_y]) = \\text{cov}(A_x, A_y) + \\text{cov}(B_x, B_y)+ \\text{cov}(C_x, C_y)+ \\text{cov}(D_x, D_y)\n\\end{align}\nHowever, I am stuck when I have to compute $\\text{cov}(\\sqrt{[A_x^2, B_x^2, C_x^2, D_x^2]}, \\sqrt{[A_y^2, B_y^2, C_y^2, D_y^2]})$.\nI am not sure if the relation $\\sqrt{\\text{cov}(A^2, B^2)} = \\text{cov}(A, B)$ works.\nAny help will be appreciated.\nApologies, if this question is not in the right format to ask.\nEDIT:\nFollowing of how $X$ is computed using $A$, $B$, $C$ and $D$,\n\\begin{align}\nX = \\dfrac{A}{B} + C + D\n\\end{align}\n", "A": "Generally speaking, the relation $\\sqrt{Cov(A^2,B^2)}=Cov(A,B)$ does not hold. Consider the following counterexample:\nLet $X\\sim U[0,2\\pi]$ and $Y=\\sin(x), Z=\\cos(X)$. You can see here the proof for $Cov(Y,Z)=0$. Now, let's examine $Cov(Y^2,Z^2)$:\n$$Cov(Y^2,Z^2)=E[(Y^2-E[Y^2])(Z^2-E[Z^2])]=E\\left[(\\sin^2(X)-\\int_0^{2\\pi}\\sin^2(x)dx)(\\cos^2(X)-\\int_0^{2\\pi}\\cos^2(x)dx)\\right]$$\nThe result of both integrals is $\\pi$ so\n$$Cov(Y^2,Z^2)=E\\left[(\\sin^2(X)-\\pi)(\\cos^2(X)-\\pi)\\right]\\\\=E[\\sin^2(X)\\cos^2(X)-\\pi\\sin^2(X)-\\pi\\cos^2(X)+\\pi^2]\\\\=\\int_0^{2\\pi}\\sin^2(x)\\cos^2(x)dx-E[\\pi(\\sin^2(X)+\\cos^2(X))+\\pi^2]\\\\=\\int_0^{2\\pi}{\\sin^2(x)\\cos^2(x)dx}-E[\\pi\\cdot1+\\pi^2]\\\\=\\int_0^{2\\pi}{\\sin^2(x)\\cos^2(x)dx}-\\pi+\\pi^2$$\nThe result of this integral is $\\pi/4$ so overall we get $Cov(Y^2,Z^2)=\\pi^2-\\frac{3}{4}\\pi$ and then $\\sqrt{Cov(Y^2,Z^2)}\\ne Cov(Y,Z)$.\n\nThe covariance of $x,y$ is defined as $Cov(x,y)=E[xy]-E[x]E[y]$. Given $x\\sim N(\\mu_x,\\sigma_x^2), y\\sim N(\\mu_y,\\sigma_y^2)$, we know that $E[x]E[y]=\\mu_x \\mu_y$ and we're left with finding $E[xy]$.\nAs explained here, we can write $xy$ as $\\frac{1}{4}(x+y)^2-\\frac{1}{4}(x-y)^2$ (check it!). For our $x,y$ we get that $$(x+y)\\sim N(\\mu_x+\\mu_y,\\sigma_x^2+\\sigma_y^2+2\\sigma_x\\sigma_y),\\qquad (x-y)\\sim N(\\mu_x-\\mu_y,\\sigma_x^2+\\sigma_y^2-2\\sigma_x\\sigma_y)$$\nDenote $S_+=\\sqrt{\\sigma_x^2+\\sigma_y^2+2\\sigma_x\\sigma_y}$ and $S_-=\\sqrt{\\sigma_x^2+\\sigma_y^2-2\\sigma_x\\sigma_y}$, we get:\n$$\\frac{(x+y)}{S_+}\\sim N\\left(\\frac{\\mu_x+\\mu_y}{S_+},1\\right),\\qquad \\frac{(x-y)}{S_-}\\sim N\\left(\\frac{\\mu_x-\\mu_y}{S_-},1\\right)$$\nNext, we can write\n$$E[xy]=\\frac{1}{4}E[(x-y)^2]-\\frac{1}{4}E[(x-y)^2]=\\frac{1}{4}E\\left[S^2_+\\left(\\frac{x+y}{S_+}\\right)^2\\right]-\\frac{1}{4}E\\left[S^2_-\\left(\\frac{x-y}{S_-}\\right)^2\\right]\\\\=\\frac{1}{4}S^2_+E\\left[\\left(\\frac{x+y}{S_+}\\right)^2\\right]-\\frac{1}{4}S^2_-E\\left[\\left(\\frac{x-y}{S_-}\\right)^2\\right]$$\nNow look at $E\\left[\\left(\\frac{x+y}{S_+}\\right)^2\\right]$: we know that $\\frac{(x+y)}{S_+}\\sim N\\left(\\frac{\\mu_x+\\mu_y}{S_+},1\\right)$ so its square has a chi-square distribution with non-centrality parameter $\\lambda_+=\\frac{(\\mu_x+\\mu_y)^2}{S_+^2}$, thus $E\\left[\\left(\\frac{x+y}{S_+}\\right)^2\\right]=1+\\lambda_+$. In a similar manner, $E\\left[\\left(\\frac{x-y}{S_-}\\right)^2\\right]=1+\\lambda_-$ where $\\lambda_-=\\frac{(\\mu_x-\\mu_y)^2}{S_-^2}$. We overall get:\n$$E[xy]=\\frac{1}{4}S^2_+E\\left[\\left(\\frac{x+y}{S_+}\\right)^2\\right]-\\frac{1}{4}S^2_-E\\left[\\left(\\frac{x-y}{S_-}\\right)^2\\right]\\\\=\n\\frac{1}{4}(S^2_++(\\mu_x+\\mu_y)^2)-\\frac{1}{4}(S^2_-+(\\mu_x-\\mu_y)^2)\\\\=\n\\frac{1}{4}(\\sigma_x^2+\\sigma_y^2+2\\sigma_x\\sigma_y-\\sigma_x^2-\\sigma_y^2+2\\sigma_x\\sigma_y+\\mu^2_x+2\\mu_x\\mu_y+\\mu^2_y-\\mu^2_x+2\\mu_x\\mu_y-\\mu^2_y)\\\\=\\frac{1}{4}(4\\sigma_x\\sigma_y+4\\mu_x\\mu_y)$$\nAnd finally\n$$Cov(x,y)=E[xy]-E[x]E[y]=\\sigma_x\\sigma_y$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/550338", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
