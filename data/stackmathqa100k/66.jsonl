{"Q": "Picking socks probability proof I'm trying to follow the proof from question 1b out of the book \"50 Challenging Problems in Probability\" by Mosteller. The problem states:\nA drawer contains red and black socks. When two socks are drawn at random, the probability that both are red is $1/2$. How small can the number of socks in the drawer be if the number of black socks is even?\nThe answer given starts as follows:\nLet there be $r$ red and $b$ black socks. The probability of the first socks' being red is $\\frac{r}{r+b}$ and if the first sock is red, the probability of the second's being red now that a red has been removed is $\\frac{r-1}{r+b-1}$. Then we require the probability that both are red to be $\\frac{1}{2}$, or $$\\frac{r}{r+b} \\times \\frac{r-1}{r+b-1} = \\frac{1}{2}$$\nNotice that \n$$\\frac{r}{r+b} > \\frac{r-1}{r+b-1} \\quad \\text{for b} >0$$\nTherefore we can create the inequalities \n$$\\left(\\frac{r}{r+b}\\right)^2 > \\frac{1}{2} > \\left(\\frac{r-1}{r+b-1}\\right)^2$$\nThis is where I'm confused. Why is it that $\\left(\\frac{r}{r+b}\\right)^2 > \\frac{1}{2}$? If $r=1, b=100$ then obviously $\\left(\\frac{1}{101}\\right)^2 < \\frac{1}{2}$. Am I missing some obvious assumption?\n", "A": "An explicit way to see the inequality is as follows. Start with the inequality between the multiplied fractions\n$$\\frac{r}{r+b}>\\frac{r-1}{r+b-1}$$\nMultiply both sides by $\\frac{r}{r+b}$:\n$$\\left( \\frac{r}{r+b}\\right)^2>\\frac{r}{r+b}\\times\\frac{r-1}{r+b-1}$$\nThe right hand side of the inequality is just $\\frac{1}{2}$ from the initial probability equation. So\n$$\\left( \\frac{r}{r+b}\\right)^2>\\frac{1}{2}$$\nSimilarly, if you multiplied the original inequality by $\\frac{r-1}{r+b-1}$, you would get:\n$$\\frac{r}{r+b}\\times\\frac{r-1}{r+b-1}=\\frac{1}{2}>\\left(\\frac{r-1}{r+b-1}\\right)^2$$\nCombine the two inequalities so obtained to conclude\n$$\\left( \\frac{r}{r+b}\\right)^2>\\frac{1}{2}>\\left(\\frac{r-1}{r+b-1}\\right)^2$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/104482", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 1}}
{"Q": "$E(X)E(1/X) \\leq (a + b)^2 / 4ab$ I've worked on the following problem and have a solution (included below), but I would like to know if there are any other solutions to this problem, especially more elegant solutions that apply well known inequalities.\nQUESTION: Suppose we have a random variable s.t. $P(a<X<b) =1$  where 0 < a < X < b , a and b both positive constants.\nShow that $E(X)E(\\frac{1}{X}) \\le \\frac{(a+b)^2}{4ab}$\nHint:  find constant c and d s.t. $\\frac{1}{x} \\le cx+d$ when $a<x<b$, and argue that then we shall have $E(\\frac{1}{X}) \\le cE(X)+d$\nMY SOLUTION: For a line $cx+d$ that cuts through $\\frac{1}{X}$ at the points x=a and x = b, it's easy to show that $ c = - \\frac{1}{ab} $  and $d = \\frac{a+b}{ab} $, \nSo, $ \\frac{1}{X} \\le - \\frac{1}{ab} X + \\frac{a+b}{ab} $, and therefore:\n$$ E(\\frac{1}{X}) \\le - \\frac{1}{ab} E(X) + \\frac{a+b}{ab} $$  \n$$ abE(\\frac{1}{X}) + E(X) \\le (a+b) $$\nNow, because both sides of the inequality are positive, it follows that:\n$$ [abE(\\frac{1}{X}) + E(X)]^2 \\le (a+b)^2 $$\n$$ (ab)^2E(\\frac{1}{X})^2 + 2abE(\\frac{1}{X})E(X) + E(X)^2  \\le (a+b)^2 $$ \nThen, for the LHS, we can see that \n$2abE(\\frac{1}{X})E(X) \\le (ab)^2E(\\frac{1}{X})^2 + E(X)^2$ \nbecause  \n$0 \\le (ab)^2E(\\frac{1}{X})^2 - 2abE(\\frac{1}{X})*E(X) + E(X)^2 = [abE(\\frac{1}{X}) - E(X)]^2 $\nSO,\n$$ 4abE(\\frac{1}{X})E(X) \\le (ab)^2E(\\frac{1}{X})^2 + 2abE(\\frac{1}{X})E(X) + E(X)^2  \\le (a+b)^2 $$\nand therefore:\n$$ E(\\frac{1}{X})E(X) \\le \\frac{(a+b)^2}{4ab} $$  Q.E.D.\nThanks for any additional solutions you might be able to provide.  Cheers!\n", "A": "I know it's stated in the problem, but I figured I'd put it in the answer bank:\nFor some line $cx+d$ that cuts through $\\frac{1}{X}$ at the points x=a and x = b, it's easy to show that $ c = - \\frac{1}{ab} $  and $d = \\frac{a+b}{ab} $, \nSo, $ \\frac{1}{X} \\le - \\frac{1}{ab} X + \\frac{a+b}{ab} $, and therefore:\n$$ E(\\frac{1}{X}) \\le - \\frac{1}{ab} E(X) + \\frac{a+b}{ab} $$  \n$$ abE(\\frac{1}{X}) + E(X) \\le (a+b) $$\nNow, because both sides of the inequality are positive, it follows that:\n$$ [abE(\\frac{1}{X}) + E(X)]^2 \\le (a+b)^2 $$\n$$ (ab)^2E(\\frac{1}{X})^2 + 2abE(\\frac{1}{X})E(X) + E(X)^2  \\le (a+b)^2 $$ \nThen, for the LHS, we can see that \n$2abE(\\frac{1}{X})E(X) \\le (ab)^2E(\\frac{1}{X})^2 + E(X)^2$ \nbecause  \n$0 \\le (ab)^2E(\\frac{1}{X})^2 - 2abE(\\frac{1}{X})*E(X) + E(X)^2 = [abE(\\frac{1}{X}) - E(X)]^2 $\nSO,\n$$ 4abE(\\frac{1}{X})E(X) \\le (ab)^2E(\\frac{1}{X})^2 + 2abE(\\frac{1}{X})E(X) + E(X)^2  \\le (a+b)^2 $$\nand therefore:\n$$ E(\\frac{1}{X})E(X) \\le \\frac{(a+b)^2}{4ab} $$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/141766", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Sum of binomial coefficients with increasing $n$ Is there any formula for calculating $$\\binom{n}{1} + \\binom{n+1}{2} + \\binom{n+2}{3} + ... + \\binom{n+m}{m+1}$$\nI have tried iterative method but is there any constant time method exist.\n", "A": "$\\binom{n+1}{2} = \\binom{n}{1} + \\binom{n}{2} = \\binom{n}{1} + \\frac{n-1}{2} \\binom{n}{1}$\n$\\binom{n+2}{3} = \\binom{n+1}{2} + \\binom{n+1}{3} = \\binom{n+1}{2} + \\frac{n-1}{2} \\binom{n+1}{2}$\n$\\binom{n+3}{4} = \\binom{n+2}{3} + \\binom{n+2}{4} = \\binom{n+2}{3} + \\frac{n-1}{3} \\binom{n+2}{3}$\n...\n...\n$\\binom{n+m}{m} = \\binom{n+m-1}{m-1} + \\binom{n+m-2}{m} = \\binom{n+m-1}{m-1} + \\frac{n-1}{m-1}\\binom{n+m-1}{m-1} $\nStart with $n$ and store each value starting from $\\binom{n+1}{2}$ to calculate $\\binom{n+2}{3}$ [O(1) operation] and so on.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/144765", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 2, "answer_id": 0}}
{"Q": "Expectation of joint probability mass function Let the joint probabilty mass function of discrete random variables X and Y be given by\n$f(x,y)=\\frac{x^2+y^2}{25}$, for $(x,y) = (1,1), (1,3), (2,3)$\nThe value of E(Y) is  ?\nAttempt\n$E(Y) = \\sum_{x,y} y\\cdot\\frac{x^2 + y^2}{25}$\n$E(Y) = \\sum_{x,y}\\frac{x^2y + y^3}{25}$\nSubstituting for $(x,y) = (1,1), (1,3), (2,3)$\n$E(Y) = \\frac1{25} + \\frac{30}{25} + \\frac{39}{25}$\n$E(Y) = 2.80$\nIs this right?\n", "A": "\\begin{align}\n\\mathbb E[Y] &= \\sum_y y\\cdot\\mathbb P(Y=y)\\\\\n&= 1\\cdot\\mathbb P(Y=1) + 3\\cdot\\mathbb P(Y=3)\\\\\n&= \\frac{1^2+1^2}{25} + 3\\left(\\frac{1^2+3^2}{25}+\\frac{2^2+3^2}{25} \\right)\\\\\n&= \\frac{71}{25}.\n\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/239111", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 0}}
{"Q": "What is the name of this random variable? Let $X \\sim \\text{Normal}(\\mu, \\sigma^2)$. Define $Y = \\frac{e^X -1}{e^X+1}$. The inverse transformation is $X = \\text{logit}\\left(\\frac{1+Y}{2}\\right) = \\log\\left(\\frac{1+Y}{1-Y} \\right)$. By the transformation theorem\n$$\nf_Y(y) = f_X\\left[ \\log\\left(\\frac{1+y}{1-y}\\right) \\right]\\times\\frac{2}{(1-y)(1+y)}.\n$$\nDoes this distribution have a name that I can look up? I have to evaluate this density pretty often when I use random walk Metropolis-Hastings and I sample for parameters $-1 < Y < 1$ (e.g. correlation parameters, AR(1) parameters, etc.) by transforming them into $X$ first, and then adding Gaussian noise to them.\n", "A": "Thanks to @whuber's comment, we know if $X \\sim \\text{Normal}(\\mu, \\sigma^2)$, then $Z = e^x/(1+e^x)$ follows a $\\text{Logit-Normal}(\\mu, \\sigma)$ and has density \n$$\nf_Z(z) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\frac{1}{z(1-z)}\\exp\\left[-\\frac{(\\text{logit}(z) - \\mu)^2}{2\\sigma^2}\\right].\n$$\nThen $Y = \\frac{e^X -1}{e^X+1} = 2\\left(\\frac{e^X}{1+e^X}\\right)-1 = 2Z-1$ is just a scaled and shifted logit-normal random variable with density\n\\begin{align*}\nf_Y(y) &= f_Z\\left(\\frac{y+1}{2}\\right)\\times \\frac{1}{2} \\\\\n&= \\frac{1}{\\sigma\\sqrt{2\\pi}}\\frac{2}{(1+y)(1-y)}\\exp\\left[-\\frac{\\left\\{\\log\\left(\\frac{1+y}{1-y}\\right) - \\mu\\right\\}^2}{2\\sigma^2}\\right]\\\\\n&= f_X\\left[ \\log\\left(\\frac{1+y}{1-y}\\right) \\right]\\times\\frac{2}{(1-y)(1+y)}.\n\\end{align*}\nNot part of the same family, but still good to know.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/321905", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
