{"Q": "Hellinger distance for two shifted log-normal distributions If I am not mistaken, Hellinger distance between P and Q is generally given by:\n$$\nH^2(P, Q) = \\frac12 \\int \\left( \\sqrt{dP} - \\sqrt{dQ} \\right)^2\n.$$\nIf P and Q, however, are two differently shifted log-normal distributions of the following form\n$$\n{\\frac {1}{(x-\\gamma)\\sigma {\\sqrt {2\\pi \\,}}}}\\exp \\left(-{\\frac {[\\ln (x-\\gamma)-\\mu ]^{2}}{2\\sigma ^{2}}}\\right)\n,$$\nhow would the Hellinger distance then be formed?\nin terms of: $$\\gamma1,\\gamma2, \\mu1, \\mu2 .. etc$$\n", "A": "Note that\n\\begin{align}\nH^2(P, Q)\n  &= \\frac12 \\int (\\sqrt{dP} - \\sqrt{dQ})^2\n\\\\&= \\frac12 \\int dP + \\frac12 \\int dQ - \\int \\sqrt{dP} \\sqrt{dQ}\n\\\\&= 1 - \\int \\sqrt{dP} \\sqrt{dQ}\n,\\end{align}\nand that the density function is 0 if $x \\le \\gamma$.\nThus your question asks to compute\n\\begin{align}\n1 - H^2(P, Q)\n  &= \\int_{\\max(\\gamma_1,\\gamma_2)}^\\infty\n    \\sqrt{\\frac{1}{(x - \\gamma_1) \\sigma_1 \\sqrt{2 \\pi}} \\exp\\left( - \\frac{\\left(\\ln(x - \\gamma_1) - \\mu_1\\right)^2}{2 \\sigma_1^2} \\right)}\n\\\\&\\qquad\\qquad\n    \\sqrt{\\frac{1}{(x - \\gamma_2) \\sigma_2 \\sqrt{2 \\pi}} \\exp\\left( - \\frac{\\left(\\ln(x - \\gamma_2) - \\mu_2\\right)^2}{2 \\sigma_2^2} \\right)}\n    dx\n\\\\&= \\sqrt{\\frac{1}{2 \\pi \\sigma_1 \\sigma_2}} {\\huge\\int}_{\\max(\\gamma_1,\\gamma_2)}^\\infty\n    \\frac{\\exp\\left( - \\frac{\\left(\\ln(x - \\gamma_1) - \\mu_1\\right)^2}{4 \\sigma_1^2} - \\frac{\\left(\\ln(x - \\gamma_2) - \\mu_2\\right)^2}{4 \\sigma_2^2} \\right)}{\\sqrt{(x - \\gamma_1)(x - \\gamma_2)}}\n    dx\n.\\end{align}\nAssume (WLOG) that $\\gamma_1 \\ge \\gamma_2$, and\ndo a change of variables to $y = \\ln(x - \\gamma_1)$, $dy = \\frac{1}{x - \\gamma_1} dx$. Let $\\Delta = \\gamma_1 - \\gamma_2$, so we have\n$\nx - \\gamma_2\n= \\exp(y) + \\Delta\n.$\nThen we get $1 - H^2(P, Q)$ as\n\\begin{align}\n\\sqrt{\\frac{1}{2 \\pi \\sigma_1 \\sigma_2}} \\int_{-\\infty}^\\infty\n    \\exp\\left( - \\frac{\\left(y - \\mu_1\\right)^2}{4 \\sigma_1^2} - \\frac{\\left(\\ln(\\exp(y) + \\Delta) - \\mu_2\\right)^2}{4 \\sigma_2^2} \\right)\n \\sqrt{\\frac{\\exp(y)}{\\exp(y) + \\Delta}}\n    dy\n.\\end{align}\nIf $\\gamma_1 = \\gamma_2$ so $\\Delta = 0$, this works out to\n$$\nH^2(P, Q)\n= 1 -\n\\sqrt{\\frac{2 \\sigma_1 \\sigma_2}{\\sigma_1^2 + \\sigma_2^2}}\n\\exp\\left( - \\frac{(\\mu_1 - \\mu_2)^2}{4 (\\sigma_1^2 + \\sigma_2^2)} \\right)\n.$$\nFor $\\Delta \\ne 0$, though, neither I nor Mathematica made any immediate headway.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/361280", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Range of integration for joint and conditional densities Did I mess up the range of integration in my solution to the following problem ?\nConsider an experiment for which, conditioned on $\\theta,$ the density of $X$ is\n\\begin{align*}\n    f_{\\theta}(x) = \\frac{2x}{\\theta^2},\\,\\,0 < x< \\theta.\n\\end{align*}\nSuppose the prior density for $\\theta$ is \n\\begin{align*}\n    \\pi(\\theta) = 1,\\,\\,\\,0 \\leq \\theta \\leq 1\n\\end{align*}\nFind the posterior density of $\\theta,$ then find $\\mathbb{E}[\\theta|X]$. Do the same for $X = (X_1,\\dots, X_n)$\nwhere $X_1,\\dots, X_n$ are i.i.d and have the density above.\\\nThe joint density of $\\theta$ and $X$ is given by \n\\begin{align*}\n    f_{\\theta}(x)\\pi(\\theta) = \\frac{2x}{\\theta^2},\\,\\,0 < x< \\theta \\leq 1.\n\\end{align*}\nand so the marginal density $g(x)$ of $X$ is given by \n\\begin{align*}\n    g(x)=\\int_{x}^1f_{\\theta}(x)\\pi(\\theta)d\\theta &= \\int_{x}^1\\frac{2x}{\\theta^2}d\\theta\\\\\n    &=2x\\int_{x}^1\\theta^{-2}d\\theta\\\\\n    &=2x[-\\frac{1}{\\theta}]_x^1\\\\\n    &= -2(x -1),\\,\\,\\,0 <x<1\n\\end{align*}\nSo the posterior density of $\\theta$ is \n\\begin{align*}\n    f_x(\\theta) = \\frac{f_{\\theta}(x)\\pi(\\theta)}{g(x)} = \\frac{-x}{(x-1)\\theta^2}, \\,\\, x < \\theta \\leq 1\n\\end{align*}\nand \n\\begin{align*}\n    \\mathbb{E}[\\theta|X]&= \\int_{x}^1\\frac{-x}{x-1}\\theta^{-1}d\\theta\\\\\n    &=\\frac{-x}{x-1}\\ln\\theta|_x^1\\\\\n    &= \\frac{x}{x-1}\\ln x\n\\end{align*}\nNow let $X = (X_1,\\dots, X_n)$ where each $X_i$ has the density above. Then the joint density is \n\\begin{align*}\n    f_{\\theta}(x)\\pi(\\theta) = \\prod_{i = 1}^n\\frac{2x_i}{\\theta^2},\\,\\, 0 < x_{[1]} \\leq x_{[n]} < \\theta \\leq 1\n\\end{align*}\nand so the marginal density $g(x)$ of $X$ is given by \n\\begin{align*}\n    g(x)=\\int_{x_{[n]}}^1f_{\\theta}(x)\\pi(\\theta)d\\theta &= \\int_{x_{[n]}}^1\\prod_{i = 1}^n\\frac{2x_i}{\\theta^2}d\\theta\\\\\n    &=\\prod_{i = 1}^n2x_i\\int_{x_{[n]}}^1\\theta^{-2}d\\theta\\\\\n    &=\\prod_{i = 1}^n2x_i[-\\frac{1}{\\theta}]_{x_{[n]}}^1\\\\\n    &=\\Bigg(\\frac{1}{x_{[n]}} -1\\Bigg) \\prod_{i = 1}^n2x_i,\\,\\,\\,0 <x<1\n\\end{align*}\nand so the posterior density is \n\\begin{align*}\n    f_{x}(\\theta) = \\Bigg(\\prod_{i = 1}^n\\frac{2x_i}{\\theta^2}\\Bigg) \\cdot \\Bigg( \\Bigg(\\frac{1}{x_{[n]}} -1\\Bigg) \\prod_{i = 1}^n2x_i \\Bigg)^{-1}\n\\end{align*}\n", "A": "The univariate case seems correct to me. The multivariate case should be as follows:\n$$\\begin{align*}\n    g(x)=\\int_{x_{[n]}}^1f_{\\theta}(x)\\pi(\\theta)d\\theta &= \\int_{x_{[n]}}^1\\prod_{i = 1}^n\\left(\\frac{2x_i}{\\theta^2}\\right)d\\theta\\\\\n    &=\\left(\\prod_{i = 1}^n2x_i\\right)\\int_{x_{[n]}}^1\\theta^{-2n}d\\theta\\\\\n    &=\\left(\\prod_{i = 1}^n2x_i\\right)\\left[-\\frac{1}{(2n-1)\\theta^{2n-1}}\\right]_{x_{[n]}}^1\\\\\n    &=\\left(\\frac{1}{2n-1}\\right)\\Bigg(\\frac{1}{\\left(x_{[n]}\\right)^{2n-1}} -1\\Bigg) \\left(\\prod_{i = 1}^n2x_i\\right),\\,\\,\\,0 <x<1\n\\end{align*}$$\nThen, the posterior is\n$$\\begin{align*}\n    f_{x}(\\theta) = \\frac{2n-1}{\\theta^{2n}} \\Bigg(\\frac{1}{\\left(x_{[n]}\\right)^{2n-1}} -1\\Bigg)^{-1}, x_{[n]}<\\theta\\leq 1\n\\end{align*}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/431601", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "MLE Derivation for AR Model So I am trying to derive the MLE for an AR(1) model. Here are my thoughts thus far:\nThe AR process is: $z_t = \\delta + \\psi_1z_{t-1} + \\epsilon_t$\nThe expected value of $z_t = \\frac{\\delta}{1 - \\psi_1}$.\nThe variance of $z_t = \\frac{1}{1 - \\psi_1^2}$.\nSo this is where I am getting caught up.\nI have the PDF of $z_t$ as:\n\\begin{align}\nf(z_t;\\theta) &= (2 \\pi \\sigma^2)^{-\\frac{1}{2}} \n    \\exp \\left [-\\frac{1}{2} \\left (\\frac{z_t - \\mathbb{E}[z_t]}{\\sqrt{\\sigma^2}}\\right )  \\right] \\\\\n    &= \\left (2 \\pi \\frac{1}{1 - \\psi_1^2} \\right )^{-\\frac{1}{2}}\n    \\exp \\left [-\\frac{1}{2} \\left (\\frac{z_t - \\frac{\\delta}{1 - \\psi_1}}\n    {\\sqrt{\\frac{1}{1 - \\psi_1^2}}}  \\right )^2  \\right] \\\\\n    &= \\left (2 \\pi \\frac{1}{1 - \\psi_1^2} \\right )^{-\\frac{1}{2}}\n    \\exp \\left [-\\frac{1}{2} \\left (\\frac{ \\left(z_t - \\frac{\\delta}{1 - \\psi_1} \\right )^2}{\\frac{1}{1 - \\psi_1^2}}  \\right )  \\right] \\\\\n    &= \\left (2 \\pi \\frac{1}{1 - \\psi_1^2} \\right )^{-\\frac{1}{2}}\n    \\exp \\left [-\\frac{1 - \\psi_1^2}{2} \\left( z_t - \\frac{\\delta}{1 - \\psi_1} \\right)^2  \\right]\n\\end{align}\nNow, can I assume i.i.d. here? I feel like no because then I would have a time series that is just white noise right? However, if I did assume i.i.d., I would have:\n$\\mathscr{L} = \\prod_{t=1}^T \\left (2 \\pi \\frac{1}{1 - \\psi_1^2} \\right )^{-\\frac{1}{2}}\n    \\exp \\left [-\\frac{1 - \\psi_1^2}{2} \\left( z_t - \\frac{\\delta}{1 - \\psi_1} \\right)^2  \\right]$\nAnd then from here what exactly would my log likelihood function be? I feel like I am totally screwing this up but this is what I have for it:\n$\\ln \\mathscr{L} = -\\frac{T}{2} \\ln \\left ( 2 \\pi \\frac{1}{1 - \\psi_1^2} \\right )\n- \\frac{(1 - \\psi_1^2) \\sum_{t=1}^T \\left (z_t - \\frac{\\delta}{1 - \\psi_1}  \\right )^2}{2}$\nAny help is greatly appreciated! Thank you!!\n", "A": "I'm not directly answering your question, but a quick note on the construction of the likelihood function in your model. The likelihood of a $T$-sized sample of $\\mathbf{e} = \\left[ \\epsilon_{1}, \\, \\epsilon_{2}, \\ldots, \\, \\epsilon_{T} \\right]^{\\mathsf{T}}$ of i.i.d. normal distributed $\\epsilon \\sim N(0,\\sigma^{2})$ is\n$$\nL(\\mathbf{e}) = (2 \\pi \\sigma^{2})^{\\frac{T}{2}} \\exp \\left[ \\frac{- \\mathbf{e}^\\mathsf{T} \\mathbf{e}}{2\\sigma^{2}}   \\right]\n$$\nBut obviously you observe $\\mathbf{z} = \\left[ z_{1}, \\, z_{2}, \\ldots, \\, z_{T} \\right]^{\\mathsf{T}}$ instead of $\\mathbf{e}$. From your AR(1) setup you have $\\mathbf{e} = \\mathbf{G} \\mathbf{z} - \\delta \\mathbf{I}_{T}$ where\n$$\n\\mathbf{G} = \\begin{bmatrix}\n\\sqrt{1-\\psi_{1}^2} & 0 & 0 & \\cdots & 0 \\\\\n-\\psi_{1} & 1 & 0 & \\cdots & 0 \\\\\n0 & -\\psi_{1} & 1 & \\cdots &  0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix},\n$$\n(see Prais & Winsten, eqn. 15). Then\n$$L(\\mathbf{z}) = L(\\mathbf{e}) \\left| \\frac{d \\mathbf{e}}{d\\mathbf{z}} \\right|$$\nwhere $\\left| \\frac{d \\mathbf{e}}{d\\mathbf{z}} \\right|$ is the Jacobian (determiant) of the transformation, in this case $\\operatorname{det} \\mathbf{G}$, which works out to be $\\left| 1 -\\psi_{1}^{2} \\right|^{\\frac{1}{2}}$ because of all the off-diagonal zeros in $\\mathbf{G}$. See Beach & MacKinnon (1978) for more details.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/511402", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 2, "answer_id": 1}}
{"Q": "Posterior distribution of Normal Normal-inverse-Gamma Conjugacy Here is the setting:\nThe likelihood of data is\n\\begin{align}\n    p(\\boldsymbol{x} | \\mu, \\sigma^2) \n    &= (\\frac{1}{2\\pi \\sigma^2})^{\\frac{n}{2}} exp\\left\\{ -\\frac{1}{2\\sigma^2} \\sum\\limits_{i=1}^n (x_i - \\mu)^2 \\right\\} \\nonumber \\\\\n    &= \\frac{1}{(2\\pi)^{n/2}} (\\sigma^2)^{-n/2} exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[  \\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu)^2 \\right] \\right\\}, \\nonumber\n\\end{align}\nand we use the Normal-inverse-Gamma as prior\n\\begin{align}\n    p(\\mu , \\sigma^2) \n    &= \\mathcal{N} (\\mu | \\mu_0 , \\sigma^2 V_0) IG(\\sigma^2 | \\alpha_0 , b_0 ) \\nonumber \\\\\n    &= \\frac{1}{\\sqrt{2\\pi V_0}} \\frac{b_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\frac{1}{\\sigma} (\\sigma^2)^{-\\alpha_0 - 1} exp\\left( -\\frac{1}{2\\sigma^2} [V_0^{-1}(\\mu - \\mu_0 )^2 + 2b_0] \\right). \\nonumber\n\\end{align}\nThen, the posterior can be derivated via\n\\begin{align}\n    p(\\mu , \\sigma^2 | \\boldsymbol{x})\n    &\\propto p(\\boldsymbol{x} | \\mu , \\sigma^2 ) p(\\mu , \\sigma^2) \\nonumber \\\\\n    &\\propto \\left[ (\\sigma^2)^{-n/2} exp \\left( -\\frac{1}{2\\sigma^2} \\big[\\sum\\limits_{i=1}^b (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu )^2\\big] \\right) \\right]  \\times \\left[ \\sigma^{-1} (\\sigma^2)^{-\\alpha_0 - 1} exp \\left( -\\frac{1}{2\\sigma^2} \\left[ V_0^{-1} (\\mu - \\mu_0 )^2 + 2 b_0 \\right] \\right) \\right] \\nonumber \\\\\n    &= \\sigma^{-1} (\\sigma^2)^{-(\\alpha_0 + \\frac{n}{2}) - 1} exp \\left( -\\frac{1}{2\\sigma^2} \\big[ V_0^{-1} (\\mu - m_0 )^2 + 2 b_0 + \\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 + n(\\overline{x} - \\mu)^2 \\big] \\right) \\nonumber \\\\\n    &= \\sigma^{-1} (\\sigma^2)^{-(\\alpha_0 + \\frac{n}{2}) - 1} exp \\Big\\{ -\\frac{1}{2\\sigma^2} \\Big[ (V_0^{-1} + n)(\\mu - \\frac{V_0^{-1} m_0 + n\\overline{x}}{V_0^{-1} + n})^2 + \\big(b_0 + \\frac{1}{2}\\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 + \\frac{V_0^{-1} n}{2(V_0^{-1} + n)} (m_0 - \\overline{x})^2 \\big) \\Big] \\Big\\} \\nonumber\n\\end{align}\nWe recognize this is an unnormalized Normal-inverse-Gamma distribution, therefore\n\\begin{align}\n    p(\\mu , \\sigma^2 | \\boldsymbol{x}) = NIG(\\mu , \\sigma^2 | m_n , V_n , \\alpha_n , b_n ), \\nonumber\n\\end{align}\nwhere\n\\begin{align}\n    m_n &= \\frac{V_0^{-1} m_0 + n \\overline{x}}{V_0^{-1} + n} \\nonumber \\\\\n    V_n^{-1} &= V_0^{-1} + n \\nonumber \\\\\n    \\alpha_n &= \\alpha_0 + \\frac{n}{2} \\nonumber \\\\\n    b_n &= b_0 + \\frac{1}{2}\\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 + \\frac{V_0^{-1} n}{2(V_0^{-1} + n)}(m_0 - \\overline{x})^2. \\nonumber\n\\end{align}\nAs indicated in this paper (see Eq(200)), the last term can be further expressed as\n\\begin{align}\n    b_n &= b_0 + \\frac{1}{2} \\left[ m_0^2 V_0^{-1} + \\sum\\limits_{i=1}^n x_i^2 - m_n^2 V_n^{-1} \\right]. \\nonumber \n\\end{align}\nBut I fail to prove it, i.e.,\n\\begin{align}\n    \\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 + \\frac{V_0^{-1} n}{(V_0^{-1} + n)}(m_0 - \\overline{x})^2 &= \\left[ m_0^2 V_0^{-1} + \\sum\\limits_{i=1}^n x_i^2 - m_n^2 V_n^{-1} \\right]. \\nonumber \n\\end{align}\n", "A": "This is much simpler to prove compared with your earlier question.\n\\begin{align}\n\\sum\\limits_{i=1}^n (x_i - \\overline{x})^2 &+ \\frac{V_0^{-1} n}{(V_0^{-1} + n)}(m_0 - \\overline{x})^2 = \\sum\\limits_{i=1}^n x_i^2 - n\\overline{x}^2\\\\\n&\\qquad + \\frac{V_0^{-1} n}{(V_0^{-1} + n)}(m_0^2 - 2 m_0\\overline{x} +  \\overline{x}^2)\\\\\n&= \\sum\\limits_{i=1}^n x_i^2 - n\\overline{x}^2 + \\frac{V_0^{-1} (n+V_0^{-1}-V_0^{-1})}{(V_0^{-1} + n)}m_0^2\\\\\n&\\quad-2\\frac{V_0^{-1} nm_0\\overline{x}}{(V_0^{-1} + n)}\n+\\frac{(V_0^{-1}+n-n) n}{(V_0^{-1} + n)}\\overline{x}^2\\\\\n&= \\sum\\limits_{i=1}^n x_i^2 + V_0^{-1}m_0^2 - \\frac{V_0^{-2}m_0^2}{(V_0^{-1} + n)}\\\\\n&\\quad -2\\frac{V_0^{-1} m_0n\\overline{x}}{(V_0^{-1} + n)}\n-\\frac{n^2\\overline{x}^2}{(V_0^{-1} + n)}\\\\\n&= V_0^{-1}m_0^2 + \\sum\\limits_{i=1}^n x_i^2 - \\frac{(n\\overline x+V_0^{-1} m_0)^2}{V_n^{-1}}\\\\\n&= V_0^{-1}m_0^2 + \\sum\\limits_{i=1}^n x_i^2 -V_n^{-1}m_n^2\n\\end{align}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/512681", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Derivation of F-distribution from inverse Chi-square? I am trying to derive F-distribution from Chi-square and inverse Chi-square. Somewhere in process I make a mistake and result slightly differs from the canonical form of Fisher-Snedecor F distribution. Please, help find it.\nIn order to derive p.d.f. of F-distribution, let us substitute the p.d.f. of chi-square and inverse chi-square distributions into F-distribution probability density function and integrate it over all possible values of $\\chi^2_n=t$, such that $\\frac{\\chi_n^2}{\\chi_m^2} = x$:\n$f_{\\frac{\\chi_n^2}{\\chi_m^2}}(x) = \\int \\limits_{t=0}^{\\infty} f_{\\chi^2_n}(t) f_{\\frac{1}{\\chi^2_m}}(\\frac{x}{t})dt = \\int \\limits_{t=0}^{\\infty} \\frac{t^{n/2-1}e^{-t/2}}{2^{n/2}\\Gamma(n/2)} \\frac{{\\frac{t}{x}}^{m/2+1}e^{-\\frac{t}{2x}}}{2^{m/2}\\Gamma(m/2)}dt = $\n$ = \\frac{1}{\\Gamma(n/2)\\Gamma(m/2) 2^{\\frac{m+n}{2}} x^{m/2+1}} \\int \\limits_{t=0}^{\\infty}t^{\\frac{n+m}{2}}e^{-(t+\\frac{t}{x})/2}dt = \\frac{1}{\\Gamma(n/2)\\Gamma(m/2) 2^{\\frac{m+n}{2}} x^{m/2+1}} \\int \\limits_{t=0}^{\\infty}t^{\\frac{n+m}{2}}e^{-\\frac{t}{2}(1+\\frac{1}{x})}dt$.\nWe aim to convert our integral into a gamma-function $\\Gamma(n) = \\int \\limits_{0}^{\\infty} z^{n-1}e^{-z}dz$.\nIn order to do that we shall perform a variable substitution $z = \\frac{x+1}{x}\\frac{t}{2}$, hence, $t = \\frac{2x}{x+1}z$. Our integral then will take form of a gamma-function:\n$\\int \\limits_{t=0}^{\\infty}t^{\\frac{n+m}{2}}e^{-\\frac{t}{2}(1+\\frac{1}{x})}dt = \\int \\limits_{z=0}^{\\infty} (\\frac{2zx}{x+1})^{\\frac{n+m}{2}} e^{-z} \\frac{2x}{x+1} dz = (\\frac{x}{x+1})^{\\frac{n+m}{2}+1} \\cdot 2^{\\frac{n+m}{2}+1} \\cdot \\int \\limits_{z=0}^{\\infty} z^{\\frac{n+m}{2}}e^{-z}dz = \\frac{x}{x+1}^{\\frac{n+m}{2}+1} 2^{\\frac{n+m}{2}+1} \\Gamma(\\frac{n+m}{2}+1)$\nSubstituting it into the expression for p.d.f., we get:\n$f_{\\frac{\\chi^2_n}{\\chi^2_m}}(x) = \\frac{\\Gamma(\\frac{n+m}{2}+1)}{\\Gamma(n/2)\\Gamma(m/2)} \\frac{2^{\\frac{n+m}{2}+1}}{2^{\\frac{m+n}{2}}} (\\frac{x}{x+1})^{\\frac{n+m}{2}+1} \\frac{1}{x^{\\frac{m}{2}+1}} = \\frac{2\\Gamma(\\frac{n+m}{2}+1)}{\\Gamma(n/2)\\Gamma(m/2)} \\frac{x^{\\frac{n}{2}}}{(x+1)^{\\frac{n+m}{2}+1}} = \\frac{\\Gamma(\\frac{m+n}{2})}{\\Gamma(\\frac{m}{2}) \\Gamma(\\frac{n}{2})} \\frac{x^{\\frac{n}{2}-1}}{(x+1)^{\\frac{n+m}{2}}} \\frac{2x}{\\frac{n+m}{2}(x+1)}$.\nAs you can see the result differs from the canonical p.d.f. of F distribution $\\frac{\\Gamma(\\frac{m+n}{2})}{\\Gamma(\\frac{m}{2}) \\Gamma(\\frac{n}{2})} \\frac{x^{\\frac{n}{2}-1}}{(x+1)^{\\frac{n+m}{2}}}$ by the multiplier $\\frac{2x}{\\frac{n+m}{2}(x+1)}$. Can you point to a mistake in this derivation?\n", "A": "In\n$$f_{\\frac{\\chi_n^2}{\\chi_m^2}}(x) = \\int \\limits_{t=0}^{\\infty} f_{\\chi^2_n}(t) f_{\\frac{1}{\\chi^2_m}}(\\frac{x}{t})dt$$\nthe Jacobian term is missing.\nIndeed, if $Z\\sim\\chi^2_n$ and $Y\\sim\\chi^{-2}_m$, and if $X=ZY$, the joint density of $(Z,X)$ is\n$$f_{\\chi^2_n}(z) f_{\\chi^{-2}_m}(\\frac{x}{z})\\left|\\frac{\\text dy}{\\text dx}\\right|=f_{\\chi^2_n}(z) f_{\\chi^{-2}_m}(\\frac{x}{z})\\frac{1}{z}$$\nand\n\\begin{align*}f_X(x) &= \\int_0^\\infty f_{\\chi^2_n}(z) f_{\\chi^{-2}_m}(\\frac{x}{z})\\frac{1}{z}\\,\\text dz\\\\\n&= K_{n,m} \\int_0^\\infty z^{n/2-1}e^{-z/2}(x/z)^{-m/2-1}e^{-z/2x}\\frac{1}{z}~\\text dz\\\\\n&= K_{n,m} x^{-m/2-1} \\int_0^\\infty z^{(m+n)/2-1}e^{-(1+x)z/2x}~\\text dz\\\\\n&= K_{n,m} x^{-m/2-1} \\{(1+x)/x\\}^{-(m+n)/2+1-1}\\int_0^\\infty \\zeta^{(m+n)/2-1}e^{-\\zeta/2}~\\text d\\zeta\\\\\n&= K^\\prime_{n,m}\\,\\dfrac{x^{n/2-1}}{(1+x)^{(m+n)/2}}\\\\\n\\end{align*}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/531835", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Cumulative incidence of X Suppose the joint survival function of the latent failure times for two competing risks, $X$ and $Y$, is $S(x,y)=(1-x)(1-y)(1+0.5xy)$, $0<x<1$, $0<y<1$. Find the cumulative incidence function of $X$?\nI first solved the marginal cumulative distribution function of $X$: $(1-x)$. Then I tried to find the joint density function: $1.5-x-y+2xy$, but I am unable to determine how to properly integrate this to find the cumulative incidence function of $X$.\n", "A": "For $0 \\leq x \\leq 1$ the cumulative incidence of $X$ is defined as\n$$\n\\mathbb P \\left (X \\leq x, X \\leq Y \\right)\n$$\nTo compute this probability we need to integrate the joint density of $(X,Y)$,\n$$\nf(x,y) = \\frac{3}{2} -x-y+2xy\n$$\nover the set $\\mathcal{A} \\equiv \\{(u,v) \\in [0,1]^2 \\mid u \\leq x \\wedge u \\leq v \\} $\nThat is,\n\\begin{align*}\n\\mathbb P \\left (X \\leq x , X \\leq Y \\right) &= \\int_\\mathcal{A} f(u,v)\\text{d}u\\text{d}v \\\\\n&= \\int_0^x \\left( \\int_u^1 f(u,v)\\text{d}v \\right )\\text{d}u \\\\\n&= \\int_0^x \\left( \\int_u^1  \\left(\\frac{3}{2} -u-v+2uv \\right)\\text{d}v \\right )\\text{d}u \\\\\n&= \\int_0^x \\left( \\frac{3}{2}(1-u) -u+u^2 - \\left[ \\frac{v^2}{2} \\right]_u^1 +2u \\left[ \\frac{v^2}{2} \\right]_u^1 \\right) \\text{d}u\\\\\n&=\\int_0^x \\left( \\frac{3}{2} - \\frac{3}{2} u -u+u^2 - \\left( \\frac{1}{2}-\\frac{u^2}{2}  \\right) +2u \\left( \\frac{1}{2} - \\frac{u^2}{2} \\right) \\right) \\text{d}u\\\\\n&=\\int_0^x \\left( 1- \\frac{3u}{2} + \\frac{3u^2}{2} -u^3\\right) \\text{d}u\\\\\n&= \\left[ u - \\frac{3u^2}{4} + \\frac{u^3}{3} - \\frac{u^4}{4} \\right ]_0^x \\\\\n&=  x - \\frac{3x^2}{4} + \\frac{x^3}{3} - \\frac{x^4}{4} \\\\\n&= \\frac{12x -9x^2 + 6x^3 - 3x^4}{12}.\n\\end{align*}\nA quick check, take  $x=1$ we have\n$$\n\\frac{12x -9x^2 + 6x^3 -3x^4}{12} = \\frac{1}{2}\n$$ and\n$$\n\\mathbb P \\left (X \\leq 1, X \\leq Y \\right) \\stackrel{(1)}{=} \\mathbb P(X\\leq Y) \\stackrel{(2)}{=}\\frac{1}{2}\n$$\nHere $(1)$ comes from the fact that since $X \\leq 1$ with probability $1$, the event $\\{X \\leq 1 \\cap X \\leq Y\\}$ has the same probability as the event $\\{X \\leq Y\\}$ and since $X$ and $Y$ are somehow \"symmetric\" we have $\\mathbb P(X\\leq Y) = \\mathbb P(Y\\leq X) = \\frac{1}{2}$ hence equality $(2)$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/550004", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Non-IID Uniform Distribution $A$ is uniform (0, 2) and $B$ is uniform(1, 3). Find the Cov$(W, Z)$, where $W=\\min(A,B)$ and $Z=\\max(A,B).$\nSince $WX = AB,$ then by independence of $A$ and $B$,  $E(WZ) = E(A)E(B),$ so that\n$$Cov(WZ) = E(A) E (B) - E (W) E (Z) = (1)(2) - E (W) E (Z)$$\nIt suffices to find E(W) and E(Z) which I have\n\\begin{align*}\nF_{W}(w) = P(W\\le w)  = 1- [(1-P(A< w) )(1-P(B< w ))] \n= 1- \\left[ \\left( 1 - \\frac{w}{2}\\right) \\left( 1 - \\frac{w-1}{2}\\right) \\right] \n= 1 - \\left[ \\left( \\frac{2-w}{2}\\right) \\left(\\frac{3-w}{2}\\right) \\right].\n\\end{align*}\nThen $f_W(w) = -\\frac{2w}{4} - \\frac{5}{4}$ which is negative violating nonnegativity of PDF. Where did I go wrong with the computations?\n\\begin{align*}\nE[W] = E_W (A\\le B) + E_W (A> B) = \\int_0^1\\int_u^3 \\times PDF(W) da \\ db +\\int_0^1\\int_1^u PDF(W) db \\ da\n\\end{align*}\nAssuming we found a correct PDF how to proceed with the expectation? More especially the bounds on the integration?\n", "A": "\n\\begin{align*}\nF_{W}(w) = P(W\\le w)  = 1- [(1-P(A< w) )(1-P(B< w ))] \n= 1- \\left[ \\left( 1 - \\frac{w}{2}\\right) \\left( 1 - \\frac{w-1}{2}\\right) \\right] \n= 1 - \\left[ \\left( \\frac{2-u}{2}\\right) \\left(\\frac{3-u}{2}\\right) \\right].\n\\end{align*}\n\nWhy does the notation $w$ change to $u$?\nAnd the PDF should be\n$$\n1 - F_W(w) = P(A > w, B > w) = \n\\begin{cases}\n\\frac{2 - w}{2}, & 0 < w < 1\\\\\n\\frac{2-w}2\\frac{3-w}{2}, & 1 \\le w < 2 \\\\\n\\end{cases}\n$$\nFor any non negative random variable Y,\n$$\nEY = \\int_0^\\infty (1 - F_Y(y)) dy \n$$\nSupposing the PDF of $W$ I calculated is correct,\n$$\nEW = \\int_0^1 \\frac{2-w}{2} dw + \\int_1^2 \\frac{(2-w)(3-w)}{4} dw = \\frac{23}{24}\n$$\nNote that $W + Z = A + B$, $EZ = EA + EB - EW = \\frac{49}{24}$.\n\nupdate:\nThe support of $W = \\min(A, B)$ can be easily derived by noting that $W \\le A$ and $W \\le B$. (try thinking about the support of $Z$)\nAs for the PDF of $W$,\n$$\n1 - F_W(w) = P(W > w) = P(A > w) P(B > w)\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/603510", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
