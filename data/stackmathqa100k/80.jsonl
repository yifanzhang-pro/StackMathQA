{"Q": "Probability of a sum of probabilistic random variables? Suppose we have $\\mathbb P(A > x) \\leq m$ and $\\mathbb P(B > x) \\leq m$.  What is $\\mathbb P(A + B > y)$?  I have been looking for a related axiom and not had any luck.\n", "A": "Without any additional assumptions (such as independence), we can say the following:\nIf $x > \\frac{1}{2}y$, we can't bound $\\mathbb P(A + B > y)$. In fact, for any $p$, we can find $A$ and $B$ such that $\\mathbb P(A + B > y) = p$ regardless of the value of $m$. Otherwise, when $x \u2264 \\frac{1}{2}y$, we can infer that $\\mathbb P(A + B > y) \u2264 m$.\nTo prove this, first consider the case when $x > \\frac{1}{2}y$. Let $p \u2208 [0, 1]$. Set $\u03b5 = \\frac{1}{2}(x - \\frac{1}{2}y)$. Give $(A, B)$ the following joint distribution:\n\n\n*\n\n*With probability $p$, yield $(\\frac{1}{2}y + \u03b5,\\; \\frac{1}{2}y + \u03b5)$,  \n\n*With probability $1 - p$, yield $(\\frac{1}{2}(y - 1),\\; \\frac{1}{2}(y - 1))$.\n\n\nThen $A + B$ has the following distribution:\n\n\n*\n\n*With probability $p$, yield $\\frac{1}{2}y + \u03b5 + \\frac{1}{2}y + \u03b5 = y + 2\u03b5$.\n\n*With probability $1 - p$, yield $\\frac{1}{2}(y - 1) + \\frac{1}{2}(y - 1) = y - 1$.\n\n\nThus, $\\mathbb P(A + B > y) = p$. To check that $\\mathbb P(A > x) \u2264 m$ (and, similarly, that $\\mathbb P(B > x) \u2264 m$), notice that $A$ is surely at most $\\frac{1}{2}y + \u03b5$, and\n$$\\begin{align}\\tfrac{1}{2}y + \u03b5\n&= \\tfrac{1}{2}y + \\tfrac{1}{2}(x - \\tfrac{1}{2}y) \\\\\n&= \\tfrac{1}{2}y + \\tfrac{1}{2}x - \\tfrac{1}{4}y \\\\\n&= \\tfrac{1}{2}x + \\tfrac{1}{4}y \\\\\n&< \\tfrac{1}{2}x + \\tfrac{1}{2}x \\\\\n&= x,\\end{align}$$\nso $P(A > x) = 0 \u2264 m$.\nNow consider the case when $x \u2264 \\frac{1}{2}y$. Whenever $A + B > y$, $A + B > 2x$, so either $A > x$ or $B > x$. Thus, either $\\mathbb P(A + B > y) \u2264 \\mathbb P(A > x)$ or $\\mathbb P(A + B > y) \u2264 \\mathbb P(B > x)$, so $\\mathbb P(A + B < y) \u2264 m$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/141534", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Why isn't variance defined as the difference between every value following each other? This may be a simple question for many but here it is:\nWhy isn't variance defined as the difference between every value following each other instead of the difference to the average of the values?\nThis would be the more logical choice to me, I guess I'm obviously overseeing some disadvantages. Thanks\n\nEDIT:\nLet me rephrase as clearly as possible. This is what I mean:\n\n*\n\n*Assume you have a range of numbers, ordered: 1,2,3,4,5\n\n*Calculate and sum up (the absolute) differences (continuously, between every following value, not pairwise) between values (without using the average).\n\n*Divide by number of differences\n\n*(Follow-up: would the answer be different if the numbers were un-ordered)\n\n-> What are the disadvantages of this approach compared to the standard formula for variance?\n", "A": "Just a complement to the other answers, variance can be computed as the squared difference between terms:\n$$\\begin{align}\n&\\text{Var}(X) = \\\\\n&\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left(x_i-x_j\\right)^2 = \\\\\n&\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left(x_i - \\overline x -x_j + \\overline x\\right)^2 = \\\\\n&\\frac{1}{2\\cdot n^2}\\sum_i^n\\sum_j^n \\left((x_i - \\overline x) -(x_j - \\overline x\\right))^2 = \\\\\n&\\frac{1}{n}\\sum_i^n \\left(x_i - \\overline x \\right)^2\n\\end{align}$$\nI think this is the closest to the OP proposition. Remember the variance is a measure of dispersion of every observation at once, not only between \"neighboring\" numbers in the set.\n\nUPDATE\nUsing your example: $X = {1, 2, 3, 4, 5}$. We know the variance is $Var(X) = 2$.\nWith your proposed method $Var(X) = 1$, so we know beforehand taking the differences between neighbors as variance doesn't add up. What I meant was taking every possible difference squared then summed:\n$$Var(X) = \\\\ = \\frac{(5-1)^2+(5-2)^2+(5-3)^2+(5-4)^2+(5-5)^2+(4-1)^2+(4-2)^2+(4-3)^2+(4-4)^2+(4-5)^2+(3-1)^2+(3-2)^2+(3-3)^2+(3-4)^2+(3-5)^2+(2-1)^2+(2-2)^2+(2-3)^2+(2-4)^2+(2-5)^2+(1-1)^2+(1-2)^2+(1-3)^2+(1-4)^2+(1-5)^2}{2 \\cdot 5^2} = \\\\\n=\\frac{16+9+4+1+9+4+1+1+4+1+1+4+1+1+4+9+1+4+9+16}{50} = \\\\\n=2$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/225734", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "20", "answer_count": 8, "answer_id": 6}}
{"Q": "PDF of $X^2+2aXY+bY^2$ It is my first post on this forum. I am not a mathematician (so excuse me if I don't use the right vocabulary). I have two independent Normal random variables $X$ and $Y$:\n\\begin{aligned}\nX&\\sim N(0,\\sigma^{2})\\\\ \nY&\\sim N(0,s^{2})\n\\end{aligned}\nHow can I find the PDF of: $$J=X^2+2aXY+bY^2$$\nwhere $b$ is positive, $a$ can be negative but $|a|<b$.\nI've done some simulations in MATLAB, and it seems that the PDF is exponential (i.e. $\\rho_J(J) \\propto e^{-J/J_0})$.\nDoes anyone have an idea to calculate $\\rho_J(J)$ ?\nThank you!\n", "A": "First of all, $J$ can be rewritten like this:\n$$J=\\frac{b-a^2}{b} X^2+b\\left(\\frac{a}{b}X+Y \\right)^2$$\nThis way, you can easily see that $J$ must be non-negative and that $J\\ge \\frac{b-a^2}{b} X^2$ which restricts what $X$ can be if you know $J$.\nNow, find the cumulative distribution function:\n$$P[J \\le t]=P\\left[\\frac{b-a^2}{b} X^2+b\\left(\\frac{a}{b}X+Y \\right)^2\\le t \\right]$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[\\frac{b-a^2}{b} x^2+b\\left(\\frac{a}{b}x+Y \\right)^2\\le t \\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[b\\left(\\frac{a}{b}x+Y \\right)^2\\le t-\\frac{b-a^2}{b} x^2 \\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[\\left(\\frac{a}{b}x+Y \\right)^2\\le \\frac{t-\\frac{b-a^2}{b} x^2}b \\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[\n-\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} \\le \\frac{a}{b}x+Y \\le \\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} \n\\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[\n-\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} -\\frac{a}{b}x\\le Y \\le \\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b}-\\frac{a}{b}x \n\\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \nP\\left[\n\\frac{-\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} -\\frac{a}{b}x}{\\sigma'} \\le \\frac{Y}{\\sigma'} \\le \\frac{\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} -\\frac{a}{b}x}{\\sigma'} \n\\right] f_X(x)dx$$\n$$=\\int_{-\\sqrt{\\frac{b}{b-a^2}t}}^{\\sqrt{\\frac{b}{b-a^2}t}} \n\\left(\\Phi\\left( \\frac{\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} -\\frac{a}{b}x}{\\sigma'} \\right)-\n\\Phi\\left( \\frac{-\\sqrt{\\frac{t-\\frac{b-a^2}{b} x^2}b} -\\frac{a}{b}x}{\\sigma'} \\right)\\right)  \\frac{1}{\\sigma} \\phi(\\frac{x}{\\sigma}) dx$$\nwhere $\\Phi$ is the standard normal distribution function and $\\phi$ is the standard normal density function.\nDifferentiate with respect to $t$ to find the density function.\nSince the limts of the integral are functions of $t$, you can use Leibniz' rule to do this.\nYou will then need to use numerical integration to evaluate it because there is still an integral.  It doesn't look like a simple known distribution such as the Exponential. The reason your simulations suggest it might be exponential is that it is positive and possibly for particular values of $a$ and $b$ and the standard deviations it looks close to an Exponential. Try other values of $a$ and $b$ and standard deviations.\nThe mean and variance of $J$ are:\n$$E[J]=\\sigma^2+b\\sigma'^2$$\n$$Var[J]=2(\\sigma^4+2a^2\\sigma^2 \\sigma'^2+b^2\\sigma'^4)$$\nFor an exponential random variable, the variance is the square of the mean.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/507303", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Why the variance of Maximum Likelihood Estimator(MLE) will be less than Cramer-Rao Lower Bound(CRLB)? Consider this example. Suppose we have three events to happen with probability $p_1=p_2=\\frac{1}{2}\\sin ^2\\theta ,p_3=\\cos ^2\\theta $ respectively. And we suppose the true value $\\theta _0=\\frac{\\pi}{2}$. Now if we do $n$ times experiments, we will only see event 1 and event 2 happen. Hence the MLE should be\n$$\\mathrm{aug} \\underset{\\theta}{\\max}\\left[ \\frac{1}{2}\\sin ^2\\theta \\right] ^{m_1}\\left[ \\frac{1}{2}\\sin ^2\\theta \\right] ^{m_2}$$\nwhere $m_1$ is how many times event 1 happened and $m_2$ is how many times event 2 happened.\nObviously, the solution to the above optimization problem(also it's our MLE) is $\\pi/2$ which is a constant and has nothing to do with $m_1,m_2$.\nBut let's see what is our fisher information, whose inverse should give us a lower bound of the variance of any unbiased estimator:\n$$\\begin{align}\n&\\frac{\\left[ \\partial _{\\theta}\\frac{1}{2}\\sin ^2\\theta \\right] ^2}{\\frac{1}{2}\\sin ^2\\theta}+\\frac{\\left[ \\partial _{\\theta}\\frac{1}{2}\\sin ^2\\theta \\right] ^2}{\\frac{1}{2}\\sin ^2\\theta}+\\frac{\\left[ \\partial _{\\theta}\\cos ^2\\theta \\right] ^2}{\\cos ^2\\theta}\n\\\\\n&=2\\frac{\\left[ \\partial _{\\theta}\\frac{1}{2}\\sin ^2\\theta \\right] ^2}{\\frac{1}{2}\\sin ^2\\theta}+\\frac{\\left[ \\partial _{\\theta}\\cos ^2\\theta \\right] ^2}{\\cos ^2\\theta}\n\\\\\n&=4\\cos ^2\\theta +4\\sin ^2\\theta \n\\\\\n&=4\n\\end{align}$$\nHence there's a conflict, where do I understand wrong?\nThanks in advance!\nEdit\nIf I calculate MLE not based on the observed data, but instead, based on all the possible outcomes before we do experiments, it should be:\n$$\\mathrm{aug}\\underset{\\theta}{\\max}\\left[ \\frac{1}{2}\\sin ^2\\theta \\right] ^{m_1}\\left[ \\frac{1}{2}\\sin ^2\\theta \\right] ^{m_2}\\left[ \\cos ^2\\theta \\right] ^{m_3}$$\ntaking ln we will have\n$$\\left( m_1+m_2 \\right) \\left( 2\\ln\\sin \\theta -\\ln 2 \\right) +2m_3\\ln\\cos \\theta $$\ntaking derivative w.r.t. $\\theta$ and set derivative to be zero we will get\n$$2\\left( m_1+m_2 \\right) \\frac{\\cos \\theta}{\\sin \\theta}=2m_3\\frac{\\sin \\theta}{\\cos \\theta}$$\nHence if true value $\\theta_0=\\pi/2$, $m_3$ will always be $0$, and we will always have a conclusion that $\\hat\\theta=\\pi/2$ which will have no variance at all. Hence the variance of $\\hat\\theta$ is $0$ which against the rule of Cramer-Rao bound.\n", "A": "The first issue you have here is that your likelihood function does not appear to match the description of your sampling mechanism.  You say that you only observe events 1 and 2 happen, but if the sample size $n$ is known then this still fixes the number of times that event 3 happens (since $m_1 + m_2 + m_3 = n$).  Taking an observation $\\mathbf{m} \\sim \\text{Mu}(n, \\mathbf{p})$ gives the likelihood function:\n$$L_\\mathbf{m}(\\theta)\n= \\bigg( \\frac{1}{2} \\sin^2 (\\theta) \\bigg)^{m_1 + m_2} \\bigg( \\cos^2 (\\theta) \\bigg)^{n - m_1 - m_2},$$\nwhich gives the log-likelihood:\n$$\\ell_\\mathbf{m}(\\theta)\n= \\text{const} + (m_1 + m_2) \\log (\\sin^2 (\\theta)) + (n - m_1 - m_2) \\log (\\cos^2 (\\theta)).$$\nAs you can see, the likelihood function has an extra term that you have not included in your question.  We can see that $M_* = M_1+M_2 \\sim \\text{Bin}(n, \\sin^2 (\\theta))$ is a sufficient statistic in this problem so the problem is essentially one of binomial inference with a transformed probability parameter.  With a bit of calculus it can be shown that the MLE solves:\n$$\\sin^2 (\\hat{\\theta}) = \\frac{m_1 + m_2}{n}\n\\quad \\quad \\quad \\implies \\quad \\quad \\quad \n\\hat{\\theta} = \\text{arcsin} \\bigg( \\sqrt{\\frac{m_1 + m_2}{n}} \\bigg).$$\nThis estimator for the parameter is generally biased (it is unbiased in the case where $\\sin^2 (\\theta) = \\tfrac{1}{2}$) so the applicable version of the Cram\u00e9r\u2013Rao lower bound in this case is the generalisation for biased estimators:\n$$\\mathbb{V}(\\hat{\\theta}) \\geqslant \\frac{|\\psi'(\\theta)|}{I(\\theta)}\n\\quad \\quad \\quad \\quad \\quad\n\\psi(\\theta) \\equiv \\mathbb{E}(\\hat{\\theta}).$$\nThe expectation function is:\n$$\\begin{align}\n\\psi(\\theta)\n&= \\sum_{m=0}^n \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\cdot \\text{Bin} (m|n,\\sin^2 (\\theta)) \\\\[6pt]\n&= \\sum_{m=0}^n {n \\choose m} \\cdot \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\cdot \\sin^{2m} (\\theta) \\cdot \\cos^{2(n-m)} (\\theta). \\\\[6pt]\n&= \\frac{\\pi}{2} \\cdot \\sin^{2n} (\\theta) - \\frac{\\pi}{2} \\cdot \\cos^{2n} (\\theta) + \\sum_{m=1}^{n-1} {n \\choose m} \\cdot \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\cdot \\sin^{2m} (\\theta) \\cdot \\cos^{2(n-m)} (\\theta), \\\\[6pt]\n\\end{align}$$\nand its derivative (which appears in the bound) is:\n$$\\begin{align}\n\\psi'(\\theta) \n&= \\sum_{m=0}^n {n \\choose m} \\cdot \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\frac{d}{d\\theta} \\bigg[ \\sin^{2m} (\\theta) \\cdot \\cos^{2(n-m)} (\\theta) \\bigg] \\\\[6pt]\n&= n\\pi \\cdot \\sin(\\theta) \\cos(\\theta) [\\sin^{2(n-1)} (\\theta) +  \\cos^{2(n-1)} (\\theta)] \\\\[6pt]\n&\\quad + \n\\sum_{m=1}^{n-1} {n \\choose m} \\cdot \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\sin^{2m-1} (\\theta) \\cos^{2(n-m)-1} (\\theta) \\\\[6pt]\n&\\quad \\quad \\quad \\quad \\quad \\times \\bigg[ 2m \\cos^2 (\\theta) - 2(n-m) \\sin^2 (\\theta) \\bigg]. \\\\[6pt]\n&= n\\pi \\cdot \\sin(\\theta) \\cos(\\theta) [\\sin^{2(n-1)} (\\theta) +  \\cos^{2(n-1)} (\\theta)] \\\\[6pt]\n&\\quad + \n2 \\sum_{m=1}^{n-1} {n \\choose m} \\cdot \\text{arcsin} \\bigg( \\sqrt{\\frac{m}{n}} \\bigg) \\sin^{2m-1} (\\theta) \\cos^{2(n-m)-1} (\\theta) (m - n \\sin^2 (\\theta)). \\\\[6pt]\n\\end{align}$$\nAs you can see, this is a more complicated form for the Cram\u00e9r\u2013Rao lower bound.  Nevertheless, it should hold in this problem.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/592676", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "7", "answer_count": 3, "answer_id": 0}}
