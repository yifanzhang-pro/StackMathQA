{"Q": "Bounding the difference between square roots I want to compute the value of $\\frac{1}{\\sqrt{a + b + c}}$.  Say I can observe a and b, but not c.  Instead, I can observe d which is a good approximation for c in the sense that $P( |c-d| \\leq 0.001 )$ is large (say 95%), and both c and d are known to have $|c| \\leq 1, |d| \\leq 1$ so a difference of 0.001 is actually small.\nI want to argue that $\\frac{1}{\\sqrt{a + b + d}}$ is a good approximation because d is a good approximation for c.  Is there anything I can say about the difference\n$\\frac{1}{\\sqrt{a + b + c}} - \\frac{1}{\\sqrt{a + b + d}}$?\nMaybe I could say something like $P(\\frac{1}{\\sqrt{a + b + c}} - \\frac{1}{\\sqrt{a + b + d}} \\leq 0.001) = x?$  I'm worried that being under a square root might mess things up.\nWould I need to find out the exact distribution of $c - d$, or anything else before I can make such claims?\n", "A": "$\\frac{1}{\\sqrt{a + b + c}} - \\frac{1}{\\sqrt{a + b + d}} = \\frac{\\sqrt{a + b + d}-\\sqrt{a + b + c}}{\\sqrt{a + b + c}\\sqrt{a + b + d}} $\n$ =\\frac{(\\sqrt{a + b + d}+\\sqrt{a + b + c})(\\sqrt{a + b + d}-\\sqrt{a + b + c})}{(\\sqrt{a + b + d}+\\sqrt{a + b + c})\\sqrt{a + b + c}\\sqrt{a + b + d}}$\n$ =\\frac{(d-c)}{(\\sqrt{a + b + d}+\\sqrt{a + b + c})\\sqrt{a + b + c}\\sqrt{a + b + d}}$\nDenominator is positive.  This difference is smaller when denominator is greater than 1.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/20409", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 3, "answer_id": 1}}
{"Q": "Decision Tree Probability - With Back Step For the below decision tree, I can see how the probabilities of each end state are calculated... simply multiply the previous decisions:\n\nBut for this one below, I'm totally stumped. It seems in my head the chance at resetting back to the first decision is completely negated because essentially the whole decision restarts like the first decision was never made.  But based on the end probabilities this gives the s3 a larger chance at being chosen?\n\nWhat does the math behind this look like? How are those final probabilities calculated given the reset in the decision tree?\n", "A": "The probability of arriving at S1 is \n$$\\frac 12\\cdot \\frac 29 \n+ \\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)\\cdot\\frac 29\n+\\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)^2\\cdot\\frac 29\n+\\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)^3\\cdot\\frac 29\n+ \\cdots\\\\\n= \\frac 19\\cdot \\left[1 + \\left(\\frac 29\\right) \n+ \\left(\\frac 29\\right)^2 + \\left(\\frac 29\\right)^3 + \\cdots \\right]\\\\\n= \\frac 19 \\cdot \\frac{1}{1-\\frac 29} = \\frac 17 = \\frac{2}{14}.$$\nA similar calculation (replacing the trailing $\\displaystyle\\frac 29$'s\nby\n$\\displaystyle\\frac 39$'s gives the probability of arriving at S2 as \n$\\displaystyle \\frac{3}{14}$.\nAt this point, we can jump to the conclusion that the probability of\narriving at S3 must be $\\displaystyle\\frac{9}{14}$ without sullying our hands with\nmore summations of geometric series, but more skeptical folks can work with\n$$\\frac 12 \n+ \\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)\n+\\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)^2\n+\\frac 12\\cdot \\left(\\frac 49\\cdot \\frac 12\\right)^3\n+ \\cdots\\\\$$\nwhich looks a lot like the sum on the second line of this\nanswer except for those trailing $\\displaystyle \\frac 29$'s, and so\nwe get that the probability of arriving at S3 is $\\displaystyle\\frac 92$\ntimes the probability of arriving at S2, which gives us\n$\\displaystyle \\frac 92\\cdot \\frac{2}{14} = \\frac{9}{14}$.\nLook, Ma! No more summations of geometric series! \n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/242996", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Cdf of the joint density $f(x, y) = \\frac{3}{2 \\pi} \\sqrt{1-x^2-y^2}$ $$f(x, y) = \\frac{3}{2\\pi} \\sqrt{1-x^2-y^2}, \\quad x^2 + y^2 \\leq 1$$\n\nFind the cdf $F(x, y)$. To do this, we need to compute the integral\n$$ \\int_{-1}^{x} \\int_{-1}^{y} \\frac{3}{2\\pi} \\sqrt{1-u^2-v^2} dv du .$$\nThis is where I'm stuck. Converting to polar coordinates wouldn't seem to help since the area to be integrated isn't a circle.\nEdit: Adjusting the limits of integration to \n$$\\int_{-1}^{x} \\int_{-\\sqrt{1-x^2}}^{y} \\frac{3}{2\\pi} \\sqrt{1-u^2-v^2} dv du,$$\nI tried using the substitution \n$$v=\\sqrt{1\u2212u^2}\\sin{\\theta}, \\quad dv=\\sqrt{1\u2212u^2}cos{\\theta}d\\theta$$ \nThen $\\theta = \\arcsin(\\frac{v}{\\sqrt{1-u^2}})$, giving\n$$\\int_{-1}^{x} \\int_{\\arcsin(-\\frac{\\sqrt{1-x^2}}{\\sqrt{1-u^2}})}^{\\arcsin(\\frac{y}{\\sqrt{1-u^2}})} \\frac{3}{2\\pi} \\sqrt{1-u^2-(\\sqrt{1\u2212u^2}\\sin{\\theta})^2} \\sqrt{1\u2212u^2}cos{\\theta}d\\theta du$$\n$$=\\int_{-1}^{x} \\frac{3}{2\\pi} (1\u2212u^2) \\int_{\\arcsin(-\\frac{\\sqrt{1-x^2}}{\\sqrt{1-u^2}})}^{\\arcsin(\\frac{y}{\\sqrt{1-u^2}})} cos^2{\\theta}d\\theta du$$\n$$=\\int_{-1}^{x} \\frac{3}{2\\pi} (1\u2212u^2) \\int_{\\arcsin(-\\frac{\\sqrt{1-x^2}}{\\sqrt{1-u^2}})}^{\\arcsin(\\frac{y}{\\sqrt{1-u^2}})} \\frac{1}{2}[1+cos(2\\theta)]d\\theta du$$\n$$=\\int_{-1}^{x} \\frac{3}{4\\pi} (1\u2212u^2) [\\theta+\\frac{1}{2}\\sin(2\\theta)]_{\\arcsin(-\\frac{\\sqrt{1-x^2}}{\\sqrt{1-u^2}})}^{\\arcsin(\\frac{y}{\\sqrt{1-u^2}})} du$$\nAt this point the integral got really messy to deal with. Is there another way to approach this?\n", "A": "This problem will be easier to solve if you first try and visualize what the joint pdf looks as a surface above the $x$-$y$ plane in three-dimensional space. Hint: ignoring the scale factor $\\frac{3}{2\\pi}$, what is the surface defined by \n$$z = \\begin{cases}\\sqrt{1 - x^2 - y^2}, & x^2+y^2 \\leq 1,\\\\\n0, &\\text{otherwise}, \\end{cases}~~~~?$$ Can you think of ways that you might be able to compute the volume between this surface and the $x$-$y$ plane in the region $\\{(x,y) \\colon x\\leq a, y \\leq b\\}$. The visualization will also help you set the lower and upper limits on the integrals as suggested by JarleTufto.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/249345", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Distribution of $\\sqrt{X^2+Y^2}$ when $X,Y$ are independent $U(0,1)$ variables \nAs a routine exercise, I am trying to find the distribution of $\\sqrt{X^2+Y^2}$ where  $X$ and $Y$ are independent $ U(0,1)$ random variables.\n\nThe joint density of $(X,Y)$ is  $$f_{X,Y}(x,y)=\\mathbf 1_{0<x,y<1}$$\nTransforming to polar coordinates $(X,Y)\\to(Z,\\Theta)$ such that $$X=Z\\cos\\Theta\\qquad\\text{ and }\\qquad Y=Z\\sin\\Theta$$ \nSo, $z=\\sqrt{x^2+y^2}$ and $0< x,y<1\\implies0< z<\\sqrt 2$.\nWhen $0< z<1$, we have $0< \\cos\\theta<1,\\,0<\\sin\\theta<1$ so that $0<\\theta<\\frac{\\pi}{2}$.\nWhen $1< z<\\sqrt 2$, we have $z\\cos\\theta<\\implies\\theta>\\cos^{-1}\\left(\\frac{1}{z}\\right)$, as $\\cos\\theta$ is decreasing on $\\theta\\in\\left[0,\\frac{\\pi}{2}\\right]$; and $z\\sin\\theta<1\\implies\\theta<\\sin^{-1}\\left(\\frac{1}{z}\\right)$, as $\\sin\\theta$ is increasing on $\\theta\\in\\left[0,\\frac{\\pi}{2}\\right]$.\nSo, for $1< z<\\sqrt 2$, we have $\\cos^{-1}\\left(\\frac{1}{z}\\right)<\\theta<\\sin^{-1}\\left(\\frac{1}{z}\\right)$.\nThe absolute value of jacobian of transformation is $$|J|=z$$\nThus the joint density of $(Z,\\Theta)$ is given by \n$$f_{Z,\\Theta}(z,\\theta)=z\\mathbf 1_{\\{z\\in(0,1),\\,\\theta\\in\\left(0,\\pi/2\\right)\\}\\bigcup\\{z\\in(1,\\sqrt2),\\,\\theta\\in\\left(\\cos^{-1}\\left(1/z\\right),\\sin^{-1}\\left(1/z\\right)\\right)\\}}$$\nIntegrating out $\\theta$, we obtain the pdf of $Z$ as\n$$f_Z(z)=\\frac{\\pi z}{2}\\mathbf 1_{0<z<1}+\\left(\\frac{\\pi z}{2}-2z\\cos^{-1}\\left(\\frac{1}{z}\\right)\\right)\\mathbf 1_{1<z<\\sqrt 2}$$\n\nIs my reasoning above correct? In any case, I would like to avoid this method and instead try to find the cdf of $Z$ directly. But I couldn't find the desired areas while evaluating $\\mathrm{Pr}(Y\\le \\sqrt{z^2-X^2})$ geometrically.\n\nEDIT.\nI tried finding the distribution function of $Z$ as\n\\begin{align}\nF_Z(z)&=\\Pr(Z\\le z)\n\\\\&=\\Pr(X^2+Y^2\\le z^2)\n\\\\&=\\iint_{x^2+y^2\\le z^2}\\mathbf1_{0<x,y<1}\\,\\mathrm{d}x\\,\\mathrm{d}y\n\\end{align}\nMathematica says this should reduce to\n$$F_Z(z)=\\begin{cases}0 &,\\text{ if }z<0\\\\ \\frac{\\pi z^2}{4} &,\\text{ if } 0< z<1\\\\ \\sqrt{z^2-1}+\\frac{z^2}{2}\\left(\\sin^{-1}\\left(\\frac{1}{z}\\right)-\\sin^{-1}\\left(\\frac{\\sqrt{z^2-1}}{z}\\right)\\right) &,\\text{ if }1< z<\\sqrt 2\\\\ 1 &,\\text{ if }z>\\sqrt 2 \\end{cases}$$\nwhich looks like the correct expression. Differentiating $F_Z$ for the case $1< z<\\sqrt 2$ though brings up an expression which doesn't readily simplify to the pdf I already obtained.\nFinally, I think I have the correct pictures for the CDF:\nFor $0<z<1$ :\n \nAnd for $1<z<\\sqrt 2$ :\n\nShaded portions are supposed to indicate the area of the region $$\\left\\{(x,y):0<x,y< 1\\,,\\,x^2+y^2\\le z^2\\right\\}$$\nThe picture immediately yields \n\\begin{align}\nF_Z(z)&=\\Pr\\left(-\\sqrt{z^2-X^2}\\le Y\\le\\sqrt{z^2-X^2}\\right)\n\\\\&=\\begin{cases}\\frac{\\pi z^2}{4} &,\\text{ if } 0<z<1\\\\\\\\ \\sqrt{z^2-1}+\\int_{\\sqrt{z^2-1}}^1 \\sqrt{z^2-x^2}\\,\\mathrm{d}x &,\\text{ if }1< z<\\sqrt 2 \\end{cases}\n\\end{align}\n, as I had previously found.\n", "A": "That the pdf is correct can be checked by a simple simulation\nsamps=sqrt(runif(1e5)^2+runif(1e5)^2)\nhist(samps,prob=TRUE,nclass=143,col=\"wheat\")\ndf=function(x){pi*x/2-2*x*(x>1)*acos(1/(x+(1-x)*(x<1)))}\ncurve(df,add=TRUE,col=\"sienna\",lwd=3)\n\n\nFinding the cdf without the polar change of variables goes through\n\\begin{align*}\n\\mathrm{Pr}(\\sqrt{X^2+Y^2}\\le z) &= \\mathrm{Pr}(X^2+Y^2\\le z^2)\\\\\n&= \\mathrm{Pr}(Y^2\\le z^2-X^2)\\\\\n&=\\mathrm{Pr}(Y\\le \\sqrt{z^2-X^2}\\,,X\\le z)\\\\\n&=\\mathbb{E}^X[\\sqrt{z^2-X^2}\\mathbb{I}_{[0,\\min(1,z)]}(X)]\\\\\n&=\\int_0^{\\min(1,z)} \\sqrt{z^2-x^2}\\,\\text{d}x\\\\\n&=z^2\\int_0^{\\min(1,z^{-1})} \\sqrt{1-y^2}\\,\\text{d}y\\qquad [x=yz\\,,\\ \\text{d}x=z\\text{d}y]\\\\\n&=z^2\\int_0^{\\min(\\pi/2,\\cos^{-1} z^{-1})} \\sin^2{\\theta} \\,\\text{d}\\theta\\qquad [y=\\cos(\\theta)\\,,\\ \\text{d}y=\\sin(\\theta)\\text{d}\\theta]\\\\\n&=\\frac{z^2}{2}\\left[ \\min(\\pi/2,\\cos^{-1} z^{-1}) - \\sin\\{\\min(\\pi/2,\\cos^{-1} z^{-1})\\}\\cos\\{\\min(\\pi/2,\\cos^{-1} z^{-1}\\}\\right]\\\\\n&=\\frac{z^2}{2}\\begin{cases}\n\\pi/2 &\\text{ if }z<1\\\\\n\\cos^{-1} z^{-1}-\\sin\\{\\cos^{-1} z^{-1})\\}z^{-1}&\\text{ if }z\\ge 1\\\\\n\\end{cases}\\\\\n&=\\frac{z^2}{2}\\begin{cases}\n\\pi/2 &\\text{ if }z<1\\\\\n\\cos^{-1} z^{-1}-\\sqrt{1-z^{-2}}z^{-1}&\\text{ if }z\\ge 1\\\\\n\\end{cases}\n\\end{align*}\nwhich ends up with the same complexity! (Plus potential mistakes of mine along the way!)\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/323617", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "11", "answer_count": 3, "answer_id": 1}}
{"Q": "Probability that the absolute value of a normal distribution is greater than another Greatly appreciate anyone that is willing to Help. I am thinking about the question of comparing the absolute value of normal distributions. Given $a > b$, $X$ ~ $N(0,a)$ and $Y$ ~ $N(0,b)$, what is the distribution of $|X| - |Y|$?\n", "A": "It may be shown that $|X| \\sim HN(a)$ and $|Y| \\sim HN(b)$, where $HN(\\cdot)$ represents a half-normal distribution.  For completeness, the probability density functions of $|X|$ and $|Y|$ are\n\\begin{eqnarray*}\nf_{|X|} (x) &=& \\frac{\\sqrt{2}}{a\\sqrt{\\pi}} \\exp \\left(-\\frac{x^2}{2a^2}\\right), \\quad x>0 \\\\\nf_{|Y|} (y) &=& \\frac{\\sqrt{2}}{b\\sqrt{\\pi}} \\exp \\left(-\\frac{y^2}{2b^2}\\right), \\quad y>0.\n\\end{eqnarray*}\nIt is also useful to note that if $W \\sim HN(\\sigma)$, then the moment generating function of $W$ is\n\\begin{eqnarray*}\n\\mbox{E} \\left[\\exp \\left(tW\\right) \\right] = 2 \\exp \\left(\\frac{\\sigma^2 t^2}{2}\\right) \\Phi \\left(\\sigma t\\right),\n\\end{eqnarray*}\nwhere $\\Phi (\\cdot)$ denotes the CDF of the standard normal distribution.\nWe wish to find the distribution of $Z = |X| - |Y|$.  By definition, the CDF of $Z$ of is defined as\n\\begin{eqnarray*}\nF_{Z} (z) = \\int_{0}^{\\infty}\\int_{0}^{\\infty} f_{|X|} (x) f_{|Y|} (y) \\mbox{I} \\left[x-y \\le z\\right] \\mbox{d}x \\mbox{d}y,\n\\end{eqnarray*}\nwhere $\\mbox{I} \\left[\\cdot\\right]$ denotes the indicator function.  Note that the double integral takes place in the first quadrant and the indicator function specifies all points above the line $y = x-z$. Now if $z \\ge 0$, this line will intersect the $x$-axis, otherwise it will intersect the $y$-axis.  Now ordering the integration appropriately will greatly simplify the double integral.  See the following plots where the dark black line denotes $y=x-z$ and the red lines denote the direction and bounds of integration (albeit not extended indefinitely).\n\nTherefore, we can write the CDF of $Z$ as\n\\begin{eqnarray*}\nF_{Z} (z) = \\mbox{I} \\left[z \\ge 0 \\right]\\int_{0}^{\\infty}\\int_{0}^{y+z} f_{|X|} (x) f_{|Y|} (y) \\mbox{d}x \\mbox{d}y + \\mbox{I} \\left[z \\lt 0 \\right]\\int_{0}^{\\infty}\\int_{x-z}^{\\infty} f_{|X|} (x) f_{|Y|} (y) \\mbox{d}y \\mbox{d}x.\n\\end{eqnarray*}\nIn the first double integral, consider the change of variables from $(x,y)$ to $(v,y)$, where $v=x-y$.  The Jacobian of this transformation is $1$.  The transformation is useful since it removes the occurrence of $y$ in the bound of integration when differentiating with respect to $v$.  A similar transformation was defined for the second double integral.  Plugging these values in, we obtain\n\\begin{eqnarray*}\nF_{Z} (z) = \\mbox{I} \\left[z \\ge 0 \\right]\\int_{0}^{\\infty} f_{|Y|} (y)  \\int_{-\\infty}^{z} f_{|X|} (v+y) \\mbox{d}v \\mbox{d}y + \\mbox{I} \\left[z \\lt 0 \\right]\\int_{0}^{\\infty} f_{|X|} (x) \\int_{-\\infty}^{z}  f_{|Y|} (x-v) \\mbox{d}v \\mbox{d}x.\n\\end{eqnarray*}\nBy Leibniz's integral rule (or the fundamental theorem of calculus), the PDF of $Z$ is\n\\begin{eqnarray*}\nf_{Z} (z) &=& \\mbox{I} \\left[z \\ge 0 \\right]\\int_{0}^{\\infty} f_{|Y|} (y)  f_{|X|} (z+y) \\mbox{d}y + \\mbox{I} \\left[z \\lt 0 \\right]\\int_{0}^{\\infty} f_{|X|} (x)   f_{|Y|} (x-z)  \\mbox{d}x \\\\ \n&=& \\mbox{I} \\left[z \\ge 0 \\right]\\int_{0}^{\\infty} f_{|Y|} (y)  f_{|X|} (y+|z|) \\mbox{d}y + \\mbox{I} \\left[z \\lt 0 \\right]\\int_{0}^{\\infty} f_{|X|} (x)   f_{|Y|} (x+|z|)  \\mbox{d}x.\n\\end{eqnarray*}\nThese integrals may be solved quite simply by making use of the moment generating function result.  I shall only solve the first one.\n\\begin{eqnarray*}\n\\int_{0}^{\\infty} f_{|Y|} (y)  f_{|X|} (y+|z|) \\mbox{d}y &=& \\frac{2}{ab \\pi}\\exp \\left(-\\frac{z^2}{2a^2} \\right) \\int_{0}^{\\infty} \\exp \\left(-\\frac{|z|}{a^2} y \\right) \\exp \\left(-\\frac{y^2}{2} \\left[\\frac{1}{a^2}+\\frac{1}{b^2}\\right] \\right) \\mbox{d}y.\n\\end{eqnarray*}\nNow the second term within the integral is proportional to a $HN(\\sigma)$ PDF with $\\sigma^2 = \\frac{a^2b^2}{a^2+b^2}$ and the first term is of the form of the MGF with $t = - \\frac{|z|}{a^2}$.  Hence, multiplying and dividing by the proportionality constant, $\\frac{\\sqrt{2}\\sqrt{a^2+b^2}}{ab\\sqrt{\\pi}}$, it may be shown that the above reduces to\n\\begin{eqnarray*}\n2 \\sqrt{\\frac{2}{\\pi}} (a^2+b^2)^{(-.5)} \\exp \\left(-\\frac{z^2}{2(a^2+b^2)} \\right) \\Phi \\left(-\\frac{b}{a} \\frac{|z|}{\\sqrt{a^2+b^2}}\\right).\n\\end{eqnarray*}\nMaking use of the standard normal PDF $ \\phi(\\cdot)$, the above can be written as\n\\begin{eqnarray*}\n\\frac{4}{\\sqrt{a^2+b^2}} \\phi\\left(\\frac{z}{\\sqrt{a^2+b^2}}\\right)\\Phi \\left(-\\frac{b}{a} \\frac{|z|}{\\sqrt{a^2+b^2}}\\right).\n\\end{eqnarray*}\nSolving for the other portion of the PDF of $Z$, one will result in the equation\n\\begin{eqnarray*}\nf_Z (z) = \\begin{cases}\n\\frac{4}{\\sqrt{a^2+b^2}} \\phi\\left(\\frac{z}{\\sqrt{a^2+b^2}}\\right)\\Phi \\left(-\\frac{b}{a} \\frac{|z|}{\\sqrt{a^2+b^2}}\\right), & \\mbox{for } z \\ge 0 \\\\\n\\frac{4}{\\sqrt{a^2+b^2}} \\phi\\left(\\frac{z}{\\sqrt{a^2+b^2}}\\right)\\Phi \\left(-\\frac{a}{b} \\frac{|z|}{\\sqrt{a^2+b^2}}\\right), & \\mbox{for } z \\lt 0\n\\end{cases}.\n\\end{eqnarray*}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/560633", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "How to derive the Jensen-Shannon divergence from the f-divergence? The Jensen-Shannon divergence is defined as $$JS(p, q) = \\frac{1}{2}\\left(KL\\left(p||\\frac{p+q}{2}\\right) + KL\\left(q||\\frac{p+q}{2}\\right) \\right).$$ In Wikipedia it says that it can be derived from the f-divergence $$D_f(p||q) = \\int_{-\\infty}^{\\infty} q(x) \\cdot f\\left(\\frac{p(x)}{q(x)}\\right) dx$$ with $f(x) = -(1+x)\\cdot \\text{log}(\\frac{1+x}{2}) + x \\cdot \\text{log} (x)$. However, when I try it I end up with $JS(p, q) = 2 \\cdot D_f(p||q)$.\n\\begin{align*}\nJS(p, q) &= \\frac{1}{2}\\left( KL \\left( p|| \\frac{p+q}{2} \\right) + KL \\left( q|| \\frac{p+q}{2} \\right) \\right)\\\\\n&= \\frac{1}{2} \\int_{-\\infty}^{\\infty} p(x) \\cdot \\text{log} \\left( \\frac{2 \\cdot p(x)}{p(x)+q(x)} \\right) + q(x) \\cdot \\text{log} \\left( \\frac{2 \\cdot q(x)}{p(x)+q(x)} \\right) dx\\\\ \n&= \\frac{1}{2} \\int_{-\\infty}^{\\infty} p(x) \\cdot \\left( \\text{log} \\left( 2 \\right) + \\text{log} \\left( p(x) \\right)  - \\text{log} \\left( p(x)+q(x) \\right) \\right) + q(x) \\cdot \\left( \\text{log} \\left( 2 \\right) + \\text{log} \\left( q(x) \\right)   \\right) - \\text{log} \\left( p(x)+q(x) \\right)  dx\\\\ \n&= \\frac{1}{2} \\int_{-\\infty}^{\\infty} (p(x)+q(x)) \\cdot \\text{log}\\left(2 \\right) - (p(x)+q(x)) \\cdot \\text{log}\\left(p(x)+ q(x) \\right)  + p(x) \\cdot \\text{log} \\left(p(x)\\right) + q(x) \\cdot  \\text{log}\\left(q(x) \\right)  dx \\\\\n&= \\frac{1}{2} \\int_{-\\infty}^{\\infty} p(x) \\cdot \\text{log} \\left(p(x)\\right) + q(x) \\cdot  \\text{log}\\left(q(x) \\right) - (p(x)+q(x)) \\cdot \\text{log}\\left(\\frac{p(x)+ q(x)}{2} \\right) dx \n\\end{align*}\nand\n\\begin{align*}\nD_f(p||q) &= \\int_{-\\infty}^{\\infty} q(x) \\cdot f\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty} q(x) \\cdot \\left(-(1+\\frac{p(x)}{q(x)}) \\cdot \\text{log}\\left(\\frac{1+\\frac{p(x)}{q(x)}}{2} \\right) + \\frac{p(x)}{q(x)} \\cdot \\text{log}\\left(\\frac{p(x)}{q(x)}\\right) \\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty}  -(q(x)+p(x)) \\cdot \\text{log}\\left(\\frac{1+\\frac{p(x)}{q(x)}}{2} \\right) + p(x) \\cdot \\text{log}\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty}  p(x) \\cdot \\text{log}\\left(p(x) \\right) - p(x) \\cdot \\text{log}\\left(q(x)\\right) - p(x) \\cdot \\text{log}\\left(1+\\frac{p(x)}{q(x)} \\right) +  p(x) \\cdot \\text{log}\\left(2 \\right)  - q(x) \\cdot \\text{log}\\left(1+\\frac{p(x)}{q(x)} \\right) +  q(x) \\cdot \\text{log}\\left(2 \\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty}  p(x) \\cdot \\text{log}\\left(p(x) \\right) - p(x) \\cdot \\text{log}\\left(q(x)\\right) - p(x) \\cdot \\text{log}\\left(\\frac{p(x)+q(x)}{q(x)} \\right) +  p(x) \\cdot \\text{log}\\left(2 \\right)  - q(x) \\cdot \\text{log}\\left(\\frac{p(x)+q(x)}{q(x)} \\right) +  q(x) \\cdot \\text{log}\\left(2 \\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty}  p(x) \\cdot \\text{log}\\left(p(x) \\right) - p(x) \\cdot \\text{log}\\left(q(x)\\right) - p(x) \\cdot \\text{log}\\left(p(x)+q(x) \\right) + p(x) \\cdot \\text{log}\\left(q(x) \\right)  +  p(x) \\cdot \\text{log}\\left(2 \\right)  - q(x) \\cdot \\text{log}\\left(p(x)+q(x) \\right) + q(x)\\cdot \\text{log}\\left(q(x) \\right) +  q(x) \\cdot \\text{log}\\left(2 \\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty}  p(x) \\cdot \\text{log}\\left(p(x) \\right) - p(x) \\cdot \\text{log}\\left(p(x)+q(x) \\right) +  p(x) \\cdot \\text{log}\\left(2 \\right)  - q(x) \\cdot \\text{log}\\left(p(x)+q(x) \\right) + q(x)\\cdot \\text{log}\\left(q(x) \\right) +  q(x) \\cdot \\text{log}\\left(2 \\right) dx \\\\\n&= \\int_{-\\infty}^{\\infty} p(x) \\cdot \\text{log} \\left(p(x)\\right) + q(x) \\cdot  \\text{log}\\left(q(x) \\right)  - (p(x)+q(x)) \\cdot \\text{log}\\left(p(x)+ q(x)\\right) + (p(x)+q(x)) \\cdot \\text{log}\\left(2\\right) dx \\\\\n&= \\frac{2}{2} \\int_{-\\infty}^{\\infty} p(x) \\cdot \\text{log} \\left(p(x)\\right) + q(x) \\cdot  \\text{log}\\left(q(x) \\right) - (p(x)+q(x)) \\cdot \\text{log}\\left(\\frac{p(x)+ q(x)}{2} \\right) dx \\\\\n&= 2 \\cdot JS(p, q)\n\\end{align*}\nWhere is my mistake?\n", "A": "Few observations:\n$\\rm [I] ~(p. 90)$ defines Jensen-Shannon divergence for $P, Q, ~P\\ll Q$ as\n$$\\mathrm{JS}(P,~Q) := D\\left(P\\bigg \\Vert \\frac{P+Q}{2}\\right)+D\\left(Q\\bigg \\Vert \\frac{P+Q}{2}\\right)\\tag 1\\label 1$$\nand the associated function to generate $\\rm JS(\\cdot,\\cdot) $ from $D_f(\\cdot\\Vert\\cdot) $ is\n$$f(x) :=x\\log\\frac{2x}{x+1}+\\log\\frac{2}{x+1}. \\tag 2$$\nThe definition of Jensen-Shannon in $\\eqref{1}$ lacks the constant $1/2,$ however in the original paper $\\rm [II]~ (sec. IV, ~ p. 147)$ it wasn't defined so.\nIn $\\rm [III], $ the authors noted the corresponding function $g(t), ~t:=p_i(x) /q_j(x) $ as\n$$ g(t) := \\frac12\\left(t\\log\\frac{2t}{t+1}+\\log\\frac{2}{t+1}\\right).\\tag 3\\label 3$$\nAlso in $\\rm [IV] ~(p. 4)$ the author mentioned the required function to be $\\frac12\\left(u\\log u -(u+1)\\log\\left(\\frac{u+1}{2}\\right)\\right)  $ which is equivalent to $\\eqref 3.$\n\nIn $\\rm [II] ~(p. 147),$ the author noted that the $K$ divergence, defined in terms of the Kullback $I$ (in author's terminology)\n$$K(p_1, p_2) := I\\left(p_1,\\frac12(p_1+p_2)\\right),\\tag 4$$\ncoincides with the $f$ divergence for $$x\\mapsto x\\log\\frac{2x}{1+x}.\\tag a\\label a$$\nThe symmetrised version of $K$ is\n$$L(p_1, p_2) := K(p_1, p_2)+K( p_2, p_1). \\tag 5$$\nAs the author subsequently defined $\\rm JS_{\\pi}(\\cdot,\\cdot);$ for $\\pi=\\frac12, $\n$$\\mathrm{JS}_\\frac{1}{2}(p_1,p_2)=\\frac12 L(p_1,p_2).\\tag 6\\label 6$$\nNow, using $\\eqref{a}, $\n\\begin{align}\\frac12 L(p, q) &=\\frac12[K(p,q)+K(q,p)]\\\\ &=\\frac12\\left[\\int q~\\frac{p}{q}\\log\\frac{\\frac{2p}{q}}{1+\\frac{p}{q}}~\\mathrm d\\mu+ \\int p~\\frac{q}{p}\\log\\frac{\\frac{2q}{p}}{1+\\frac{q}{p}}~\\mathrm d\\mu\\right]\\\\ &= \\frac12\\left[\\int p\\log\\frac{2p}{q+p}~\\mathrm d\\mu+ \\int q\\log\\frac{2q}{p+q}~\\mathrm d\\mu\\right]\\\\&= \\frac{1}{2}\\left[\\int q~\\frac{p}{q}\\log\\frac{2\\frac{p}{q}}{\\frac{p}{q}+1}~\\mathrm d\\mu + \\int q \\log\\frac{2}{1+\\frac{p}{q}}~\\mathrm d\\mu\\right]\\tag 7\\label 7\\\\&=\\int q~f_{\\rm{JS}}\\left(\\frac{p}{q}\\right)~\\mathrm d\\mu,\\end{align}\nwhere, from $\\eqref{7}, $\n$$f_{\\rm{JS}} (x) :=\\frac12\\left[x\\log\\frac{2x}{1+x}+ \\log\\frac{2}{1+x}\\right] .\\tag 8 $$\n\nReferences:\n$\\rm [I]$ Information Theory:\nFrom Coding to Learning, Yury Polyanskiy, Yihong Wu, Cambridge University Press.\n$\\rm [II]$ Divergence Measures Based on the Shannon Entropy, Jianhua Lin, IEEE, Vol. $37$, No. $\\rm I,$ January $1991.$\n$\\rm [III]$ $f$-Divergence is a Generalized Invariant Measure Between Distributions, Yu Qiao, Nobuaki Minematsu, DOI:$10.21437/\\rm Interspeech.2008-393.$\n$\\rm [IV]$ On a generalization of the Jensen-Shannon divergence and the\nJS-symmetrization of distances relying on abstract means, Frank Nielsen, May $2019, $ DOI:$10.3390/\\rm e21050485.$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/593928", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Find mean and variance using characteristic function Consider a random variable with characteristic function\n$$\n\\phi(t)=\\frac{3\\sin(t)}{t^3}-\\frac{3\\cos(t)}{t^2}, \\ \\text{when} \\ t \\neq0\n$$\nHow can I compute the $E(X)$ and $Var(X)$ by using this characteristic function? I'm stuck because if I differentiate I got $\\phi'(t)=\\frac{3t^2\\sin(t)+9t\\cos(t)-9\\sin(t)}{t^4}$ which is undefined at $t=0$.\nDo I need to use Taylor expansion to approximate sin and cos ?\n", "A": "In general, if the power-series expansion holds for a characteristic function of random variable $X$, which is the case of this $\\varphi(t)$ (because power-series expansions for $\\sin t$ and $\\cos t$ hold and the negative exponent terms cancelled each other, as will be shown in the derivation below), the moments of $X$ can be read off from it (for a rigorous proof, see Probability and Measure by Patrick Billingsley, pp. 342 -- 345):\n\\begin{align}\n\\varphi^{(k)}(0) = i^kE[X^k]. \\tag{1}\n\\end{align}\n$(1)$ is analogous to the relationship $E[X^k] = m^{(k)}(0)$ between moments and the moment generating function, which is perhaps more familiar to statisticians.\nTherefore, to determine $E[X]$ and $\\operatorname{Var}(X)$, it is sufficient to evaluate $\\varphi'(0)$ and $\\varphi''(0)$, to which you can apply L'Hopital's rule for multiple times (which is more verbose). First, because $\\sin t - t\\cos t \\to 0$ as $t \\to 0$,\n\\begin{align}\n\\lim_{t \\to 0}\\varphi(t) = 3\\lim_{t \\to 0}\\frac{\\sin t - t\\cos t}{t^3} \n= 3\\lim_{t \\to 0}\\frac{\\cos t - (\\cos t - t\\sin t)}{3t^2} \n= \\lim_{t \\to 0} \\frac{\\sin t}{t} = 1,\n\\end{align}\nwhich lends us the legitimacy of using L'Hopital rule for evaluating\n\\begin{align}\n\\varphi'(0) &= \\lim_{t \\to 0}\\frac{\\varphi(t) - \\varphi(0)}{t} \\\\\n&= \\lim_{t \\to 0} \\varphi'(t) \\\\\n&= 3\\lim_{t \\to 0}\\frac{t^4\\sin t - 3t^2(\\sin t - t\\cos t)}{t^6} \\\\\n&= 3\\lim_{t \\to 0}\\frac{t^2\\sin t - 3\\sin t + 3t\\cos t}{t^4} \\\\\n&= 3\\lim_{t \\to 0}\\frac{2t\\sin t + t^2\\cos t - 3\\cos t + 3\\cos t - 3t\\sin t}{4t^3} \\\\\n&= \\frac{3}{4}\\lim_{t \\to 0}\\frac{t\\cos t - \\sin t}{t^2} \\\\\n&= \\frac{3}{4}\\lim_{t \\to 0}\\frac{\\cos t - t\\sin t - \\cos t}{2t} = 0. \n\\end{align}\nI will leave the task of getting $\\varphi''(0)$ in this way back to you.\nAlternatively, a direct power-series expansion method (which I highly recommend as the first option for general limit evaluation tasks) has been mentioned in whuber's answer and my previous comments.  In detail, it follows by (see, for example, Eq. (23) and Eq. (6) in this link)\n\\begin{align}\n& \\sin t = t - \\frac{1}{3!}t^3 + \\frac{1}{5!}t^5 + O(t^7), \\\\\n& \\cos t = 1 - \\frac{1}{2}t^2 + \\frac{1}{4!}t^4 + O(t^6)\n\\end{align}\nthat\n\\begin{align}\n & \\varphi(t) = 3t^{-3}\\sin t - 3t^{-2}\\cos t \\\\\n=& 3t^{-3}\\left(t - \\frac{1}{3!}t^3 + \\frac{1}{5!}t^5 + O(t^7)\\right) \n- 3t^{-2}\\left(1 - \\frac{1}{2}t^2 + \\frac{1}{4!}t^4 + O(t^6)\\right) \\\\\n=& 1 + \\frac{1}{40}t^2 - \\frac{1}{8}t^2 + O(t^4) \\\\\n=& 1 - \\frac{1}{10}t^2 + O(t^4). \n\\end{align}\nFrom which it is immediate to conclude\n\\begin{align}\n\\varphi(0) = 1, \\; \\varphi'(0) = 0, \\; \\varphi''(0) = -\\frac{1}{5}. \n\\end{align}\nIt then follows by $(1)$ that\n\\begin{align}\nE[X] = -i\\varphi'(0) = 0, \\; E[X^2] = -\\varphi''(0) = \\frac{1}{5}.\n\\end{align}\nTherefore, $E[X] = 0, \\operatorname{Var}(X) = \\frac{1}{5}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/605455", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 2, "answer_id": 1}}
