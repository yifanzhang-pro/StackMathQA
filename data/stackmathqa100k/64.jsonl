{"Q": "What is the probability that the matrix is orthogonal? Let $X$ and $Y$ be independent $Bin(3,\\frac{1}{3})$ random variables. Then what is  the probability that the matrix, $$\nP=\\begin{bmatrix}\n\\frac{X}{\\sqrt2}&\\frac{Y}{\\sqrt2}\\\\\\frac{-1}{\\sqrt2}&\\frac{1}{\\sqrt2}\\end{bmatrix}$$is orthogonal?\nMy approach:$$PP^T=\\begin{bmatrix}\n\\frac{X}{\\sqrt2}&\\frac{Y}{\\sqrt2}\\\\\\frac{-1}{\\sqrt2}&\\frac{1}{\\sqrt2}\\end{bmatrix}\\begin{bmatrix}\n\\frac{X}{\\sqrt2}&\\frac{-1}{\\sqrt2}\\\\\\frac{Y}{\\sqrt2}&\\frac{1}{\\sqrt2}\\end{bmatrix}=\\begin{bmatrix}\n\\frac{X^2+Y^2}{2}&\\frac{Y-X}{2}\\\\\\frac{Y-X}{2}&{1}\\end{bmatrix}=\\begin{bmatrix}\n1&0\\\\0&1\\end{bmatrix}$$Now,to find the required probability,it is enough to find,$P[Y-X=0].$Is this right?if it is,How can we find that?\n", "A": "$X=Y$ is not enough. You also need $X^2+Y^2=2$. These two conditions yield $X=Y=1$. Since $\\mathbb{P}(X=1)= {{3}\\choose{1}}(\\frac{1}{3})(1-\\frac{1}{3})^2= \\frac{4}{9}$ and $X$ and $Y$ are independent, the required probability is $\\frac{16}{81}$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/219527", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
{"Q": "Why is $n < p$ a problem for OLS regression? I realize I can't invert the $X'X$ matrix but I can use gradient descent on the quadratic loss function and get a solution. I can then use those estimates to calculate standard errors and residuals. Am I going to encounter any problems doing this?\n", "A": "Here is a little specific example to illustrate the issue:\nSuppose you want to fit a regression of $y_i$ on $x_i$, $x_i^2$ and a constant, i.e. \n$$ y_i = a x_i + b x_i^2 + c + u_i $$\nor, in matrix notation,\n\\begin{align*}\n \\mathbf{y} =\n \\begin{pmatrix}\n  y_1 \\\\\n  \\vdots \\\\\n  y_n \n \\end{pmatrix}, \\quad\n \\mathbf{X} =\n \\begin{pmatrix}\n  1 & x_1 & x_1^2 \\\\\n  \\vdots & \\vdots & \\vdots \\\\\n  1 & x_n & x_n^2\n \\end{pmatrix}, \\quad\n \\boldsymbol{\\beta} =\n \\begin{pmatrix}\n  c \\\\ a \\\\ b\n \\end{pmatrix}, \\quad\n \\mathbf{u} =\n \\begin{pmatrix}\n  u_1 \\\\ \\vdots \\\\ u_n\n \\end{pmatrix}\n\\end{align*}\nSuppose you observe $\\mathbf{y}^T=(0,1)$ and $\\mathbf{x}^T=(0,1)$, i.e., $n=2<p=3$. \nThen, the OLS estimator is\n\\begin{align*}\n \\widehat{\\boldsymbol{\\beta}} =& \\, (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n =& \\, \\left[\n    \\begin{pmatrix}\n   1 & 0 & 0 \\\\\n   1 & 1 & 1  \n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n     1 & 0 & 0 \\\\\n   1 & 1 & 1\n    \\end{pmatrix}\n    \\right]^{-1}\n    \\begin{pmatrix}\n   1 & 0 & 0 \\\\\n   1 & 1 & 1  \n    \\end{pmatrix}^T\n    \\begin{pmatrix}\n     0 \\\\ 1\n    \\end{pmatrix} \\\\\n =& \\, \\left[\\underbrace{\\begin{pmatrix}\n   2 & 1 & 1 \\\\\n   1 & 1 & 1 \\\\\n   1 & 1 & 1\n    \\end{pmatrix}}_{\\text{not invertible, as $\\mathrm{rk}()=2\\neq 3$}}\n\\right]^{-1}  \n    \\begin{pmatrix}\n     1 \\\\ 1 \\\\ 1\n    \\end{pmatrix}\n\\end{align*}\nThere are infinitely many solutions to the problem: setting up a system of equations and inserting the observations gives\n\\begin{align*}\n 0 =& \\, a \\cdot 0 + b \\cdot 0^2 + c \\ \\ \\Rightarrow c=0 \\\\\n 1 =& \\, a \\cdot 1 + b \\cdot 1^2 + c \\ \\ \\Rightarrow 1 = a + b\n\\end{align*}\nHence, all $a=1-b$, $b \\in \\mathbb{R}$ satisfy the regression equation.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/282663", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 3, "answer_id": 0}}
{"Q": "Proof of Convergence in Distribution with unbounded moment I posted the question here, but no one has provided an answer, so I am hoping I could get an answer here. Thanks very much!\nProve that given $\\{X_n\\}$ being a sequence of iid r.v's with density $|x|^{-3}$ outside $(-1,1)$, the following is true:\n$$\n\\frac{X_1+X_2 + \\dots +X_n}{\\sqrt{n\\log n}} \\xrightarrow{\\mathcal{D}}N(0,1).\n$$\nThe original post has a 2 in the square root of the denominator. There should not be a 2. \n", "A": "This is a proof by c.f. approach:\nThe c.f. of $X_i$ is\n$$\n \\phi_i(t) = \\int_{R}e^{itx}|x|^{-3}\\boldsymbol{1}_{x \\notin (-1,1)}dx = 2\\int_{1}^{\\infty}\\frac{\\cos(tx)}{x^3}dx.\n$$\n        Hence, for $Y_n = (X_1+X_2+\\dots+X_n)(\\sqrt{n\\log n})^{-1}$, we have\n        \\begin{align*}\n  \\phi_{Y_n}(t) =& \\phi_i\\left(\\frac{t}{\\sqrt{n\\log n}}\\right)^n\\\\\n      =& \\left(2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}dx\\right)^n.\\\\\n  \\end{align*}\n        We first consider the integral:\n        \\begin{align*}\n   2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}dx =& 1 + 2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx\\\\\n                       =& 1 + 2\\int_{1}^{\\sqrt{n\\log\\log n}}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx \\\\\n                       +& 2\\int_{\\sqrt{n\\log\\log n}}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx,\n  \\end{align*}\n        since for $x \\in [1, \\sqrt{n\\log\\log n}]$, ${\\displaystyle \\frac{tx}{\\sqrt{n\\log n}}} \\to 0$ as $n \\to \\infty$. Hence, we can apply the Taylor expansion of the cosine term in the first integral around $0$. Then we have\n        \\begin{align*}\n   2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}dx =& 1 + 2\\int_{1}^{\\sqrt{n\\log\\log n}}-\\frac{t^2}{2n\\log nx} + \\left[\\frac{t^4x}{24(n\\log n)^2 }-\\dots\\right]dx \\\\\n                       +& 2\\int_{\\sqrt{n\\log\\log n}}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx\\\\\n                       =& 1 + 2\\int_{1}^{\\sqrt{n\\log\\log n}}-\\frac{t^2}{2n\\log nx}dx + o(1/n)\\\\\n                       +& 2\\int_{\\sqrt{n\\log\\log n}}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx\\\\\n                       =& 1 -\\frac{t^2\\log( n\\log\\log n)}{2n\\log n} + o(1/n)\\\\\n                       +& 2\\int_{\\sqrt{n\\log\\log n}}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}dx\\\\\n  \\end{align*}\n        Now \n        \\begin{align*}\n   \\int_{\\sqrt{n\\log\\log n}}^{\\infty}|\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}-\\frac{1}{x^3}|dx \\leq& \\int_{\\sqrt{n\\log\\log n}}^{\\infty}\\frac{2}{x^3}dx\\\\\n                               =& \\frac{1}{n\\log\\log n} \\in o(1/n).\n  \\end{align*}\n        Hence, \n        $$\n  2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}dx = 1 -\\frac{t^2\\log( n\\log\\log n)}{2n\\log n} + o(1/n).\n  $$\n        Let $n \\to \\infty$, we have\n        $$\n  \\lim_{n \\to \\infty}\\left(2\\int_{1}^{\\infty}\\cos\\left(\\frac{tx}{\\sqrt{n\\log n}}\\right)\\frac{1}{x^3}dx\\right)^n = \\lim_{n \\to \\infty}\\left(1 -\\frac{t^2\\log( n\\log\\log n)}{2n\\log n}\\right)^n = \\lim_{n \\to \\infty}\\left(1-\\frac{t^2}{2n}\\right)^n = e^{-t^2/2},\n  $$\nwhich completes the proof.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/376848", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "5", "answer_count": 4, "answer_id": 1}}
{"Q": "Prove that $\\frac{1}{n(n-1)}\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}$ is an unbiased estimate of $\\text{Var}(\\overline{X})$ If $X_{1},X_{2},\\ldots,X_{n}$ are independent random variables with common mean $\\mu$ and variances $\\sigma^{2}_{1},\\sigma^{2}_{2},\\ldots,\\sigma^{2}_{n}$. Prove that\n\\begin{align*}\n\\frac{1}{n(n-1)}\\sum_{i=1}^{n}(X_{i} - \\overline{X})^{2}\n\\end{align*}\nis an unbiased estimate of $\\text{Var}[\\overline{X}]$.\nMY ATTEMPT\nTo begin with, I tried to approach the problem as it follows\n\\begin{align*}\n\\text{Var}(\\overline{X}) = \\text{Var}\\left(\\frac{X_{1} + X_{2} + \\ldots + X_{n}}{n}\\right) = \\frac{\\sigma^{2}_{1} + \\sigma^{2}_{2} + \\ldots + \\sigma^{2}_{n}}{n^{2}}\n\\end{align*}\nBut I do not know how to proceed from here. Any help is appreciated.\n", "A": "Let's minimize the algebra.  We can do this by focusing on the coefficient of $\\sigma_1^2.$\nFirst, because all the variables have the same mean,\n$$E[X_i - \\bar X]=\\mu - \\mu = 0$$\nfor all $i,$ implying\n$$\\operatorname{Var}(X_i - \\bar X) = E[(X_i - \\bar X)^2] - E[X_i - \\bar X]^2 = E[(X_i - \\bar X)^2].$$\nSince the sum of the $(X_i-\\bar X)^2$ is invariant under any permutation of the indexes, let's study the case $i=1$ because it will show us what happens with all the $i.$\nWe can  easily split $X_1 - \\bar X$ into independent (and therefore uncorrelated) parts as\n$$X_1 - \\bar X = X_1 - \\frac{1}{n}\\left(X_1 + X_2 + \\cdots + X_n\\right) = \\frac{n-1}{n}\\left(X_1 - \\frac{1}{n-1}\\left(X_2 + \\cdots + X_n\\right)\\right).$$\nTaking variances immediately gives\n$$\\operatorname{Var}(X_1 - \\bar X) = \\left(\\frac{n-1}{n}\\right)^2 \\left(\\sigma_1^2 + \\frac{1}{(n-1)^2}\\left(\\sigma_2^2 + \\cdots + \\sigma_n^2\\right)\\right).$$\n(This is the payoff from observing that the expectation of the square of $X_i - \\bar X$ is its variance.)\nWhen summing over all $i$ and ignoring the common factor of $((n-1)/n)^2,$ $\\sigma_1^2$ will therefore appear once and it will appear $n-1$ more times with a factor of $1/(n-1)^2,$ for a total coefficient of\n$$1 + (n-1)\\left(\\frac{1}{(n-1)^2}\\right) = \\frac{n}{n-1}.$$\nConsequently every $\\sigma_i^2$ appears with this coefficient, whence\n$$\\eqalign{\nE \\Bigg[ \\frac{1}{n(n-1)}\\sum_{i=1}^{n}(X_i - \\bar{X})^{2} \\Bigg]\n&= \\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\operatorname{Var}(X_i - \\bar{X}) \\\\\n&= \\frac{1}{n(n-1)} \\left(\\frac{n-1}{n} \\right)^2 \\left[  \\frac{n}{n-1} \\sum_{i=1}^n \\sigma_i^2 \\right] \\\\\n&= \\frac{\\sigma_1^2 + \\sigma_2^2 + \\cdots + \\sigma_n^2}{n^2},\n}$$\nQED.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/404174", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 3, "answer_id": 2}}
{"Q": "Bishop derivation completing the square in variational inference I don't understand the derivation on page 467. Bishop says:\nGiven the optimal factor $q_1^*(z_1)$\n\\begin{equation}\nln~q_1(z_1) = -\\frac{1}{2} z_1^2 \\Lambda_{11} + z_1 \\mu_1 \\Lambda_{11} - z_1 \\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2) + cst\n\\end{equation}\nusing the technique completing the square, we can identify the mean and precision of this gaussian, giving:\n\\begin{equation}\n    q^*(z_1) = \\mathcal{N}(z_1 \\mid m_1, \\Lambda_{11}^{-1})\n\\end{equation}\nwhere\n\\begin{equation}\n    m_1 = \\mu_1 - \\Lambda_{11}^{-1} \\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2)\n\\end{equation}\nI don't really understand this method completing the square in this situation and how he got $m_1$\n", "A": "$$\n\\begin{align}\nln~q_1(z_1) &= -\\frac{1}{2} z_1^2 \\Lambda_{11} + z_1 \\mu_1 \\Lambda_{11} - z_1 \\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2) + const. \\\\\n    &= -\\frac{1}{2} \\Lambda_{11} \\left(z_1^2 - 2\\Lambda_{11}^{-1}z_1 \\mu_1\\Lambda_{11} + 2\\Lambda_{11}^{-1}z_1\\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2)\\right) + const. \\\\\n    &= -\\frac{1}{2} \\Lambda_{11} \\left(z_1^2 - 2z_1(\\mu_1 - \\Lambda_{11}^{-1}\\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2))\\right) + const. \\\\\n\\end{align}\n$$\nnow, let's say\n$$\nm_1 = \\mu_1 - \\Lambda_{11}^{-1}\\Lambda_{12}( \\mathbb{E}[z_2] - \\mu_2)\n$$\nthen we can rewrite previous equation in terms of $m_1$\n$$\n\\begin{align}\nln~q_1(z_1) &= -\\frac{1}{2} \\Lambda_{11} \\left(z_1^2 - 2z_1m_1\\right) + const. \\\\\n&= -\\frac{1}{2} \\Lambda_{11} \\left(z_1^2 - 2z_1m_1 + m_1^2 - m_1^2\\right) + const. \\\\\n&= -\\frac{1}{2} \\Lambda_{11} \\left(z_1^2 - 2z_1m_1 + m_1^2\\right) + \\frac{1}{2} \\Lambda_{11}m_1^2 + const. \\\\\n&= -\\frac{1}{2} \\Lambda_{11} (z_1 - m_1)^2 + \\frac{1}{2} \\Lambda_{11}m_1^2 + const. \\\\\n&= -\\frac{1}{2} \\Lambda_{11} (z_1 - m_1)^2 + const. \\\\\n\\end{align}\n$$\nNow you can see that $const.$ will be just a part of the normalisation constant of the gaussian distribution, $m_1$ is a mean of the gaussian and $\\Lambda_{11}$ is a precision\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/447062", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 1}}
{"Q": "trying to derive expression with polynomial arithmetic I'm trying to figure out how this is derived in \"Time Series Analysis - Forecasting and Control\" (Box, Jenkins): \n$$\na_t = \\frac{1}{1-\\theta B}z_t = (1 + \\theta B + \\theta^2 B^2+ ... +  + \\theta^k B^k)(1-\\theta^{k+1} B^{k+1})^{-1}z_t\n$$\nI can derive this : \n$$\\frac{1}{1-\\theta B} = (1 + \\theta B + \\theta^2 B^2+ ... ) = (1 + \\theta B + \\theta^2 B^2 + ... + \\theta^kB^k) + \\frac{\\theta^{k+1}B^{k+1}}{1-\\theta B} $$\nHow does one derive the top expression? \n", "A": "Here's the way I got through it:\n$\\frac{1}{1 - \\theta B} = (1 + \\theta B + \\theta^2 B^2 + ...) = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k) + (\\theta^{k + 1}B^{k + 1} + \\theta^{k + 2}B^{k + 2} + ...)$\n$ = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k) + \\theta^{k+1}b^{k+1}(1 + \\theta B + \\theta^2 B^2 + ...)$\nSo now we have \n$(1 + \\theta B + \\theta^2 B^2 + ...) = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k) + \\theta^{k+1}b^{k+1}(1 + \\theta B + \\theta^2 B^2 + ...)$.\nSubtracting gives\n$(1 + \\theta B + \\theta^2 B^2 + ...) - \\theta^{k+1}b^{k+1}(1 + \\theta B + \\theta^2 B^2 + ...) = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k)$\n$\\Rightarrow (1 - \\theta^{k+1}b^{k+1})(1 + \\theta B + \\theta^2 B^2 + ...) = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k)$\nAnd finally, dividing both sides by $(1 - \\theta^{k+1}b^{k+1})$, gives the desired result:\n$\\frac{1}{1 - \\theta B} = (1+ \\theta B + \\theta ^2B^2 + ... + \\theta^kB^k)(1 - \\theta^{k+1}b^{k+1})^{-1}$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/449672", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Jacobian of Inverse Gaussian Transformation in Schwarz & Samanta (1991) In the sample size $n=2$ case when transforming $\\{x_1, x_2\\}$ to $\\{\\bar{x}, s\\}$ (where $X_1, X_2 \\overset{iid}{\\sim} IG(\\mu, \\lambda)$, $\\bar{X}=\\frac{\\sum_i^2 X_i}{n}$, and $S=\\sum_i^2 (\\frac{1}{X_i}-\\frac{1}{\\bar{X}}$), Schwarz and Samanta (1991) write:\n\nI understand the motivation for restricting the transformation to this half-plane, but I'm at a loss for how to find this Jacobian myself. I understand how to find Jacobians for one-to-one transformations, but I'd appreciate a push in the right direction for cases like this.\n", "A": "For simplicity write $x=x_1$ and $y=x_2,$ so that\n$$2\\bar x = x+y,\\quad s = \\frac{1}{x} + \\frac{1}{y} - \\frac{4}{x+y}.\\tag{*}$$\nTheir Jacobian $J(\\bar x, s)$ can be computed by comparing the differential elements in the two coordinate systems\n$$\\mathrm{d} \\bar x \\wedge \\mathrm{d} s = \\frac{1}{2}(\\mathrm{d} x + \\mathrm{d} y) \\wedge \\left(-\\frac{\\mathrm{d} x}{x^2} - \\frac{\\mathrm{d} y}{y^2} + \\frac{4(\\mathrm{d} x + \\mathrm{d} y)}{(x+y)^2}\\right)=\\frac{1}{2} \\left(\\frac{1}{x^2}-\\frac{1}{y^2}\\right)\\mathrm{d} x\\wedge \\mathrm{d} y$$\n(These differential forms and the wedge product are described at Construction of Dirichlet distribution with Gamma distribution.)\nIt remains only to divide both sides by $(1/x^2 - 1/y^2)/2$ and then re-express the left hand side entirely in terms of $\\bar x$ and $s.$  Elementary algebra (see below for details) gives\n$$\\frac{1}{x^2}-\\frac{1}{y^2} = \\frac{s^{1/2}(2 + s \\bar{x})^{3/2}}{\\bar{x}^{3/2}},$$\nwhence\n$$\\mathrm{d} x\\wedge \\mathrm{d} y = \\frac{1}{\\frac{1}{2} \\left(\\frac{1}{x^2}-\\frac{1}{y^2}\\right)}\\mathrm{d} \\bar{x} \\wedge \\mathrm{d} s =\\frac{2\\bar{x}^{3/2}}{\\sqrt{s}(2 + s \\bar{x})^{3/2}}\\,\\mathrm{d} \\bar x \\wedge \\mathrm{d} s  = J(\\bar x, s) \\,\\mathrm{d} \\bar x \\wedge \\mathrm{d} s.$$\nThus, when converting an integral in terms of $(x,y)$ to one in terms of $(\\bar x, s)$, the differential element on the left side must be replaced by the differential element on the right; the absolute value of the factor $J(\\bar x, s)$ that is introduced is the Jacobian.\n\nHere is one way to do the algebra. Multiplying $s$ by $2\\bar x$ to clear the fraction in $(*)$ gives\n$$2\\bar{x} s = \\frac{(y-x)^2}{xy}.$$\nAdding $4$ to that produces\n$$2\\bar{x} s + 4 = \\frac{(x+y)^2}{xy} = \\frac{(2\\bar x)^2}{xy},$$\nwhich upon dividing both sides by $(2\\bar x)^2$ establishes\n$$\\frac{1}{xy} = \\frac{2\\bar{x} s + 4}{(2\\bar x)^2}.$$\nFor $y\\gt x \\gt 0$ we may combine the three preceding results to obtain \n$$\\left(\\frac{1}{x^2} - \\frac{1}{y^2}\\right)^2 = \\frac{(y-x)^2(x+y)^2}{(xy)^4} = (2\\bar{x}s)(2\\bar{x}s + 4)\\left(\\frac{2\\bar{x} s + 4}{(2\\bar x)^2}\\right)^2.$$\nCollecting identical terms in the fraction and taking square roots gives\n$$\\frac{1}{x^2} - \\frac{1}{y^2} = \\left(\\frac{s(2\\bar{x} s + 4)^3}{(2\\bar x)^3}\\right)^{1/2}$$\nwhich easily simplifies to the expression in the question.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/450621", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Is there any way of simplifying the covariance of a function of a random variable and a random variable? For example, if we had random variables $X$ and $Y$ and we know that $corr(X,Y)=\\rho$, how would you solve for $Cov(e^X,Y)$?\n", "A": "The following is given,\n$$\n\\int dxdy \\ p(x, y)(x - \\mu_x)(y - \\mu_y) = \\sigma_x\\sigma_y\\rho,\n$$\nwhere $\\mu_x(\\mu_y)$ and $\\sigma_x(\\mu_y)$ are the mean and standard deviation of $X(Y)$,\nthe requested covariance is given by\n$$\n\\begin{aligned}\n{\\rm Cov}(e^X, Y) &= \\int dxdy \\  p(x, y)(e^x - \\mu_{e^x})(y - \\mu_y)\\\\\n&=\\int dxdy \\  p(x, y)\\left(1 + x + \\frac{1}{2!}x^2 + \\frac{1}{3!}x^3 + \\cdots - \\mu_{e^x}\\right)(y - \\mu_y)\n\\end{aligned}\n$$\nThe mean $\\mu_{e^x}$\n$$\n\\begin{aligned}\n\\mu_{e^x} &= \\int dxdy \\  p(x, y) e^x\\\\\n&= M_X(t=1)\\\\\n&= 1 + \\mu_x + \\frac{1}{2!}m_{2,x} + \\frac{1}{3!}m_{3,x} + \\cdots\n\\end{aligned}\n$$\nwhere $M_X$(t) is the moment generating function (see the definition here), and $m_{n,x}$ is the $n$-th moment of $p_X(x) = \\int dy \\ p(x, y)$.\nWe can further simplify the expression as\n$$\n\\begin{aligned}\n{\\rm Cov}(e^X, Y) &= \\int dxdy \\  p(x, y)(y - \\mu_y)\\left(1 + x + \\frac{1}{2!}x^2 + \\cdots - 1 - \\mu_x - \\frac{1}{2!}m_{2,x} - \\cdots\\right)\\\\\n&= \\sigma_x\\sigma_y\\rho + \\sum_{n=2}^{\\infty}\\frac{1}{n!}\\int dxdy \\  p(x, y)(y - \\mu_y)(x^n - m_{n,x}).\n\\end{aligned}\n$$\nTherefore we would need a lot more information to obtain the desired correlation.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/570616", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "PDF of $Z=X^2 + Y^2$ where $X,Y\\sim N(0,\\sigma)$ Using normal distribution probablilty density function(pdf),\n\\begin{align}\nf_Y(x) = f_X(X) &= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{x^2}{2\\sigma^2}} \\\\\n\\end{align}\nTaking $Z' = X^2 = Y^2$, the corresponding pdf is,\n\\begin{align}\nf_{Z'}(z) &= f_x(x) \\left| \\frac{\\delta x}{\\delta z}\\right| = \\frac{1}{\\sigma\\sqrt{2 \\pi}} e^{- \\frac{z}{2 \\sigma^2}} \\left( \\frac{1}{2\\sqrt{z}}\\right) \\\\\n\\end{align}\nSince $X$ and $Y$ are independent, so $Z$ pdf can be calculated using the following\n\\begin{align}\nf_z(z) = \\int^{\\infty}_{-\\infty}f_{X^2}(x)f_{Y^2}(z-x)dx\n\\end{align}\nIs this approach correct?\n", "A": "Following the comment by whuber, for problems like this that involve convolutions of IID random variables, it is generally simpler to work with the characteristic function than with the density function.  Using the law of the unconscious statistician we can obtain the characteristic function for $X^2$ as follows:\n$$\\begin{align}\n\\phi_{X^2}(t) \n&\\equiv \\mathbb{E}(\\exp(itX^2)) \\\\[16pt]\n&= \\int \\limits_{-\\infty}^\\infty \\exp(it x^2) \\cdot \\text{N}(x|0, \\sigma^2) \\ dx \\\\[6pt]\n&= \\int \\limits_{-\\infty}^\\infty \\exp(it x^2) \\cdot \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot \\exp \\bigg( -\\frac{1}{2 \\sigma^2} \\cdot x^2 \\bigg) \\ dx \\\\[6pt]\n&= \\int \\limits_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\cdot \\exp \\bigg( -\\frac{1-2it \\sigma^2}{2 \\sigma^2} \\cdot x^2 \\bigg) \\ dx \\\\[6pt]\n&= \\frac{1}{\\sqrt{1-2it \\sigma^2}} \\int \\limits_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi}} \\sqrt{\\frac{1-2it \\sigma^2}{\\sigma^2}} \\cdot \\exp \\bigg( -\\frac{1-2it \\sigma^2}{2 \\sigma^2} \\cdot x^2 \\bigg) \\ dx \\\\[6pt]\n&= \\frac{1}{\\sqrt{1-2it \\sigma^2}} \\int \\limits_{-\\infty}^\\infty \\text{N}\\bigg( x \\bigg| 0, \\frac{\\sigma^2}{1-2it \\sigma^2} \\bigg) \\ dx \\\\[6pt]\n&= \\frac{1}{\\sqrt{1-2it \\sigma^2}}. \\\\[6pt]\n\\end{align}$$\n(And of course, we have $\\phi_{X^2}(t) = \\phi_{Y^2}(t)$ in this case so the latter characteristic function is the same.)  We then have:\n$$\\begin{align}\n\\phi_Z(t) \n&\\equiv \\mathbb{E}(\\exp(itZ)) \\\\[16pt]\n&= \\mathbb{E}(\\exp(itX^2 + itY^2)) \\\\[16pt]\n&= \\mathbb{E}(\\exp(itX^2)) \\cdot \\mathbb{E}(\\exp(itY^2)) \\\\[12pt]\n&= \\frac{1}{\\sqrt{1-2it \\sigma^2}} \\cdot \\frac{1}{\\sqrt{1-2it \\sigma^2}} \\\\[6pt]\n&= \\frac{1}{1-2it \\sigma^2}. \\\\[6pt]\n\\end{align}$$\nThis is the characteristic function for the scaled chi-squared distribution with two degrees-of-freedom.  Using the fact that the characteristic function is a unique representative of the distribution, you then have:\n$$Z \\sim \\sigma^2 \\cdot \\text{ChiSq}(\\text{df} = 2).$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/588820", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
