{"Q": "KL divergence between two univariate Gaussians I need to determine the KL-divergence between two Gaussians. I am comparing my results to these, but I can't reproduce their result. My result is obviously wrong, because the KL is not 0 for KL(p, p).\nI wonder where I am doing a mistake and ask if anyone can spot it.\nLet $p(x) = N(\\mu_1, \\sigma_1)$ and $q(x) = N(\\mu_2, \\sigma_2)$. From Bishop's\nPRML I know that\n$$KL(p, q) = - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx$$\nwhere integration is done over all real line, and that\n$$\\int p(x) \\log p(x) dx = -\\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2),$$\nso I restrict myself to $\\int p(x) \\log q(x) dx$, which I can write out as\n$$-\\int p(x) \\log \\frac{1}{(2 \\pi \\sigma_2^2)^{(1/2)}} e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}} dx,$$\nwhich can be separated into\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) - \\int p(x) \\log e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}} dx.$$\nTaking the log I get\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) - \\int p(x) \\bigg(-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2} \\bigg) dx,$$\nwhere I separate the sums and get $\\sigma_2^2$ out of the integral.\n$$\\frac{1}{2} \\log (2 \\pi \\sigma^2_2) + \\frac{\\int p(x) x^2 dx - \\int p(x) 2x\\mu_2 dx + \\int p(x) \\mu_2^2 dx}{2 \\sigma_2^2}$$\nLetting $\\langle \\rangle$ denote the expectation operator under $p$, I can rewrite this as\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\langle x^2 \\rangle - 2 \\langle x \\rangle \\mu_2 + \\mu_2^2}{2 \\sigma_2^2}.$$\nWe know that $var(x) = \\langle x^2 \\rangle - \\langle x \\rangle ^2$. Thus\n$$\\langle x^2 \\rangle = \\sigma_1^2 + \\mu_1^2$$\nand therefore \n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + \\mu_1^2 - 2 \\mu_1 \\mu_2 + \\mu_2^2}{2 \\sigma_2^2},$$\nwhich I can put as\n$$\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2}.$$\nPutting everything together, I get to\n\\begin{align*}\nKL(p, q) &= - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\\\\\\\\\n&= \\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2)\\\\\\\\\n&= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2}.\n\\end{align*}\nWhich is wrong since it equals $1$ for two identical Gaussians.\nCan anyone spot my error?\nUpdate\nThanks to mpiktas for clearing things up. The correct answer is:\n$KL(p, q) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}$\n", "A": "OK, my bad. The error is in the last equation:\n\\begin{align}\nKL(p, q) &= - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\\\\\\\\\n&=\\frac{1}{2} \\log (2 \\pi \\sigma_2^2) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} (1 + \\log 2 \\pi \\sigma_1^2)\\\\\\\\\n&= \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}\n\\end{align}\nNote the missing $-\\frac{1}{2}$. The last line becomes zero when $\\mu_1=\\mu_2$ and $\\sigma_1=\\sigma_2$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/7440", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "138", "answer_count": 2, "answer_id": 1}}
{"Q": "How to compute the standard error of the mean of an AR(1) process? I try to compute the standard error of the mean for a demeaned AR(1) process $x_{t+1} = \\rho x_t + \\varepsilon_{t+1} =\\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t+1-i}$\nHere is what I did:\n$$\n\\begin{align*}\nVar(\\overline{x}) &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} x_t\\right) \\\\\n                  &= Var\\left(\\frac{1}{N} \\sum\\limits_{t=0}^{N-1} \\sum\\limits_{i=0}^{\\infty} \\rho^i \\varepsilon_{t-i}\\right) \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_0 +  & \\rho^1 \\varepsilon_{-1} +  & \\rho^2 \\varepsilon_{-2} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{-\\infty} + \\\\ \n                                                      \\rho^0  \\varepsilon_1 +  & \\rho^1 \\varepsilon_{0}  +   & \\rho^2 \\varepsilon_{-1} +  & \\cdots  & \\rho^{\\infty} \\varepsilon_{1-\\infty} + \\\\\n                                                      \\vdots               & \\vdots                 & \\vdots                & \\ddots  & \\vdots \\\\\n                                                      \\rho^0\\varepsilon_{N-1} + & \\rho^1 \\varepsilon_{N-2} +  & \\rho^2 \\varepsilon_{N-3} + & \\cdots  & \\rho^{\\infty} \\varepsilon_{N-1-\\infty} + \\\\\n                                      \\end{pmatrix} \\\\\n                  &= \\frac{1}{N^2} Var\\begin{pmatrix} \\rho^0  \\varepsilon_{N-1} + \\\\ \n                                                      (\\rho^0 + \\rho^1)  \\varepsilon_{N-2} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  \\varepsilon_{N-3} + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  \\varepsilon_{1} + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  \\varepsilon_{0} + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})  \\varepsilon_{-1} + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  \\varepsilon_{-2} + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                 &= \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^0   + \\\\ \n                                                      (\\rho^0 + \\rho^1)  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} \\\\\n                &= \\frac{N \\sigma_{\\varepsilon}^2}{N^2} (\\rho^0 + \\rho^1 + \\dots + \\rho^{\\infty}) \\\\\n                &= \\frac{\\sigma_{\\varepsilon}^2}{N} \\frac{1}{1 - \\rho} \\\\\n\\end{align*}\n$$\nProbably, not every step is done in the most obvious way, so let me add some thoughts. In the third row, I just write out to two sum-signs. Here, the matrix has N rows. In the fourth row, I realign the matrix so that there is one row for every epsilon, so the number of rows is infinite here. Note that the last three parts in the matrix have the same number of elements, just differencing by a factor $\\rho$ in each row. In the fifth row, I apply the rule that the variance of the sum of independent shocks is the sum of the variances of those shocks and notice that each $\\rho^j$ element is summed up $N$ times.\nThe end result looks neat, but is probably wrong. Why do I think so? Because I run a MCS in R and things don't add up:\nnrMCS <- 10000\nN <- 100\npers <- 0.9\nmeans <- numeric(nrMCS)\nfor (i in 1:nrMCS) {\n   means[i] <- mean(arima.sim(list(order=c(1,0,0), ar=pers), n = N))\n}\n#quantile(means, probs=c(0.025, 0.05, 0.5, 0.95, 0.975))\n#That is the empirical standard error\nsd(means)\n0.9459876\n#This should be the standard error according to my formula\n1/(N*(1-pers))\n0.1\n\nAny hints on what I am doing wrong would be great! Or maybe a hint where I can find the correct derivation (I couldn't find anything). Is the problem maybe that I assume independence between the same errors?\n$$Var(X + X) = Var(2X) = 4Var(X) \\neq 2Var(X)$$\nI thought about that, but don't see where I make that erroneous assumption in my derivation.\nUPDATE\nI forgot to square the rhos, as Nuzhi correctly pointed out. Hence it should look like:\n$$                Var(\\overline{x}) = \\frac{\\sigma_{\\varepsilon}^2}{N^2} \\begin{pmatrix} \\rho^{2\\times0}   + \\\\ \n                                                      (\\rho^0 + \\rho^1)^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2)^2  + \\\\\n                                                      \\cdots \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-2})^2  + \\\\\n                                                      (\\rho^0 + \\rho^1 + \\rho^2 + \\dots + \\rho^{N-1})^2  + \\\\\n                                                      (\\rho^1 + \\rho^2 + \\rho^3 + \\dots + \\rho^{N})^2    + \\\\\n                                                      (\\rho^2 + \\rho^3 + \\rho^4 + \\dots + \\rho^{N+1})^2  + \\\\\n                                                      \\cdots\\\\\n                                      \\end{pmatrix} $$\n", "A": "This is the R code btw  ..\nnrMCS <- 10000\nN <- 100\npers <- 0.9\nmeans <- numeric(nrMCS)\n\n\nfor (i in 1:nrMCS) {\n  means[i] <- mean(arima.sim(model=list(ar=c(pers)), n = N,mean=0,sd=1))\n}\n#Simulation answer\nans1 <-sd(means)\n\n#This should be the standard error according to the given formula\ncov <- 0\nfor(i in 1:N){\nfor(j in 1:N){\ncov <- cov +(1/((N^2)*(1-pers^2)))*pers^abs(j-i)\n }\n}\n\nans2 <- sqrt(cov)\n\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/40585", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "6", "answer_count": 4, "answer_id": 2}}
{"Q": "Probability statistics I pick sports for fun with my friends. we pick 5 sports picks everyday. what is the probability of going at least 3/5 for 50 days in a row? How would you set this up\n", "A": "Well you haven't told us much in your question formulation so I am going to assume that the chance of  picking correctly is the same as the chance of picking incorrectly and so all events have equal probability of $0.5$.\nNow, to solve this problem, let's think about what is the probability of picking at least 3/5 sports correctly in one day. Define $X=$ the number of sports correctly picked in a day. Then, the probability of picking at least 3 out of the 5 sports correctly is the following: \n\\begin{align*}\nPr(X\\geq3) &= Pr(X=3) + Pr(X=4) + Pr(X=5)\\\\\n&={5\\choose 3}\\left(\\frac{1}{2}\\right)^3\\left(\\frac{1}{2}\\right)^2+{5\\choose 4}\\left(\\frac{1}{2}\\right)^4\\left(\\frac{1}{2}\\right)^1+{5\\choose 5}\\left(\\frac{1}{2}\\right)^5\\left(\\frac{1}{2}\\right)^0\\\\\n&=10\\left(\\frac{1}{2}\\right)^5+5\\left(\\frac{1}{2}\\right)^5+1\\left(\\frac{1}{2}\\right)^5\\\\\n&=\\left(\\frac{1}{2}\\right)^5(10+5+1)\\\\\n&=\\left(\\frac{1}{2}\\right)^5(16)\\\\\n&=0.5\n\\end{align*} \nAnd so, the probability of selecting 3 out of the 5 sports correctly in one day is 0.5. This shouldn't come as a surprise since we could have picked X={0,1,2,3,4,5}  sports correctly and only half (i.e., 0.5) of that list is at least 3 or greater.  \nNow, ultimately, you want to know what is the probability of picking 3 out of the 5 sports correctly for 50 days in a row.  And so, this probability is the following: \n\\begin{align*}\nPr(X\\geq 3 \\text{ for }50 \\text{ days in a row})&=Pr(X\\geq 3 \\text{ on day 1 and day 2 ... and day 50})\\\\\n&=Pr(X\\geq 3 \\text{ on day 1})\\times\\cdots\\times Pr(X\\geq 3 \\text{ on day 50})\\\\\n&=\\left(\\frac{1}{2}\\right)^5(16)\\times\\cdots\\times \\left(\\frac{1}{2}\\right)^5(16)\\\\\n&=\\left(\\left(\\frac{1}{2}\\right)^5(16)\\right)^{50}\\\\\n&=(0.50)^{50}\\\\\n&=8.881784e-16\n\\end{align*}\nSo practically a 0% chance of it occurring.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/182642", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Monty hall problem, getting different probabilities using different formulas? In my Monty hall problem, I am computing what is the probability that P(H=1|D=3) i.e. price is behind door 1 and the 3rd door is opened.\n\n$P(H=1|D=3) = p(H=1) * \\frac{p(D=3|H=1)} {p(D=3)} = 1/3 * 1/2 / 1/3 = 1/2 = 50\\%$\n$P(H=1|D=3) = p(H=1)  \\frac{p(D=3|H=1}{\\sum_{i=1} ^3 p(H=i) * p(D=3|H=i)} = \\frac{1/3 * 1/2} { 1/3 * 1/2 + 1/3 * 1 + 0} = 1/3 = 33\\% $\n\nAnd when I use Bayes formula without summation in the denominator I get answer 50% and when I use summation in the denominator in Bayes formula I get 33%. Why there is difference? \n", "A": "Your problem is that $P(D=3 \\mid H=2) = \\frac{1}{2}$ but you incorrectly wrote that it equals 1\nExplanation:\nLet $D$ be the door with the prize. Let $H$ be the door Mr. Hall opens. The joint distribution of $D$ and $H$ is described in this table:\n$$ \\begin{array}{cccc} &\\text{H=1}&\\text{H=2}&\\text{H=3} \\\\\nD = 1 & 0 & \\frac{1}{6} & \\frac{1}{6} \\\\\nD = 2 & \\frac{1}{6} & 0  & \\frac{1}{6} \\\\\nD = 3 & \\frac{1}{6} & \\frac{1}{6} & 0 \n\\end{array}$$\nThe problem is in the denominator of your second formula:\n\n$P(H=1|D=3) = p(H=1)  \\frac{p(D=3|H=1}{\\sum_{i=1} ^3 p(H=i) * p(D=3|H=i)} = \\frac{1/3 * 1/2} { 1/3 * 1/2 + 1/3 * 1 + 0} = 1/3 = 33\\% $\n\nYou incorrectly wrote $P(D=3 \\mid H=2) = 1$ That is incorrect.\n$$P(D=3\\mid H=2) = \\frac{P(D=3,H=2)}{P(H=2)} = \\frac{1/6}{1/3} = \\frac{1}{2}$$\nMake that correction and you have:\n$P(H=1|D=3) = p(H=1)  \\frac{p(D=3|H=1}{\\sum_{i=1} ^3 p(H=i) * p(D=3|H=i)} = \\frac{1/3 * 1/2} { 1/3 * 1/2 + 1/3 * 1/2 + 0} = 1/2$\nwhich is correct\nAn additional comment:\nThis analysis does not solve the Monty Hall problem because it completely neglects the door $C$ that the contestant chooses.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/213693", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 3, "answer_id": 0}}
{"Q": "Writing MA and AR representations I have to determine if\n$$(1 - 1.1B + 0.8B^2)Y_t = (1 - 1.7B + 0.72B^2)a_t$$\nis stationary, invertible or both.\nI have shown that $\\Phi(B) = 1 - 1.1B + 0.8B^2 = 0$ when $B_{1,2} = 0.6875 \\pm 0.8817i$, whose moduli are both larger than 1, hence is stationary. Similarly, I have shown that $\\Theta(B) = 1 - 1.7B + 0.72B^2 = 0$, when $B_1 = 1.25 > 1$ and $B_2 = 1.11 > 1$, hence is invertible.\nI also need to express the model as a MA and AR representation if it exists; which they do as I have already shown. However, to write as an MA process, I would need to write as:\n$$Y_t = \\frac{1 - 1.7B + 0.72B^2}{1 - 1.1B + 0.8B^2}a_t$$\nand for an AR process as:\n$$\\frac{1 - 1.1B + 0.8B^2}{1 - 1.7B + 0.72B^2}Y_t = a_t$$\nHowever, I am confused on how to do this given the division of the quadratic expressions. Should I use long division or is there some expansion formula I should be using?\n", "A": "Try the partial fraction decomposition:\n\\begin{align}\n\\frac{1}{(1 - \\alpha B)(1 - \\beta B)} & = \\frac{\\alpha/(\\alpha - \\beta)}{1 - \\alpha B} +  \\frac{\\beta/(\\beta - \\alpha)}{1 - \\beta B} \\\\\n& = (\\alpha - \\beta)^{-1} \\left( \\alpha(1 - \\alpha B)^{-1} - \\beta (1 - \\beta B)^{-1} \\right) \\\\\n& = (\\alpha - \\beta)^{-1} \\left( \\alpha \\sum_{k = 0}^\\infty \\alpha^k B^k - \\beta \\sum_{k = 0}^\\infty \\beta^k B^k \\right) \\\\\n& = (\\alpha - \\beta)^{-1} \\sum_{k = 0}^\\infty (\\alpha^{k+1} - \\beta^{k+1}) B^k\n\\end{align}\nand apply it to both cases like:\n\\begin{align}\n\\frac{1 + c B + d B^2}{(1 - \\alpha B)(1 - \\beta B)} & = (1 + c B + d B^2) (\\alpha - \\beta)^{-1} \\sum_{k = 0}^\\infty (\\alpha^{k+1} - \\beta^{k+1}) B^k \\\\\n& = (\\alpha - \\beta)^{-1} \\sum_{k = 0}^\\infty (\\alpha^{k+1} - \\beta^{k+1}) (1 + c B + d B^2) B^k \\\\\n& = (\\alpha - \\beta)^{-1} \\sum_{k = 0}^\\infty (\\alpha^{k+1} - \\beta^{k+1}) (B^k + c B^{k+1} + d B^{k+2})\n\\end{align}\nand by distributing and reindexing the summations, we have\n\\begin{align}\n& = (\\alpha - \\beta)^{-1} \\left( \\sum_{k = 0}^\\infty (\\alpha^{k+1} - \\beta^{k+1}) B^k + \\sum_{k = 1}^\\infty c (\\alpha^k - \\beta^k) B^k + \\sum_{k = 2}^\\infty d (\\alpha^{k-1} - \\beta^{k-1}) B^k) \\right) \\\\\n& = (\\alpha - \\beta)^{-1} \\left( (\\alpha - \\beta) + (\\alpha^2 - \\beta^2 + c(\\alpha - \\beta)) B + \\sum_{k = 2}^\\infty [(\\alpha^{k+1} - \\beta^{k+1}) + c (\\alpha^k - \\beta^k) + d (\\alpha^{k-1} - \\beta^{k-1})] B^k \\right) \\\\\n& =  1 + (\\alpha + \\beta + c) B + (\\alpha - \\beta)^{-1} \\sum_{k = 2}^\\infty [(\\alpha^{k+1} - \\beta^{k+1}) + c (\\alpha^k - \\beta^k) + d (\\alpha^{k-1} - \\beta^{k-1})] B^k\n\\end{align}\nAssuming I haven't made any mistakes, this gives the AR representation when $\\alpha = 0.9$, $\\beta = 0.8$, $c = -1.1$, and $d = 0.8$ and gives the MA process when $\\alpha, \\beta = 0.68750 \\pm 0.88167 i$, $c = -1.7$, and $d = 0.72$. Perhaps you could even simplify this more using the difference of nth powers formula (certainly you could cancel the $(\\alpha - \\beta)^{-1}$ term this way, but I don't know if you would call the result \"simpler.\"\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/513073", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Finding method of moments estimate for density function $f(x|\\alpha) = \\frac {\\Gamma(2\\alpha)} {\\Gamma(\\alpha)^2}[x(1-x)]^{\\alpha - 1}$ \nSuppose that $X_1, X_2, ..., X_n$ are i.i.d random variables on the interval $[0,1]$ with the density function\n$$\nf(x|\\alpha) = \\frac {\\Gamma(2\\alpha)} {\\Gamma(\\alpha)^2}[x(1-x)]^{\\alpha - 1}\n$$\nwhere $\\alpha > 0$ is a parameter to be estimated from the sample. It can be shown that\n\\begin{align}\nE(X) &= \\frac 1 2 \\\\\n\\text{Var}(X) &= \\frac 1 {4(2\\alpha +1)}\n\\end{align}\nHow can the method of moments be used to estimate $\\alpha$?\n\nMy attempt\nIt is clear that the first moment of $X$ is $\\mu_1 = E(X) = \\frac 1 2$.\nThe second moment of $X$ is given by\n\\begin{align}\n\\mu_2 &= E(X^2) \\\\\n&= \\text{Var}(X) + (E(X))^2 \\\\\n&= \\frac 1 {4(2\\alpha + 1)} + \\frac 1 4 \\\\\n&= \\frac {\\alpha + 1} {2(2\\alpha + 1)}\n\\end{align}\nThus we have the relation\n$$\n\\alpha = \\frac {1 - 2\\mu_2} {4\\mu_2 - 1}\n$$\nUsing the method of moments, we obtain\n\\begin{align}\n\\hat{\\alpha} &= \\frac {1 - 2\\hat{\\mu_2}} {4\\hat{\\mu_2} - 1} \\\\\n&= \\frac {1 - \\frac 2 n \\sum_{i=1}^n X_i^2} {\\frac 4 n \\sum_{i=1}^n X_i^2 - 1} \\\\\n&= \\frac {n - 2 \\sum_{i=1}^n X_i^2} {4 \\sum_{i=1}^n X_i^2 - n}\n\\end{align}\nSolution provided\n$$\n\\hat{\\alpha} = \\frac n {8 \\sum_{i=1}^n X_i^2 - 2n} - \\frac 1 2\n$$\nDid I apply the method of moments correctly for this question? I can't seem to obtain the form as suggested in the sample solution provided. Any advice would be greatly appreciated!\n", "A": "You are right. In fact\n\\begin{align}\n\\frac n {8 \\sum_{i=1}^n X_i^2 - 2n} - \\frac 1 2&= \\frac{2n - 8 \\sum_{i=1}^n X_i^2 + 2n}{16 \\sum_{i=1}^n X_i^2 - 4n}\\\\\n&= \\frac{4n - 8 \\sum_{i=1}^n X_i^2}{16 \\sum_{i=1}^n X_i^2 - 4n}\\\\\n&=\\frac {n - 2 \\sum_{i=1}^n X_i^2} {4 \\sum_{i=1}^n X_i^2 - n}\\,.\n\\end{align}\nThe last equality is just a simplification by $4$.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/541121", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 2, "answer_id": 0}}
