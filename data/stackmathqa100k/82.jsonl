{"Q": "Finite Population Variance for a Changing Population How does the addition of one unit affect the population variance of a finite population if everything else remains unchanged? What are the conditions such that the new unit leaves the variance unchanged (increases/decreases it)?\nI was able to find the following paper regarding sample variances for changing finite populations:\nhttp://www.amstat.org/sections/srms/Proceedings/papers/1987_087.pdf.\nBut I am asking specifically about population variances. Any help is appreciated.\n", "A": "I was unable to find the sample calculations that correspond to the specific problem here (as suggested by Glen_b), but I was able to confirm the following answer with numerical calculations in R at the bottom of this answer.\nLet $N$ be the initial number of units in the population and $N + 1$ be the number of units in the population after the change. Denote the initial set of observations $X = \\{x_1, \\ldots, x_N\\}$ (i.e., one observation corresponding to each population unit). Denote the set of observations after the change $Y = X \\cup \\{x_{N+1}\\}$.\nThe mean of $X$ is\n$\\mu_X = \\frac{\\sum_{i=1}^N{x_i}}{N}$.\nThe mean of Y is\n$\\mu_Y = \\frac{\\sum_{i=1}^{N+1}{x_i}}{N+1}\n= \\mu_X \\frac{N}{N+1} + \\frac{x_{N+1}}{N+1}$\nDefine $x_{N+1}$ as the original mean, $\\mu_X$, plus some $\\varepsilon$. Then,\nthe mean of $Y$ is\n$\\mu_Y = \\mu_X \\frac{N}{N+1} + \\frac{\\mu_X\n+ \\varepsilon}{N+1} = \\mu_X + \\frac{\\varepsilon}{N+1}$\nThe variance of $Y$ is \n$\\sigma^2_Y = \\frac{\\sum_{i=1}^{N+1}\n\\left(x_i - \\mu_Y \\right)^2}{N+1} =\n\\frac{\\sum_{i=1}^{N+1}\n\\left(x_i - \\mu_X - \\frac{\\varepsilon}{N + 1} \\right)^2}{N+1}$\n$= \\frac{\\sum_{i=1}^{N} x_i^2 + \\mu_X^2\n+ \\frac{\\varepsilon^2}{\\left(N+1\\right)^2} - 2x_i\\mu_X\n- 2x_i\\frac{\\varepsilon}{N+1} + 2\\mu_X\\frac{\\varepsilon}{N+1}}{N + 1}$\n$\\frac{\\left(\\mu_X + \\varepsilon - \\mu_X - \\frac{\\varepsilon}{N + 1}\\right)}{N\n+ 1} $\n$ = \\frac{N}{N+1}\\sigma^2_X + \\frac{N\\varepsilon^2}{\\left(N+1\\right)^3}\n    - \\frac{2N\\mu_X\\varepsilon}{\\left(N+1\\right)^2}\n    + \\frac{2N\\mu_X\\varepsilon}{\\left(N+1\\right)^2}\n    + \\frac{N^2\\varepsilon^2}{\\left(N+1\\right)^3}$\n$ = \\frac{N}{N+1}\n\\sigma^2_X + \\frac{N}{\\left(N+1\\right)^2}\\varepsilon^2$\nWhen $x_{N+1}$ is equal to $\\mu_X$, the variance of $Y$ is \n$\\frac{N}{N+1}\\sigma^2_X < \\sigma^2_X $\nThus, when $\\varepsilon$ is sufficiently small $\\sigma^2_Y$ is less than\n$\\sigma^2_X$. To determine how large $\\varepsilon$ should be so that the\nvariance of $Y$ is greater than the variance of $X$, I set the two variances\nequal.\n$ \\frac{N}{N+1} \\sigma^2_X\n    + \\frac{N}{\\left(N+1\\right)^2}\\varepsilon^2 = \\sigma^2_X$\n$   \\frac{N}{\\left(N+1\\right)^2}\\varepsilon^2 = \\frac{1}{N+1} \\sigma^2_X$\n$   \\varepsilon^2 = \\frac{N+1}{N} \\sigma^2_X $\n$ \\varepsilon = \\pm \\sigma_X\n    \\sqrt{\\frac{N+1}{N}}$\nThus, adding a unit who's observation is within $\\sqrt{\\frac{N+1}{N}}$ standard deviations\nof the old mean will lead to a lower variance.\n\nThe following R script verifies the above conclusion:\nN <- 10\nX <- runif(N)\nwidth <- sqrt((N+1)/N)\n# on the boundary\nvar(c(X, mean(X) + width * sqrt(var(X)))) - var(X) == 0\n# outside the boundary\nvar(c(X, mean(X) + width * sqrt(var(X)) + 1)) - var(X) > 0\n# inside the boundary\nvar(c(X, mean(X))) - var(X) < 0\n\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/117111", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 3, "answer_id": 0}}
{"Q": "Find the mean and standard error for mean. Have I used correct formulas in this given situation? \nA university has 807 faculty members. For each faculty member, the number of refereed publications was recorded. This number is not directly available on the database, so requires the investigator to examine each record separately. A frequency table number of refereed publication is given below for an SRS of 50 faculty members.  \n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{Refereed Publications}& 0 & 1 & 2& 3 & 4 & 5& 6 & 7 & 8& 9 & 10  \\\\ \\hline\n\\text{Faculty Members} &28 &4 &3& 4 & 4 & 2& 1 & 0 & 2& 1 & 1\\\\ \\hline\n\\end{array}\nb) Estimate the mean number of publications per faculty member, and give the SE for your estimate:\n  \\begin{array}{|c|c|c|c|}\n\\hline\ny& f & f*y & f*y^2 \\\\ \\hline\n0 &28 & 0&0\\\\ \\hline\n1 &  4& 4&4\\\\ \\hline\n 2&  3& 6&12\\\\ \\hline\n 3& 4& 12&36\\\\ \\hline\n 4&  4& 16&64\\\\ \\hline\n 5&  2& 10&50\\\\ \\hline\n6 & 1& 6&36\\\\ \\hline\n7 & 0& 0&0\\\\ \\hline\n 8&  2& 16&128\\\\ \\hline\n 9&  1& 9&81\\\\ \\hline\n10& 1& 10&100\\\\ \\hline\n\\text{total}& 50&89 &511\\\\ \\hline\n\\end{array}\n\n\\begin{align}\n\\bar{y} &= \\frac{\\sum fy}{\\sum f} &\\rightarrow& &\\frac{89}{50} &= 1.78  \\\\[10pt]\nSD &= \\sqrt{ \\frac{\\sum fy^2}{\\sum f}-(\\bar{y})^2} &\\rightarrow& &\\sqrt{\\frac{511}{50}-(1.78)^2} &= 2.66  \\\\[10pt]\nSE(\\bar{y}) &= \\frac{s}{\\sqrt{n}}\\sqrt{1-\\frac{n}{N}} &\\rightarrow& &\\frac{2.66}{\\sqrt{50}}\\sqrt{1-\\frac{50}{807}} &= 0.3643\n\\end{align}\nDid I do it correctly?\n\nd) Estimate the proportion of faculty members with no publications and give a $95\\%$ CI.\n\n\\begin{align}\np &= \\frac{y}{n} &\\rightarrow& &\\frac{28}{50} &= 0.56  \\\\[10pt]\nSE(p) &= \\sqrt{\\frac{p(1-p)}{n-1}\\bigg(1-\\frac{n}{N}\\bigg)} &\\rightarrow& &\\sqrt{\\frac{0.56(0.44)}{49}\\bigg(1-\\frac{50}{807}\\bigg)} &= 0.0687  \\\\[10pt]\n95\\%\\ CI &= p\\pm 1.96[SE(p)] &\\rightarrow& &0.56 \\pm1.96(0.0687) &= [0.425,0.695]\n\\end{align} \nAm I using the correct formulas?\n", "A": "THE formulas for the descriptive and inference statistics used by you are correct  in terms of SRS ((simple random sampling ) and variance-estimation.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/304894", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Stationary Distributions of a irreducible Markov chain I was trying to get all Stationary Distributions of the following Markov chain. Intuitively, I would say there are two resulting from splitting op the irreducible Markov chain into two reducible ones. However, I feel this is not mathematically correct. How else would I be able to find all stationary distributions?\n \\begin{bmatrix}\n            \\frac{1}{3} & \\frac{2}{3} & 0 & 0 & 0 \\\\ \n            \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n            0 & \\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\ \n            0 & 0 & 0 & \\frac{1}{4} & \\frac{3}{4} \\\\ \n            0 & 0 & 0 & \\frac{1}{3} & \\frac{2}{3} \\\\ \n        \\end{bmatrix}\n", "A": "Conditioned on $X_0\\in\\{0,1\\}$ we have from solving\n\\begin{align}\n\\pi_0 &= \\frac13\\pi_0 + \\frac12\\pi_1\\\\\n\\pi_1 &= \\frac23\\pi_0 + \\frac12\\pi_1\\\\\n\\pi_0 + \\pi_1 &= 1\n\\end{align}\n$\\pi_0 = \\frac37$, $\\pi_1=\\frac 47$.\nConditioned on $X_0\\in\\{3,4\\}$ we have by solving a similar system of equations $\\pi_3 = \\frac4{13}$, $\\pi_4=\\frac9{13}$.\nConditioned on $X_0=2$ we have $\\tilde\\pi_i = \\frac12\\pi_i$ for $i\\in\\{0,1,3,4\\}$.\nSo we have three stationary distributions: those obtained by conditioning on $X_0\\in\\{0,1\\}$ and $X_0\\in\\{3,4\\}$, and the one obtained by conditioning on $X_0=2$: \n$$\n\\tilde\\pi = \\left(\n\\begin{array}{ccccc}\n \\frac{3}{14} & \\frac{2}{7} & 0 & \\frac{2}{13} & \\frac{9}{26} \\\\\n\\end{array}\n\\right).\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/438165", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "MSE of the Jackknife Estimator for the Uniform distribution The Jackknife is a resampling method, a predecessor of the Bootstrap, which is useful for estimating the bias and variance of a statistic. This can also be used to apply a \"bias correction\" to an existing estimator.\nGiven the estimand $\\theta$ and an estimator $\\hat\\theta \\equiv \\hat\\theta(X_1, X_2, \\cdots X_n)$, the Jackknife estimator (with respect to $\\hat\\theta$) is defined as\n$$\\hat\\theta_J = \\hat\\theta + (n-1)\\left(\\hat\\theta - \\frac{1}{n}\\sum_{i=1}^n\\hat\\theta_{-i}\\right),$$\nwhere the $\\hat\\theta_{-i}$ terms denote the estimated value ($\\hat\\theta$) after \"holding out\" the $i^{th}$ observation.\n\nLet $X_1, X_2, \\cdots X_n \\stackrel{\\text{iid}}{\\sim} \\text{Unif}(0, \\theta)$ and consider the estimator $\\hat\\theta = X_{(n)}$ (i.e. the maximum value, also the MLE). Note that\n$$\\hat\\theta_{-i} = \\begin{cases}\nX_{(n-1)}, & X_i = X_{(n)} \\\\[1.2ex]\nX_{(n)}, & X_i \\neq X_{(n)}\n\\end{cases}.$$\nThus the Jackknife estimator here can be written as a linear combination of the two largest values \n\\begin{align*}\n\\hat\\theta_J &= X_{(n)} + \\frac{n-1}{n}\\left(X_{(n)} - X_{(n-1)}\\right) \\\\[1.3ex]\n&= \\frac{2n-1}{n}X_{(n)} - \\frac{n-1}{n} X_{(n-1)}.\n\\end{align*}\nWhat is the bias, variance and mean square error ?\n", "A": "It is well known that the order statistics, sampled from a uniform distribution, are Beta-distributed random variables (when properly scaled).\n$$\\frac{X_{(j)}}{\\theta} \\sim \\text{Beta}(j, n+1-j)$$\nUsing standard properties of the Beta distribution we can obtain the mean and variance of $X_{(n)}$ and $X_{(n-1)}$.\nBias\n\\begin{align*}\nE\\left(\\hat\\theta_J\\right) &= \\frac{2n-1}{n}E(X_{(n)}) - \\frac{n-1}{n}E(X_{(n-1)}) \\\\[1.3ex]\n&= \\frac{2n-1}{n}\\frac{n}{n+1}\\theta - \\frac{n-1}{n}\\frac{n-1}{n+1}\\theta \\\\[1.3ex]\n&= \\frac{n(n+1) - 1}{n(n+1)} \\theta\n\\end{align*}\nTherefore the bias of $\\hat\\theta_J$ is given by\n$$\\text{Bias}_\\theta(\\hat\\theta_J) = \\frac{-\\theta}{n(n+1)}$$\nVariance\nNote: Covariance is derived here.\n\\begin{align*}\n\\text{Var}\\left(\\hat\\theta_J\\right) &= \\frac{(2n-1)^2}{n^2}\\text{Var}(X_{(n)}) + \\frac{(n-1)^2}{n^2}\\text{Var}(X_{(n-1)}) - 2 \\frac{2n-1}{n}\\frac{n-1}{n}\\text{Cov}(X_{(n)}, X_{(n-1)}) \\\\[1.3ex]\n&= \\frac{(2n-1)^2}{n^2}\\frac{n\\theta^2}{(n+1)^2(n+2)} + \\frac{(n-1)^2}{n^2}\\frac{2(n-1)\\theta^2}{(n+1)^2(n+2)} -  \\frac{2(2n-1)(n-1)}{n^2}\\frac{(n-1)\\theta^2}{(n+1)^2(n+2)} \\\\[1.5ex]\n\\end{align*}\n$$= \\frac{(2n^2-1)\\theta^2}{n(n+1)^2(n+2)}$$\nMSE\nUsing the decomposition $\\text{MSE}_\\theta(\\hat\\theta) = \\text{Bias}^2_\\theta(\\hat\\theta) + \\text{Var}(\\hat\\theta)$, we have\n\\begin{align*}\n\\text{MSE}_\\theta(\\hat\\theta_J) &= \\left(\\frac{-\\theta}{n(n+1)}\\right)^2  + \\frac{(2n^2-1)\\theta^2}{n(n+1)^2(n+2)} \\\\[1.3ex]\n&= \\frac{2(n-1+1/n)\\theta^2}{n(n+1)(n+2)}\\\\[1.3ex]\n &= \\mathcal O(n^{-2})\n\\end{align*}\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/458883", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "4", "answer_count": 1, "answer_id": 0}}
{"Q": "Pdf of the sum of two independent Uniform R.V., but not identical \nQuestion. Suppose $X \\sim U([1,3])$ and $Y \\sim U([1,2] \\cup [4,5])$ are two independent random variables (but obviously not identically distributed). Find the pdf of $X + Y$.\n\nSo far. I'm familiar with the theoretical mechanics to set up a solution. So, if we let $\\lambda$ be the Lebesgue measure and notice that $[1,2]$ and $[4,5]$ disjoint, then the pdfs are\n$$f_X(x) = \n\\begin{cases}\n\\frac{1}{2}, &x \\in [1,3] \\\\\n0, &\\text{otherwise}\n\\end{cases}\n\\quad\\text{and}\\quad\nf_Y(y) =\n\\begin{cases}\n\\frac{1}{\\lambda([1,2] \\cup [4,5])} = \\frac{1}{1 + 1} = \\frac{1}{2}, &y \\in [1,2] \\cup [4,5] \\\\\n0, &\\text{otherwise}\n\\end{cases}\n$$\nNow, let $Z = X + Y$. Then, the pdf of $Z$ is the following convolution\n$$f_Z(t) = \\int_{-\\infty}^{\\infty}f_X(x)f_Y(t - x)dx = \\int_{-\\infty}^{\\infty}f_X(t -y)f_Y(y)dy.$$\nTo me, the latter integral seems like the better choice to use. So, we have that $f_X(t -y)f_Y(y)$ is either $0$ or $\\frac{1}{4}$. But I'm having some difficulty on choosing my bounds of integration?\n", "A": "Here is a plot as suggested by comments\n\nWhat I was getting at is it is a bit cumbersome to draw a picture for problems where we have disjoint intervals (see my comment above). It's not bad here, but perhaps we had $X \\sim U([1,5])$ and $Y \\sim U([1,2] \\cup [4,5] \\cup [7,8] \\cup [10, 11])$.\nUsing @whuber idea: We notice that the parallelogram from $[4,5]$ is just a translation of the one from $[1,2]$. So, if we let $Y_1 \\sim U([1,2])$, then we find that\n$$f_{X+Y_1}(z) =\n\\begin{cases}\n\\frac{1}{4}z - \\frac{1}{2}, &z \\in (2,3) \\tag{$\\dagger$}\\\\\n\\frac{1}{2}z - \\frac{3}{2}, &z \\in (3,4)\\\\\n\\frac{5}{4} - \\frac{1}{4}z, &z \\in (4,5)\\\\\n0, &\\text{otherwise}\n\\end{cases}\n$$\nSince, $Y_2 \\sim U([4,5])$ is a translation of $Y_1$, take each case in $(\\dagger)$ and add 3 to any constant term. Then you arrive at ($\\star$) below.\nBrute force way:\n\n*\n\n*$\\mathbf{2 < z < 3}$: $y=1$ to $y = z-1$, which gives $\\frac{1}{4}z - \\frac{1}{2}$.\n\n*$\\mathbf{3 < z < 4}$: $y=1$ to $y = z-1$, such that $2\\int_1^{z-1}\\frac{1}{4}dy = \\frac{1}{2}z - \\frac{3}{2}$.\n\n*$\\mathbf{4 < z < 5}$: $y=z-3$ to $y=2$, which gives $\\frac{5}{4} - \\frac{1}{4}z$.\n\n*$\\mathbf{5 < z < 6}$: $y=4$ to $y = z-1$, which gives $\\frac{1}{4}z - \\frac{5}{4}$.\n\n*$\\mathbf{6 < z < 7}$: $y = 4$ to $y = z-2$, such that $2\\int_4^{z-2}\\frac{1}{4}dy = \\frac{1}{2}z - 3$.\n\n*$\\mathbf{7 < z < 8}$: $y = z-3$ to $y=5$, which gives $2 - \\frac{1}{4}z$.\n\nTherefore,\n$$f_Z(z) =\n\\begin{cases}\n\\frac{1}{4}z - \\frac{1}{2}, &z \\in (2,3) \\tag{$\\star$}\\\\\n\\frac{1}{2}z - \\frac{3}{2}, &z \\in (3,4)\\\\\n\\frac{5}{4} - \\frac{1}{4}z, &z \\in (4,5)\\\\\n\\frac{1}{4}z - \\frac{5}{4}, &z \\in (5,6)\\\\\n\\frac{1}{2}z - 3, &z \\in (6,7)\\\\\n2 - \\frac{1}{4}z, &z \\in (7,8)\\\\\n0, &\\text{otherwise}\n\\end{cases}$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/489224", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 1, "answer_id": 0}}
{"Q": "Generating random variables from a given distribution function using inversion sampling Given this distribution function $f(x)$ :\n$$ \nf\\left(x\\right)=\\left\\{\\begin{matrix}x+1,-1\\le x\\le0\\\\1-x,0<x\\le1\\\\\\end{matrix}\\right.\n$$\nGenerate random variables using Inverse sampling method in R:\nhere is my attempt :\nf <- function(x){\nifelse(x<=0&&x>=-1,x+1,1-x)\n}\n\nintegrate(Vectorize(f),-1,1)$value == TRUE\nplot(Vectorize(f),xlim = c(-2,2))\n\n\n$$ \nF\\left(x\\right)=\\left\\{\\begin{matrix}\\frac{x^2}{2}+x,-1\\le x\\le0\\\\x-\\frac{x^2}{2},0<x\\le1\\\\\\end{matrix}\\right.\n$$\n$F^{-1}$:\nF_inver <- function(x){ifelse(x<=0&&x>=-1,1-sqrt(2*x+1),1-sqrt(-2*x+1))}\n\n\nI believe that the my inverse function isn't correct\n\n", "A": "The cumulative distribution function, $F(x)$, is given by\n$$\nF(x) = \\int_{-\\infty}^x f(t)dt\n$$\nSo for, $- 1 \\leq x \\leq 0$,\n\\begin{align*}\nF(x) &= \\int_{-\\infty}^x f(t)dt \\\\\n &= \\frac{x^2}{2} +x + \\frac{1}{2}\n\\end{align*}\nand for $0 \\leq x \\leq 1$,\n\\begin{align*}\nF(x) &= \\int_{-\\infty}^0 f(t)dt + \\int_0^x f(t)dt \\\\\n& = \\frac{1}{2} + x - \\frac{x^2}{2}\n\\end{align*}\nThus,\n$$\nF\\left(x\\right)=\\left\\{\n\\begin{align*}\n&\\frac{x^2}{2}+x + \\frac{1}{2},-1 \\leq x \\leq 0 \\\\\n&\\frac{1}{2}  +x -\\frac{x^2}{2},0< x \\leq 1 \n\\end{align*}\n\\right.\n$$\nFor $0 \\leq y \\leq \\frac{1}{2}$,\n\\begin{align*}\ny = F(x)  &\\iff y = \\frac{x^2}{2}+x + \\frac{1}{2} \\\\\n&\\iff \\frac{x^2}{2}  +x + \\frac{1}{2}  - y = 0\n\\end{align*}\nThe last line is a second order polynomial equation whose determinant is $2y >0$.\nThe solutions are thus $-1 \\pm \\sqrt{2y}$. Since $-1 \\leq 0 \\leq x$, we have $x= -1 + \\sqrt{2y}$\nFor $\\frac{1}{2} \\leq y \\leq 1$,\n\\begin{align*}\ny = F(x)  &\\iff y = -\\frac{x^2}{2}+ x + \\frac{1}{2} \\\\\n&\\iff -\\frac{x^2}{2}  +x + \\frac{1}{2}  - y = 0\n\\end{align*}\nRepeating the same process as before we find\n$$\nx = 1 - \\sqrt{2(1-y)}\n$$\nThus the inverse function of $F(x)$ (the quantile function) is given by:\n$$\nF^{-1}(y) = \\left\\{\n\\begin{align*}\n&-1 + \\sqrt{2y}, \\ 0 \\leq y \\leq \\frac{1}{2} \\\\\n&1-\\sqrt{2(1-y)}, \\ \\frac{1}{2} < y \\leq 1 \n\\end{align*}\n\\right.\n$$\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/526178", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "1", "answer_count": 1, "answer_id": 0}}
{"Q": "Matrix representation of the OLS of an AR(1) process, Is there any precise way to express the OLS estimator of the centred error terms $\\{u_t\\}\n_{t=1}^{n}$ that follows an AR(1) process? In other words, for\n\\begin{equation}\nu_t=\\rho u_{t-1}+\\varepsilon_t,\\quad \\varepsilon_t\\sim N(0,\\sigma^2)\n\\end{equation}\nis there a matrix representation for\n\\begin{equation}\n\\hat{\\rho}=\\frac{(1/n)\\sum\\limits_{t=1}^{n}u_tu_{t-1}}{(1/n)\\sum\\limits_{t=1}^{n}u_{t-1}^2}\n\\end{equation}\n? I suspect there should be. However, I seem to fail to find it in Hamilton or other sources or derive an elegant expression myself.\nMuch appreciated in advance\n", "A": "To facilitate our analysis, we will use the following $(n-1) \\times n$ matrices:\n$$\\mathbf{M}_0 \\equiv \\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 0 \\\\\n\\end{bmatrix}\n\\quad \\quad \\quad \n\\mathbf{M}_1 \\equiv \\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 \\\\\n\\end{bmatrix},$$\nand the following $n \\times n$ matrices:\n$$\\begin{align}\n\\mathbf{G}_0 \n&\\equiv \\mathbf{M}_0^\\text{T} \\mathbf{M}_1\n= \\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\end{bmatrix} \\\\[20pt]\n\\mathbf{G}_1 \n&\\equiv \\mathbf{M}_0^\\text{T} \\mathbf{M}_0\n= \\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & 0 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\end{bmatrix}.\n\\end{align}$$\nGiven the observable time-series vector $\\mathbf{u} = (u_1,...,u_n)$ we can then write the model in matrix form as:\n$$\\mathbf{M}_1 \\mathbf{u} = \\rho \\mathbf{M}_0 \\mathbf{u} + \\sigma \\boldsymbol{\\varepsilon}\n\\quad \\quad \\quad \\quad \\quad \n\\boldsymbol{\\varepsilon} \\sim \\text{N}(\\mathbf{0}, \\mathbf{I}).$$\nThe OLS estimator for the parameter $\\rho$ is:\n$$\\begin{align}\n\\hat{\\rho}_\\text{OLS} \n&= (\\mathbf{u}^\\text{T} \\mathbf{M}_0^\\text{T} \\mathbf{M}_0 \\mathbf{u} )^{-1} (\\mathbf{u}^\\text{T} \\mathbf{M}_0^\\text{T} \\mathbf{M}_1 \\mathbf{u} ) \\\\[12pt]\n&= (\\mathbf{u}^\\text{T} \\mathbf{G}_1 \\mathbf{u} )^{-1} (\\mathbf{u}^\\text{T} \\mathbf{G}_0 \\mathbf{u} ) \\\\[12pt]\n&= \\frac{\\mathbf{u}^\\text{T} \\mathbf{G}_0 \\mathbf{u}}{\\mathbf{u}^\\text{T} \\mathbf{G}_1 \\mathbf{u}} \\\\[12pt]\n&= \\frac{\\sum_{i=1}^{n-1} u_i u_{i+1}}{\\sum_{i=1}^{n-1} u_i^2 }.\n\\end{align}$$\nNote that the OLS estimator for an auto-regressive process is not equivalent to the MLE, since the log-likelihood contains a log-determinant term that is a function of the auto-regression parameter.  The MLE can be obtained via iterative methods if desired.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/539042", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "2", "answer_count": 2, "answer_id": 0}}
{"Q": "Derivation of integrating over many parameters in Neyman-Scott Problem? I am trying to follow the derivation for the variance estimator in the Neyman-Scott problem given in this article. However, I'm not sure how they go from the 2nd to the 3rd line of this derivation. Any help is appreciated, thanks!\n\n", "A": "Each of the integrals indexed by $i$ in the product has the form\n$$\\int_{\\mathbb R} \\exp\\left[\\phi(\\mu_i,x_i,y_i,\\sigma)\\right]\\,\\mathrm{d}\\mu_i.$$\nWhen $\\phi$ is linear or quadratic in $\\mu_i,$ such integrals have elementary values.  The more difficult circumstance is the quadratic.  We can succeed with such integrations merely by knowing\n$$\\int_\\mathbb{R} \\exp\\left[-\\frac{1}{2}z^2\\right]\\,\\mathrm{d}z = \\sqrt{2\\pi}.$$\nGenerally, let $\\theta$ denote everything that isn't $\\mu_i$ ($\\theta=(x_i,y_i,\\sigma)$ in this instance).  Writing $x=\\mu_i,$ the quadratic case is when $\\phi$ can be expressed in the form\n$$\\phi(x,\\theta) = -A(\\theta)^2 x^2 + B(\\theta)x + C(\\theta)$$\nfor arbitrary functions $A, B, C$ and $A(\\theta)\\ne 0.$  The reasons for expressing the coefficient of $x^2$ in this way are (i) to guarantee the integral exists and (ii) to avoid using square roots.\nThus, we are concerned with evaluating\n$$f(\\theta) = \\int_\\mathbb{R} \\exp\\left[\\phi(x,\\theta)\\right]\\,\\mathrm{d}x=\\int_\\mathbb{R} \\exp\\left[-A(\\theta)^2x^2 + B(\\theta)x + C(\\theta)\\right]\\,\\mathrm{d}x.$$\nThis done by completing the square, in exactly the same way the quadratic formula is traditionally derived.  The result amounts to changing the variable of integration to $z$ where\n$$z = A\\sqrt{2}\\,x - \\frac{B}{A\\sqrt{2}};\\quad \\mathrm{d}z = A\\sqrt{2}\\,\\mathrm{d}x.$$\nIn terms of $z,$\n$$\\exp\\left[\\phi(x,\\theta)\\right] = \\exp\\left[-\\frac{1}{2}z^2 + C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right]= \\exp\\left[-\\frac{1}{2}z^2\\right] \\exp\\left[C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right].$$\nThe integral thereby becomes\n$$\\begin{aligned}\nf(\\theta) &= \\frac{1}{A\\sqrt{2}} \\int_\\mathbb{R} \\exp\\left[-\\frac{1}{2}z^2\\right] \\exp\\left[C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right]\\,\\mathrm{d}z \\\\\n&= \\frac{1}{A\\sqrt 2} \\sqrt{2\\pi}\\exp\\left[C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right] \\\\\n&=\\frac{\\sqrt \\pi}{A} \\exp\\left[C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right].\n\\end{aligned}$$\nLet's memorialize this for future reference:\n$$\\boxed{\\int_\\mathbb{R} \\exp\\left[-A(\\theta)^2x^2 + B(\\theta)x + C(\\theta)\\right]\\,\\mathrm{d}x = \\frac{\\sqrt \\pi}{A} \\exp\\left[C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2}\\right].}\\tag{*}$$\n\nTo apply this to the derivation in the question, simply look at the argument of the exponential in the integrals and break it down into the form of $\\phi;$ namely, as a linear combination of $\\mu_i^2,$ $\\mu_i,$ and a constant:\n$$-\\frac{(x_i-\\mu_i)^2 + (y_i-\\mu_i)^2}{2\\sigma^2} = -\\frac{1}{\\sigma^2}\\mu_i^2 + \\frac{x_i+y_i}{\\sigma^2} \\mu_i + -\\frac{x_i^2 + y_i^2}{2\\sigma^2},$$\nfrom which we read off\n$$\\cases{A = \\frac{1}{\\sigma} \\\\ B = \\frac{x_i+y_i}{\\sigma^2} \\\\ C =  -\\frac{x_i^2 + y_i^2}{2\\sigma^2},}$$\nwhence\n$$C(\\theta) + \\frac{B(\\theta)^2}{4A(\\theta)^2} =  -\\frac{x_i^2 + y_i^2}{2\\sigma^2} + \\left(\\frac{x_i+y_i}{\\sigma^2}\\right)^2 \\frac{\\sigma^2}{4} = -\\frac{(x_i-y_i)^2}{4\\sigma^2}.$$\nPlugging everything into the formula $(*)$ gives--also by visual inspection--\n$$f(x_i,y_i,\\sigma) = f(\\theta) = \\sigma\\sqrt\\pi \\exp\\left[ -\\frac{(x_i-y_i)^2}{4\\sigma^2}\\right].$$\nThis takes us from the second line to the third line in the derivation.\n", "meta": {"language": "en", "url": "https://stats.stackexchange.com/questions/563452", "timestamp": "2023-03-29", "source": "stackexchange", "question_score": "3", "answer_count": 1, "answer_id": 0}}
